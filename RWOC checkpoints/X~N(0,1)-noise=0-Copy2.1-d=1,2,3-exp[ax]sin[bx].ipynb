{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline \n",
    "import torch \n",
    "#from IPython import display \n",
    "#from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "import random\n",
    "import math\n",
    "#from time import time\n",
    "#from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatedata(noise,showpermutation=False,showtrue_w=False):\n",
    "    true_w2 = torch.from_numpy(np.random.normal(0, 1,(num_X2feature,1)))\n",
    "    if showtrue_w:\n",
    "        print('true_w2:',true_w2)\n",
    "    X2_before_ =torch.from_numpy(np.random.normal(0, 1, (num_example, 1)))\n",
    "    y_ = torch.exp(true_w2[0]*X2_before_)*torch.sin(true_w2[1]*X2_before_)\n",
    "    y_ += torch.from_numpy(np.random.normal(0, noise ,size=y_.size()))\n",
    "    P_array=np.random.permutation(num_example)\n",
    "    P=torch.zeros(num_example,num_example,dtype=torch.float64)\n",
    "    for i in range(num_example):\n",
    "        P[i][P_array[i]]=1\n",
    "    if showpermutation:\n",
    "        print('打乱X2的置换矩阵为',P)\n",
    "    X2_=torch.mm(P,X2_before_)\n",
    "    conditionnumber=np.linalg.cond(X2_.numpy())\n",
    "    #X2_=X2_before_\n",
    "    error_reg=(torch.norm(y_-torch.exp(true_w2[0]*X2_before_)*torch.sin(true_w2[1]*X2_before_))/torch.norm(y_))\n",
    "    return y_,X2_,true_w2,P,error_reg,conditionnumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateinitialw(method,showinitialw=False):\n",
    "    if method=='normal':\n",
    "        w2 = torch.from_numpy(np.random.normal(0, 1,(num_X2feature,1)))\n",
    "    if method=='zeros':\n",
    "        w2=torch.zeros(num_X2feature,1,dtype=torch.float64)\n",
    "    if showinitialw:\n",
    "        print('initial w2:',w2)\n",
    "    return w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_stabilized(a, b, M, reg, numItermax=1000, tau=1e3, stopThr=1e-9,\n",
    "                        warmstart=None, verbose=False, print_period=20,\n",
    "                        log=False, **kwargs):\n",
    "\n",
    "#     a = np.asarray(a, dtype=np.float64)\n",
    "#     b = np.asarray(b, dtype=np.float64)\n",
    "#     M = np.asarray(M, dtype=np.float64)\n",
    "    a=a\n",
    "    b=b\n",
    "    M=M\n",
    "\n",
    "#     if len(a) == 0:\n",
    "#         a = np.ones((M.shape[0],), dtype=np.float64) / M.shape[0]\n",
    "#     if len(b) == 0:\n",
    "#         b = np.ones((M.shape[1],), dtype=np.float64) / M.shape[1]\n",
    "\n",
    "    # test if multiple target\n",
    "#     if len(b.shape) > 1:\n",
    "#         n_hists = b.shape[1]\n",
    "#         a = a[:, np.newaxis]\n",
    "#     else:\n",
    "#         n_hists = 0\n",
    "    n_hists = 0\n",
    "    # init data\n",
    "    dim_a = len(a)\n",
    "    dim_b = len(b)\n",
    "\n",
    "    cpt = 0\n",
    "    if log:\n",
    "        log = {'err': []}\n",
    "\n",
    "    # we assume that no distances are null except those of the diagonal of\n",
    "    # distances\n",
    "    if warmstart is None:\n",
    "        alpha, beta = torch.zeros(dim_a,1,dtype=torch.float64), torch.zeros(dim_b,1,dtype=torch.float64)\n",
    "    else:\n",
    "        alpha, beta = warmstart\n",
    "\n",
    "    if n_hists:\n",
    "        u = torch.ones((dim_a, n_hists)) / dim_a\n",
    "        v = torch.ones((dim_b, n_hists)) / dim_b\n",
    "    else:\n",
    "        u, v = torch.ones(dim_a,1,dtype=torch.float64) / dim_a, torch.ones(dim_b,1,dtype=torch.float64) / dim_b\n",
    "\n",
    "    def get_K(alpha, beta):\n",
    "        \"\"\"log space computation\"\"\"\n",
    "        return torch.exp(-(M - alpha.reshape((dim_a, 1))- beta.reshape((1, dim_b))) / reg)\n",
    "\n",
    "    def get_Gamma(alpha, beta, u, v):\n",
    "        \"\"\"log space gamma computation\"\"\"\n",
    "        return torch.exp(-(M - alpha.reshape((dim_a, 1)) - beta.reshape((1, dim_b)))\n",
    "                      / reg + torch.log(u.reshape((dim_a, 1))) + torch.log(v.reshape((1, dim_b))))\n",
    "\n",
    "    # print(np.min(K))\n",
    "\n",
    "    K = get_K(alpha, beta)\n",
    "    transp = K\n",
    "    loop = 1\n",
    "    cpt = 0\n",
    "    err = 1\n",
    "    while loop:\n",
    "\n",
    "        uprev = u\n",
    "        vprev = v\n",
    "        # sinkhornrn update\n",
    "        v = b / (torch.mm(K.transpose(1,0), u) + 1e-16)\n",
    "        u = a / (torch.mm(K, v) + 1e-16)\n",
    "        # remove numerical problems and store them in K\n",
    "        if torch.abs(u).max() > tau or torch.abs(v).max() > tau:\n",
    "            if n_hists:\n",
    "                alpha, beta = alpha + reg * \\\n",
    "                    torch.max(torch.log(u), 1), beta + reg * torch.max(np.log(v))\n",
    "            else:\n",
    "                alpha, beta = alpha + reg * torch.log(u), beta + reg * torch.log(v)\n",
    "                if n_hists:\n",
    "                    u, v = torch.ones((dim_a, n_hists)) / dim_a, torch.ones((dim_b, n_hists)) / dim_b\n",
    "                else:\n",
    "                    u, v = torch.ones(dim_a,1,dtype=torch.float64) / dim_a, torch.ones(dim_b,1,dtype=torch.float64) / dim_b\n",
    "            K = get_K(alpha, beta)\n",
    "            \n",
    "\n",
    "        if cpt % print_period == 0:\n",
    "            # we can speed up the process by checking for the error only all\n",
    "            # the 10th iterations\n",
    "            if n_hists:\n",
    "                err_u = abs(u - uprev).max()\n",
    "                err_u /= max(abs(u).max(), abs(uprev).max(), 1.)\n",
    "                err_v = abs(v - vprev).max()\n",
    "                err_v /= max(abs(v).max(), abs(vprev).max(), 1.)\n",
    "                err = 0.5 * (err_u + err_v)\n",
    "            else:\n",
    "                transp = get_Gamma(alpha, beta, u, v)\n",
    "                err = torch.norm((torch.sum(transp, axis=0) - b))\n",
    "            if log:\n",
    "                log['err'].append(err)\n",
    "\n",
    "            if verbose:\n",
    "                if cpt % (print_period * 20) == 0:\n",
    "                    print(\n",
    "                        '{:5s}|{:12s}'.format('It.', 'Err') + '\\n' + '-' * 19)\n",
    "                print('{:5d}|{:8e}|'.format(cpt, err))\n",
    "\n",
    "        if err <= stopThr:\n",
    "            loop = False\n",
    "\n",
    "        if cpt >= numItermax:\n",
    "            loop = False\n",
    "\n",
    "        if np.any(np.isnan(u.detach().numpy())) or np.any(np.isnan(v.detach().numpy())):\n",
    "            # we have reached the machine precision\n",
    "            # come back to previous solution and quit loop\n",
    "            print('Warning: numerical errors at iteration', cpt)\n",
    "            u = uprev\n",
    "            v = vprev\n",
    "            break\n",
    "\n",
    "        cpt = cpt + 1\n",
    "    #print(cpt)\n",
    "    if log:\n",
    "        if n_hists:\n",
    "            alpha = alpha[:, None]\n",
    "            beta = beta[:, None]\n",
    "        logu = alpha / reg + torch.log(u)\n",
    "        logv = beta / reg + torch.log(v)\n",
    "        log['logu'] = logu\n",
    "        log['logv'] = logv\n",
    "        log['alpha'] = alpha + reg * torch.log(u)\n",
    "        log['beta'] = beta + reg * torch.log(v)\n",
    "        log['warmstart'] = (log['alpha'], log['beta'])\n",
    "        if n_hists:\n",
    "            res = torch.zeros((n_hists))\n",
    "            for i in range(n_hists):\n",
    "                res[i] = torch.sum(get_Gamma(alpha, beta, u[:, i], v[:, i]) * M)\n",
    "            return res, log\n",
    "\n",
    "        else:\n",
    "            return get_Gamma(alpha, beta, u, v), log\n",
    "    else:\n",
    "        if n_hists:\n",
    "            res = torch.zeros((n_hists))\n",
    "            for i in range(n_hists):\n",
    "                res[i] = torch.sum(get_Gamma(alpha, beta, u[:, i], v[:, i]) * M)\n",
    "            return res\n",
    "        else:\n",
    "            return get_Gamma(alpha, beta, u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_epsilon_scaling(a, b, M, reg, numItermax=100, epsilon0=1e4,\n",
    "                             numInnerItermax=100, tau=1e3, stopThr=1e-9,\n",
    "                             warmstart=None, verbose=False, print_period=10,\n",
    "                             log=False, **kwargs):\n",
    "    #a = np.asarray(a, dtype=np.float64)\n",
    "    #b = np.asarray(b, dtype=np.float64)\n",
    "    #M = np.asarray(M, dtype=np.float64)\n",
    "    a=a\n",
    "    b=b\n",
    "    M=M\n",
    "#     if len(a) == 0:\n",
    "#         a = np.ones((M.shape[0],), dtype=np.float64) / M.shape[0]\n",
    "#     if len(b) == 0:\n",
    "#         b = np.ones((M.shape[1],), dtype=np.float64) / M.shape[1]\n",
    "\n",
    "    # init data\n",
    "    dim_a = len(a)\n",
    "    #dim_a=num_example\n",
    "    dim_b = len(b)\n",
    "    #dim_b=num_example\n",
    "    # nrelative umerical precision with 64 bits\n",
    "    numItermin = 35\n",
    "    numItermax = max(numItermin, numItermax)  # ensure that last velue is exact\n",
    "\n",
    "    cpt = 0\n",
    "    if log:\n",
    "        log = {'err': []}\n",
    "\n",
    "    # we assume that no distances are null except those of the diagonal of\n",
    "    # distances\n",
    "    if warmstart is None:\n",
    "        alpha, beta = torch.zeros(dim_a,1,dtype=torch.float64), torch.zeros(dim_b,1,dtype=torch.float64)\n",
    "    else:\n",
    "        alpha, beta = warmstart\n",
    "\n",
    "    def get_K(alpha, beta):\n",
    "        \"\"\"log space computation\"\"\"\n",
    "        return torch.exp(-(M - alpha.reshape((dim_a, 1))\n",
    "                        - beta.reshape((1, dim_b))) / reg)\n",
    "\n",
    "    # print(np.min(K))\n",
    "    def get_reg(n):  # exponential decreasing\n",
    "        return (epsilon0 - reg) * np.exp(-n) + reg\n",
    "\n",
    "    loop = 1\n",
    "    cpt = 0\n",
    "    err = 1\n",
    "    while loop:\n",
    "\n",
    "        regi = get_reg(cpt)\n",
    "\n",
    "        G, logi = sinkhorn_stabilized(a, b, M, regi,\n",
    "                                      numItermax=numInnerItermax, stopThr=1e-9,\n",
    "                                      warmstart=(alpha, beta), verbose=False,\n",
    "                                      print_period=20, tau=tau, log=True)\n",
    "\n",
    "        alpha = logi['alpha']\n",
    "        beta = logi['beta']\n",
    "\n",
    "        if cpt >= numItermax:\n",
    "            loop = False\n",
    "\n",
    "        if cpt % (print_period) == 0:  # spsion nearly converged\n",
    "            # we can speed up the process by checking for the error only all\n",
    "            # the 10th iterations\n",
    "            transp = G\n",
    "            err = torch.norm(\n",
    "                (torch.sum(transp, axis=0) - b))**2 + torch.norm((torch.sum(transp, axis=1) - a))**2\n",
    "            if log:\n",
    "                log['err'].append(err)\n",
    "\n",
    "            if verbose:\n",
    "                if cpt % (print_period * 10) == 0:\n",
    "                    print(\n",
    "                        '{:5s}|{:12s}'.format('It.', 'Err') + '\\n' + '-' * 19)\n",
    "                print('{:5d}|{:8e}|'.format(cpt, err))\n",
    "\n",
    "        if err <= stopThr and cpt > numItermin:\n",
    "            loop = False\n",
    "\n",
    "        cpt = cpt + 1\n",
    "    # print('err=',err,' cpt=',cpt)\n",
    "    if log:\n",
    "        log['alpha'] = alpha\n",
    "        log['beta'] = beta\n",
    "        log['warmstart'] = (log['alpha'], log['beta'])\n",
    "        return G, log\n",
    "    else:\n",
    "        return G\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0 =50.8659\n",
      "#Loss 1 =50.7801\n",
      "#Loss 2 =21.9877\n",
      "#Loss 3 =12.2403\n",
      "#Loss 4 =7.1721\n",
      "#Loss 5 =4.2150\n",
      "#Loss 6 =2.6507\n",
      "#Loss 7 =1.9020\n",
      "#Loss 8 =1.5380\n",
      "#Loss 9 =1.3268\n",
      "#Loss 10 =1.1802\n",
      "#Loss 11 =1.0674\n",
      "#Loss 12 =0.9773\n",
      "#Loss 13 =0.9049\n",
      "#Loss 14 =0.8464\n",
      "#Loss 15 =0.7988\n",
      "#Loss 16 =0.7599\n",
      "#Loss 17 =0.7224\n",
      "#Loss 18 =0.6910\n",
      "#Loss 19 =0.6652\n",
      "#Loss 20 =0.6425\n",
      "#Loss 21 =0.6215\n",
      "#Loss 22 =0.6024\n",
      "#Loss 23 =0.5845\n",
      "#Loss 24 =0.5676\n",
      "#Loss 25 =0.5519\n",
      "#Loss 26 =0.5352\n",
      "#Loss 27 =0.5207\n",
      "#Loss 28 =0.5068\n",
      "#Loss 29 =0.4956\n",
      "#Loss 30 =0.4864\n",
      "#Loss 31 =0.4783\n",
      "#Loss 32 =0.4714\n",
      "#Loss 33 =0.4653\n",
      "#Loss 34 =0.4604\n",
      "#Loss 35 =0.4563\n",
      "#Loss 36 =0.4527\n",
      "#Loss 37 =0.4497\n",
      "#Loss 38 =0.4467\n",
      "#Loss 39 =0.4442\n",
      "#Loss 40 =0.4421\n",
      "#Loss 41 =0.4403\n",
      "#Loss 42 =0.4388\n",
      "#Loss 43 =0.4376\n",
      "#Loss 44 =0.4367\n",
      "#Loss 45 =0.4359\n",
      "#Loss 46 =0.4350\n",
      "#Loss 47 =0.4339\n",
      "#Loss 48 =0.4330\n",
      "#Loss 49 =0.4322\n",
      "#Loss 50 =0.4315\n",
      "#Loss 51 =0.4309\n",
      "#Loss 52 =0.4304\n",
      "#Loss 53 =0.4300\n",
      "#Loss 54 =0.4296\n",
      "#Loss 55 =0.4294\n",
      "#Loss 56 =0.4291\n",
      "#Loss 57 =0.4290\n",
      "#Loss 58 =0.4288\n",
      "#Loss 59 =0.4287\n",
      "#Loss 60 =0.4286\n",
      "#Loss 61 =0.4284\n",
      "#Loss 62 =0.4283\n",
      "#Loss 63 =0.4282\n",
      "#Loss 64 =0.4281\n",
      "#Loss 65 =0.4281\n",
      "#Loss 66 =0.4280\n",
      "#Loss 67 =0.4280\n",
      "#Loss 68 =0.4279\n",
      "#Loss 69 =0.4279\n",
      "#Loss 70 =0.4279\n",
      "#Loss 71 =0.4279\n",
      "#Loss 72 =0.4279\n",
      "#Loss 73 =0.4279\n",
      "#Loss 74 =0.4279\n",
      "#Loss 75 =0.4279\n",
      "#Loss 76 =0.4278\n",
      "#Loss 77 =0.4278\n",
      " 2 100 绝对误差 tensor(1.9018, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(2.0278, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1318, dtype=torch.float64)   实验回归误差 tensor(0.0917, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4000, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =54.1144\n",
      "#Loss 1 =50.2527\n",
      "#Loss 2 =22.1703\n",
      "#Loss 3 =11.1050\n",
      "#Loss 4 =4.7896\n",
      "#Loss 5 =1.5863\n",
      "#Loss 6 =0.4824\n",
      "#Loss 7 =0.2538\n",
      "#Loss 8 =0.2241\n",
      "#Loss 9 =0.2197\n",
      "#Loss 10 =0.2180\n",
      "#Loss 11 =0.2168\n",
      "#Loss 12 =0.2160\n",
      "#Loss 13 =0.2153\n",
      "#Loss 14 =0.2148\n",
      "#Loss 15 =0.2144\n",
      "#Loss 16 =0.2141\n",
      "#Loss 17 =0.2139\n",
      "#Loss 18 =0.2137\n",
      "#Loss 19 =0.2136\n",
      "#Loss 20 =0.2135\n",
      "#Loss 21 =0.2134\n",
      "#Loss 22 =0.2133\n",
      "#Loss 23 =0.2133\n",
      "#Loss 24 =0.2133\n",
      "#Loss 25 =0.2132\n",
      "#Loss 26 =0.2132\n",
      "#Loss 27 =0.2132\n",
      "#Loss 28 =0.2132\n",
      "#Loss 29 =0.2132\n",
      "#Loss 30 =0.2132\n",
      "#Loss 31 =0.2132\n",
      "#Loss 32 =0.2132\n",
      "#Loss 33 =0.2132\n",
      "#Loss 34 =0.2132\n",
      "#Loss 35 =0.2132\n",
      " 2 100 绝对误差 tensor(0.0107, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.0157, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1359, dtype=torch.float64)   实验回归误差 tensor(0.0628, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.3491, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =58.0954\n",
      "#Loss 1 =56.1172\n",
      "#Loss 2 =23.5972\n",
      "#Loss 3 =11.8817\n",
      "#Loss 4 =6.4189\n",
      "#Loss 5 =4.0626\n",
      "#Loss 6 =3.3006\n",
      "#Loss 7 =3.0998\n",
      "#Loss 8 =3.0296\n",
      "#Loss 9 =2.9883\n",
      "#Loss 10 =2.9584\n",
      "#Loss 11 =2.9359\n",
      "#Loss 12 =2.9188\n",
      "#Loss 13 =2.9058\n",
      "#Loss 14 =2.8960\n",
      "#Loss 15 =2.8885\n",
      "#Loss 16 =2.8829\n",
      "#Loss 17 =2.8787\n",
      "#Loss 18 =2.8755\n",
      "#Loss 19 =2.8730\n",
      "#Loss 20 =2.8712\n",
      "#Loss 21 =2.8698\n",
      "#Loss 22 =2.8688\n",
      "#Loss 23 =2.8680\n",
      "#Loss 24 =2.8674\n",
      "#Loss 25 =2.8669\n",
      "#Loss 26 =2.8666\n",
      "#Loss 27 =2.8663\n",
      "#Loss 28 =2.8661\n",
      "#Loss 29 =2.8660\n",
      "#Loss 30 =2.8659\n",
      "#Loss 31 =2.8658\n",
      "#Loss 32 =2.8657\n",
      "#Loss 33 =2.8657\n",
      "#Loss 34 =2.8656\n",
      "#Loss 35 =2.8656\n",
      " 2 100 绝对误差 tensor(1.9457, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.6488, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1308, dtype=torch.float64)   实验回归误差 tensor(0.2221, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4138, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 98   条件数 1.0\n",
      "Loss 0 =5722.4908\n",
      "#Loss 1 =5716.0829\n",
      "#Loss 2 =5363.4785\n",
      "#Loss 3 =5058.8303\n",
      "#Loss 4 =200463.7474\n",
      "#Loss 5 =nan\n",
      " 2 100 绝对误差 tensor(nan, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(nan, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0126, dtype=torch.float64)   实验回归误差 tensor(nan, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 0   条件数 1.0\n",
      "Loss 0 =74.6826\n",
      "#Loss 1 =69.8652\n",
      "#Loss 2 =37.7520\n",
      "#Loss 3 =22.3560\n",
      "#Loss 4 =11.3320\n",
      "#Loss 5 =4.5257\n",
      "#Loss 6 =2.0033\n",
      "#Loss 7 =1.4417\n",
      "#Loss 8 =1.1962\n",
      "#Loss 9 =1.0140\n",
      "#Loss 10 =0.8749\n",
      "#Loss 11 =0.7682\n",
      "#Loss 12 =0.6858\n",
      "#Loss 13 =0.6218\n",
      "#Loss 14 =0.5719\n",
      "#Loss 15 =0.5328\n",
      "#Loss 16 =0.5021\n",
      "#Loss 17 =0.4778\n",
      "#Loss 18 =0.4586\n",
      "#Loss 19 =0.4433\n",
      "#Loss 20 =0.4312\n",
      "#Loss 21 =0.4215\n",
      "#Loss 22 =0.4137\n",
      "#Loss 23 =0.4075\n",
      "#Loss 24 =0.4025\n",
      "#Loss 25 =0.3986\n",
      "#Loss 26 =0.3953\n",
      "#Loss 27 =0.3928\n",
      "#Loss 28 =0.3907\n",
      "#Loss 29 =0.3890\n",
      "#Loss 30 =0.3876\n",
      "#Loss 31 =0.3865\n",
      "#Loss 32 =0.3856\n",
      "#Loss 33 =0.3849\n",
      "#Loss 34 =0.3843\n",
      "#Loss 35 =0.3839\n",
      "#Loss 36 =0.3835\n",
      "#Loss 37 =0.3832\n",
      "#Loss 38 =0.3829\n",
      "#Loss 39 =0.3827\n",
      "#Loss 40 =0.3825\n",
      "#Loss 41 =0.3824\n",
      "#Loss 42 =0.3823\n",
      "#Loss 43 =0.3822\n",
      "#Loss 44 =0.3821\n",
      "#Loss 45 =0.3821\n",
      "#Loss 46 =0.3820\n",
      "#Loss 47 =0.3820\n",
      "#Loss 48 =0.3820\n",
      "#Loss 49 =0.3819\n",
      "#Loss 50 =0.3819\n",
      "#Loss 51 =0.3819\n",
      "#Loss 52 =0.3819\n",
      "#Loss 53 =0.3819\n",
      "#Loss 54 =0.3819\n",
      "#Loss 55 =0.3819\n",
      "#Loss 56 =0.3818\n",
      "#Loss 57 =0.3818\n",
      "#Loss 58 =0.3818\n",
      "#Loss 59 =0.3818\n",
      " 2 100 绝对误差 tensor(0.0430, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.0565, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1301, dtype=torch.float64)   实验回归误差 tensor(0.0715, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.3468, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 98   条件数 1.0\n",
      "Loss 0 =111.3381\n",
      "#Loss 1 =110.8432\n",
      "#Loss 2 =72.1260\n",
      "#Loss 3 =51.4540\n",
      "#Loss 4 =29.2461\n",
      "#Loss 5 =10.1596\n",
      "#Loss 6 =3.4017\n",
      "#Loss 7 =2.7085\n",
      "#Loss 8 =2.2973\n",
      "#Loss 9 =2.0425\n",
      "#Loss 10 =1.8822\n",
      "#Loss 11 =1.7795\n",
      "#Loss 12 =1.7131\n",
      "#Loss 13 =1.6697\n",
      "#Loss 14 =1.6411\n",
      "#Loss 15 =1.6222\n",
      "#Loss 16 =1.6096\n",
      "#Loss 17 =1.6011\n",
      "#Loss 18 =1.5955\n",
      "#Loss 19 =1.5917\n",
      "#Loss 20 =1.5891\n",
      "#Loss 21 =1.5874\n",
      "#Loss 22 =1.5862\n",
      "#Loss 23 =1.5854\n",
      "#Loss 24 =1.5849\n",
      "#Loss 25 =1.5845\n",
      "#Loss 26 =1.5843\n",
      "#Loss 27 =1.5841\n",
      "#Loss 28 =1.5840\n",
      "#Loss 29 =1.5840\n",
      "#Loss 30 =1.5839\n",
      "#Loss 31 =1.5839\n",
      "#Loss 32 =1.5839\n",
      " 2 100 绝对误差 tensor(1.8224, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.9671, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0957, dtype=torch.float64)   实验回归误差 tensor(0.1193, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4040, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 98   条件数 1.0\n",
      "Loss 0 =15.6726\n",
      "#Loss 1 =15.6184\n",
      "#Loss 2 =6.3877\n",
      "#Loss 3 =3.0914\n",
      "#Loss 4 =1.6972\n",
      "#Loss 5 =1.0016\n",
      "#Loss 6 =0.6251\n",
      "#Loss 7 =0.4185\n",
      "#Loss 8 =0.3071\n",
      "#Loss 9 =0.2488\n",
      "#Loss 10 =0.2189\n",
      "#Loss 11 =0.2035\n",
      "#Loss 12 =0.1955\n",
      "#Loss 13 =0.1911\n",
      "#Loss 14 =0.1885\n",
      "#Loss 15 =0.1869\n",
      "#Loss 16 =0.1858\n",
      "#Loss 17 =0.1851\n",
      "#Loss 18 =0.1846\n",
      "#Loss 19 =0.1842\n",
      "#Loss 20 =0.1840\n",
      "#Loss 21 =0.1838\n",
      "#Loss 22 =0.1836\n",
      "#Loss 23 =0.1835\n",
      "#Loss 24 =0.1835\n",
      "#Loss 25 =0.1834\n",
      "#Loss 26 =0.1834\n",
      "#Loss 27 =0.1834\n",
      "#Loss 28 =0.1833\n",
      "#Loss 29 =0.1833\n",
      "#Loss 30 =0.1833\n",
      "#Loss 31 =0.1833\n",
      "#Loss 32 =0.1833\n",
      "#Loss 33 =0.1833\n",
      "#Loss 34 =0.1833\n",
      "#Loss 35 =0.1833\n",
      "#Loss 36 =0.1833\n",
      " 2 100 绝对误差 tensor(0.8835, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.8381, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.2786, dtype=torch.float64)   实验回归误差 tensor(0.1081, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =136.3484\n",
      "#Loss 1 =124.9487\n",
      "#Loss 2 =68.1863\n",
      "#Loss 3 =35.7586\n",
      "#Loss 4 =9.6862\n",
      "#Loss 5 =0.5327\n",
      "#Loss 6 =0.4770\n",
      "#Loss 7 =0.4492\n",
      "#Loss 8 =0.4349\n",
      "#Loss 9 =0.4276\n",
      "#Loss 10 =0.4238\n",
      "#Loss 11 =0.4218\n",
      "#Loss 12 =0.4207\n",
      "#Loss 13 =0.4202\n",
      "#Loss 14 =0.4199\n",
      "#Loss 15 =0.4198\n",
      "#Loss 16 =0.4197\n",
      "#Loss 17 =0.4197\n",
      "#Loss 18 =0.4196\n",
      "#Loss 19 =0.4196\n",
      "#Loss 20 =0.4196\n",
      "#Loss 21 =0.4196\n",
      " 2 100 绝对误差 tensor(0.0118, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.0117, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0976, dtype=torch.float64)   实验回归误差 tensor(0.0555, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.2884, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =70.3224\n",
      "#Loss 1 =69.6726\n",
      "#Loss 2 =48.4068\n",
      "#Loss 3 =37.0511\n",
      "#Loss 4 =27.8094\n",
      "#Loss 5 =19.4349\n",
      "#Loss 6 =12.8065\n",
      "#Loss 7 =8.8476\n",
      "#Loss 8 =7.1944\n",
      "#Loss 9 =6.5507\n",
      "#Loss 10 =6.1437\n",
      "#Loss 11 =5.8098\n",
      "#Loss 12 =5.5275\n",
      "#Loss 13 =5.2870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Loss 14 =5.0806\n",
      "#Loss 15 =4.9026\n",
      "#Loss 16 =4.7479\n",
      "#Loss 17 =4.6126\n",
      "#Loss 18 =4.4939\n",
      "#Loss 19 =4.3891\n",
      "#Loss 20 =4.2962\n",
      "#Loss 21 =4.2133\n",
      "#Loss 22 =4.1392\n",
      "#Loss 23 =4.0726\n",
      "#Loss 24 =4.0126\n",
      "#Loss 25 =3.9583\n",
      "#Loss 26 =3.9089\n",
      "#Loss 27 =3.8640\n",
      "#Loss 28 =3.8230\n",
      "#Loss 29 =3.7854\n",
      "#Loss 30 =3.7510\n",
      "#Loss 31 =3.7193\n",
      "#Loss 32 =3.6902\n",
      "#Loss 33 =3.6634\n",
      "#Loss 34 =3.6385\n",
      "#Loss 35 =3.6155\n",
      "#Loss 36 =3.5941\n",
      "#Loss 37 =3.5742\n",
      "#Loss 38 =3.5557\n",
      "#Loss 39 =3.5384\n",
      "#Loss 40 =3.5223\n",
      "#Loss 41 =3.5072\n",
      "#Loss 42 =3.4930\n",
      "#Loss 43 =3.4797\n",
      "#Loss 44 =3.4673\n",
      "#Loss 45 =3.4556\n",
      "#Loss 46 =3.4446\n",
      "#Loss 47 =3.4342\n",
      "#Loss 48 =3.4245\n",
      "#Loss 77 =3.7705\n",
      "#Loss 78 =3.7372\n",
      "#Loss 79 =3.7067\n",
      "#Loss 80 =3.6786\n",
      "#Loss 81 =3.6526\n",
      "#Loss 82 =3.6285\n",
      "#Loss 83 =3.6062\n",
      "#Loss 84 =3.5855\n",
      "#Loss 85 =3.5661\n",
      "#Loss 86 =3.5482\n",
      "#Loss 87 =3.5314\n",
      "#Loss 88 =3.5158\n",
      "#Loss 89 =3.5011\n",
      "#Loss 90 =3.4873\n",
      "#Loss 91 =3.4744\n",
      "#Loss 92 =3.4623\n",
      "#Loss 93 =3.4509\n",
      "#Loss 94 =3.4403\n",
      "#Loss 95 =3.4304\n",
      "#Loss 96 =3.4213\n",
      "#Loss 97 =3.4130\n",
      "#Loss 98 =3.4061\n",
      "#Loss 99 =3.4013\n",
      "#Loss 100 =3.4011\n",
      "#Loss 101 =3.4108\n",
      "#Loss 102 =3.4458\n",
      "#Loss 103 =3.5385\n",
      "#Loss 104 =3.7996\n",
      "#Loss 105 =4.4141\n",
      "#Loss 106 =6.1940\n",
      "#Loss 107 =9.2125\n",
      "#Loss 108 =16.8746\n",
      "#Loss 109 =13.5671\n",
      "#Loss 110 =14.1045\n",
      "#Loss 111 =5.1768\n",
      "#Loss 112 =4.4422\n",
      "#Loss 113 =4.0656\n",
      "#Loss 114 =3.9535\n",
      "#Loss 115 =3.8841\n",
      "#Loss 116 =3.8352\n",
      "#Loss 117 =3.7939\n",
      "#Loss 118 =3.7578\n",
      "#Loss 119 =3.7250\n",
      "#Loss 120 =3.6953\n",
      "#Loss 121 =3.6679\n",
      "#Loss 122 =3.6427\n",
      "#Loss 123 =3.6193\n",
      "#Loss 124 =3.5977\n",
      "#Loss 125 =3.5775\n",
      "#Loss 126 =3.5588\n",
      "#Loss 127 =3.5414\n",
      "#Loss 128 =3.5253\n",
      "#Loss 129 =3.5102\n",
      "#Loss 130 =3.4961\n",
      "#Loss 131 =3.4831\n",
      "#Loss 132 =3.4712\n",
      "#Loss 133 =3.4606\n",
      "#Loss 134 =3.4518\n",
      "#Loss 135 =3.4454\n",
      "#Loss 136 =3.4438\n",
      "#Loss 137 =3.4503\n",
      "#Loss 138 =3.4756\n",
      "#Loss 139 =3.5356\n",
      "#Loss 140 =3.6894\n",
      "#Loss 141 =4.0053\n",
      "#Loss 142 =4.8311\n",
      "#Loss 143 =6.2266\n",
      "#Loss 144 =9.8891\n",
      "#Loss 145 =11.5573\n",
      "#Loss 146 =16.1486\n",
      "#Loss 147 =7.7170\n",
      "#Loss 148 =6.4824\n",
      "#Loss 149 =4.5081\n",
      "#Loss 150 =4.1659\n",
      "超过迭代上限\n",
      " 2 100 绝对误差 tensor(1.8439, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.9479, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1320, dtype=torch.float64)   实验回归误差 tensor(0.2369, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4140, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =11.3599\n",
      "#Loss 1 =11.3576\n",
      "#Loss 2 =5.0732\n",
      "#Loss 3 =2.4930\n",
      "#Loss 4 =1.3744\n",
      "#Loss 5 =0.8411\n",
      "#Loss 6 =0.5633\n",
      "#Loss 7 =0.4079\n",
      "#Loss 8 =0.3158\n",
      "#Loss 9 =0.2588\n",
      "#Loss 10 =0.2222\n",
      "#Loss 11 =0.1980\n",
      "#Loss 12 =0.1815\n",
      "#Loss 13 =0.1701\n",
      "#Loss 14 =0.1620\n",
      "#Loss 15 =0.1560\n",
      "#Loss 16 =0.1516\n",
      "#Loss 17 =0.1483\n",
      "#Loss 18 =0.1458\n",
      "#Loss 19 =0.1438\n",
      "#Loss 20 =0.1422\n",
      "#Loss 21 =0.1410\n",
      "#Loss 22 =0.1400\n",
      "#Loss 23 =0.1392\n",
      "#Loss 24 =0.1386\n",
      "#Loss 25 =0.1381\n",
      "#Loss 26 =0.1377\n",
      "#Loss 27 =0.1373\n",
      "#Loss 28 =0.1371\n",
      "#Loss 29 =0.1368\n",
      "#Loss 30 =0.1367\n",
      "#Loss 31 =0.1365\n",
      "#Loss 32 =0.1364\n",
      "#Loss 33 =0.1363\n",
      "#Loss 34 =0.1362\n",
      "#Loss 35 =0.1361\n",
      "#Loss 36 =0.1361\n",
      "#Loss 37 =0.1360\n",
      "#Loss 38 =0.1360\n",
      "#Loss 39 =0.1360\n",
      "#Loss 40 =0.1360\n",
      "#Loss 41 =0.1359\n",
      "#Loss 42 =0.1359\n",
      "#Loss 43 =0.1359\n",
      "#Loss 44 =0.1359\n",
      "#Loss 45 =0.1359\n",
      "#Loss 46 =0.1359\n",
      "#Loss 47 =0.1359\n",
      "#Loss 48 =0.1359\n",
      "#Loss 49 =0.1359\n",
      "#Loss 50 =0.1359\n",
      "#Loss 51 =0.1359\n",
      "#Loss 52 =0.1359\n",
      "#Loss 53 =0.1358\n",
      "#Loss 54 =0.1358\n",
      "#Loss 55 =0.1358\n",
      " 2 100 绝对误差 tensor(0.7349, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(2.1147, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.2994, dtype=torch.float64)   实验回归误差 tensor(0.1094, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4039, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 98   条件数 1.0\n",
      "Loss 0 =148.4467\n",
      "#Loss 1 =142.7064\n",
      "#Loss 2 =82.6475\n",
      "#Loss 3 =40.4610\n",
      "#Loss 4 =0.3098\n",
      "#Loss 5 =0.5596\n",
      "#Loss 6 =1.8886\n",
      "#Loss 7 =6.4913\n",
      "#Loss 8 =30.3930\n",
      "#Loss 9 =2.1885\n",
      "#Loss 10 =10.0668\n",
      "#Loss 11 =16.6130\n",
      "#Loss 12 =66.1308\n",
      "#Loss 13 =26.4508\n",
      "#Loss 14 =4.5826\n",
      "#Loss 15 =21.6849\n",
      "#Loss 16 =8.6422\n",
      "#Loss 17 =39.1687\n",
      "#Loss 18 =0.3415\n",
      "#Loss 19 =0.6370\n",
      "#Loss 20 =2.2674\n",
      "#Loss 21 =7.6167\n",
      "#Loss 22 =35.1466\n",
      "#Loss 23 =0.4848\n",
      "#Loss 24 =1.4032\n",
      "#Loss 25 =4.8593\n",
      "#Loss 26 =23.0953\n",
      "#Loss 27 =7.4645\n",
      "#Loss 28 =34.3737\n",
      "#Loss 29 =0.6310\n",
      "#Loss 30 =2.1395\n",
      "#Loss 31 =7.2064\n",
      "#Loss 32 =33.4201\n",
      "#Loss 33 =0.9102\n",
      "#Loss 34 =3.5723\n",
      "#Loss 35 =10.8196\n",
      "#Loss 36 =47.5769\n",
      "#Loss 37 =3.6705\n",
      "#Loss 38 =10.9377\n",
      "#Loss 39 =47.9843\n",
      "#Loss 40 =3.9566\n",
      "#Loss 41 =11.5139\n",
      "#Loss 42 =50.0541\n",
      "#Loss 43 =6.4263\n",
      "#Loss 44 =14.8526\n",
      "#Loss 45 =60.9022\n",
      "#Loss 46 =18.7618\n",
      "#Loss 47 =11.2431\n",
      "#Loss 48 =49.0094\n",
      "#Loss 49 =5.6256\n",
      "#Loss 50 =13.9828\n",
      "#Loss 51 =58.1895\n",
      "#Loss 52 =14.9300\n",
      "#Loss 53 =14.4092\n",
      "#Loss 54 =59.4988\n",
      "#Loss 55 =17.0297\n",
      "#Loss 56 =12.7451\n",
      "#Loss 57 =54.1401\n",
      "#Loss 58 =10.4967\n",
      "#Loss 59 =16.3827\n",
      "#Loss 60 =65.4487\n",
      "#Loss 61 =25.0915\n",
      "#Loss 62 =5.6282\n",
      "#Loss 63 =26.4544\n",
      "#Loss 64 =4.6374\n",
      "#Loss 65 =21.9321\n",
      "#Loss 66 =8.4153\n",
      "#Loss 67 =38.2624\n",
      "#Loss 68 =0.2682\n",
      "#Loss 69 =0.3049\n",
      "#Loss 70 =0.5884\n",
      "#Loss 71 =1.8080\n",
      "#Loss 72 =8.2979\n",
      "#Loss 73 =16.4534\n",
      "#Loss 74 =65.7150\n",
      "#Loss 75 =25.4031\n",
      "#Loss 76 =5.3701\n",
      "#Loss 77 =25.2939\n",
      "#Loss 78 =5.5328\n",
      "#Loss 79 =26.0191\n",
      "#Loss 80 =4.9624\n",
      "#Loss 81 =23.4324\n",
      "#Loss 82 =7.0887\n",
      "#Loss 83 =32.7979\n",
      "#Loss 84 =1.0989\n",
      "#Loss 85 =4.5324\n",
      "#Loss 86 =12.6191\n",
      "#Loss 87 =53.8397\n",
      "#Loss 88 =9.9838\n",
      "#Loss 89 =16.4217\n",
      "#Loss 90 =65.5647\n",
      "#Loss 91 =25.2440\n",
      "#Loss 92 =5.5001\n",
      "#Loss 93 =25.8803\n",
      "#Loss 94 =5.0721\n",
      "#Loss 95 =23.9321\n",
      "#Loss 96 =6.6601\n",
      "#Loss 97 =30.9704\n",
      "#Loss 98 =1.8523\n",
      "#Loss 99 =8.3669\n",
      "#Loss 100 =16.3473\n",
      "#Loss 101 =65.3872\n",
      "#Loss 102 =24.9812\n",
      "#Loss 103 =5.7155\n",
      "#Loss 104 =26.8441\n",
      "#Loss 105 =4.3518\n",
      "#Loss 106 =20.6022\n",
      "#Loss 107 =9.6269\n",
      "#Loss 108 =43.0113\n",
      "#Loss 109 =1.2963\n",
      "#Loss 110 =4.3919\n",
      "#Loss 111 =20.8928\n",
      "#Loss 112 =9.4285\n",
      "#Loss 113 =42.2470\n",
      "#Loss 114 =1.0259\n",
      "#Loss 115 =3.4112\n",
      "#Loss 116 =16.2178\n",
      "#Loss 117 =13.6129\n",
      "#Loss 118 =56.9605\n",
      "#Loss 119 =13.5436\n",
      "#Loss 120 =15.2933\n",
      "#Loss 121 =62.2149\n",
      "#Loss 122 =20.4319\n",
      "#Loss 123 =9.7079\n",
      "#Loss 124 =43.3302\n",
      "#Loss 125 =1.4190\n",
      "#Loss 126 =4.8166\n",
      "#Loss 127 =22.8664\n",
      "#Loss 128 =7.6466\n",
      "#Loss 129 =35.1289\n",
      "#Loss 130 =0.4746\n",
      "#Loss 131 =1.3444\n",
      "#Loss 132 =4.6533\n",
      "#Loss 133 =22.1427\n",
      "#Loss 134 =8.3227\n",
      "#Loss 135 =37.8872\n",
      "#Loss 136 =0.2552\n",
      "#Loss 137 =0.2453\n",
      "#Loss 138 =0.2939\n",
      "#Loss 139 =0.5371\n",
      "#Loss 140 =1.7877\n",
      "#Loss 141 =6.1844\n",
      "#Loss 142 =29.0668\n",
      "#Loss 143 =2.9403\n",
      "#Loss 144 =13.8134\n",
      "#Loss 145 =15.2730\n",
      "#Loss 146 =62.1467\n",
      "#Loss 147 =20.3495\n",
      "#Loss 148 =9.7828\n",
      "#Loss 149 =43.6158\n",
      "#Loss 150 =1.5371\n",
      "超过迭代上限\n",
      " 2 100 绝对误差 tensor(0.0731, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.0857, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0767, dtype=torch.float64)   实验回归误差 tensor(0.1876, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.3038, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =86.1963\n",
      "#Loss 1 =85.9090\n",
      "#Loss 2 =61.9830\n",
      "#Loss 3 =47.8187\n",
      "#Loss 4 =29.2474\n",
      "#Loss 5 =12.1821\n",
      "#Loss 6 =6.8193\n",
      "#Loss 7 =6.2343\n",
      "#Loss 8 =15.6411\n",
      "#Loss 9 =73.9793\n",
      "#Loss 10 =22.0733\n",
      "#Loss 11 =9.9131\n",
      "#Loss 12 =5.7334\n",
      "#Loss 13 =3.8809\n",
      "#Loss 14 =4.6974\n",
      "#Loss 15 =28.4859\n",
      "#Loss 16 =72.8809\n",
      "#Loss 17 =60.4410\n",
      "#Loss 18 =45.4904\n",
      "#Loss 19 =24.6168\n",
      "#Loss 20 =8.4190\n",
      "#Loss 21 =4.4271\n",
      "#Loss 22 =2.8181\n",
      "#Loss 23 =4.3520\n",
      "#Loss 24 =40.7634\n",
      "#Loss 25 =52.2098\n",
      "#Loss 26 =20.5052\n",
      "#Loss 27 =9.0594\n",
      "#Loss 28 =6.0102\n",
      "#Loss 29 =6.5964\n",
      "#Loss 30 =27.9572\n",
      "#Loss 31 =58.7494\n",
      "#Loss 32 =74.6138\n",
      "#Loss 33 =61.3976\n",
      "#Loss 34 =48.0455\n",
      "#Loss 35 =29.8750\n",
      "#Loss 36 =12.6137\n",
      "#Loss 37 =7.0267\n",
      "#Loss 38 =6.5664\n",
      "#Loss 39 =16.4713\n",
      "#Loss 40 =74.2145\n",
      "#Loss 41 =23.1224\n",
      "#Loss 42 =10.1518\n",
      "#Loss 43 =5.7675\n",
      "#Loss 44 =3.7437\n",
      "#Loss 45 =2.8696\n",
      "#Loss 46 =4.8060\n",
      "#Loss 47 =44.1932\n",
      "#Loss 48 =75.4732\n",
      "#Loss 49 =63.2980\n",
      "#Loss 50 =44.9853\n",
      "#Loss 51 =26.4625\n",
      "#Loss 52 =11.5733\n",
      "#Loss 53 =6.3770\n",
      "#Loss 54 =4.7041\n",
      "#Loss 55 =7.5741\n",
      "#Loss 56 =46.3683\n",
      "#Loss 57 =39.2891\n",
      "#Loss 58 =35.5955\n",
      "#Loss 59 =17.0333\n",
      "#Loss 60 =8.4324\n",
      "#Loss 61 =6.7864\n",
      "#Loss 62 =11.7555\n",
      "#Loss 63 =54.7527\n",
      "#Loss 64 =17.9793\n",
      "#Loss 65 =9.9438\n",
      "#Loss 66 =6.8848\n",
      "#Loss 67 =12.6787\n",
      "#Loss 68 =42.1422\n",
      "#Loss 69 =81.2057\n",
      "#Loss 70 =68.1023\n",
      "#Loss 71 =58.0054\n",
      "#Loss 72 =42.9864\n",
      "#Loss 73 =23.8435\n",
      "#Loss 74 =9.9388\n",
      "#Loss 75 =5.6743\n",
      "#Loss 76 =3.6942\n",
      "#Loss 77 =2.8000\n",
      "#Loss 78 =4.2217\n",
      "#Loss 79 =37.5397\n",
      "#Loss 80 =93.3766\n",
      "#Loss 81 =100.7321\n",
      "#Loss 82 =55.6605\n",
      "#Loss 83 =39.2666\n",
      "#Loss 84 =20.6566\n",
      "#Loss 85 =9.4851\n",
      "#Loss 86 =5.7008\n",
      "#Loss 87 =4.4016\n",
      "#Loss 88 =10.3182\n",
      "#Loss 89 =70.4513\n",
      "#Loss 90 =54.6464\n",
      "#Loss 91 =63.5161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Loss 92 =51.7515\n",
      "#Loss 93 =44.6275\n",
      "#Loss 94 =37.7841\n",
      "#Loss 95 =27.7154\n",
      "#Loss 96 =9.1473\n",
      "#Loss 97 =4.6074\n",
      "#Loss 98 =2.9181\n",
      "#Loss 99 =3.4650\n",
      "#Loss 100 =24.9689\n",
      "#Loss 101 =101.0124\n",
      "#Loss 102 =431.3847\n",
      "#Loss 103 =15135469.0664\n",
      "#Loss 104 =nan\n",
      " 2 100 绝对误差 tensor(nan, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(nan, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1022, dtype=torch.float64)   实验回归误差 tensor(nan, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 0   条件数 1.0\n",
      "Loss 0 =6.8961\n",
      "#Loss 1 =6.8846\n",
      "#Loss 2 =2.6235\n",
      "#Loss 3 =1.0801\n",
      "#Loss 4 =0.5007\n",
      "#Loss 5 =0.2702\n",
      "#Loss 6 =0.1738\n",
      "#Loss 7 =0.1318\n",
      "#Loss 8 =0.1126\n",
      "#Loss 9 =0.1035\n",
      "#Loss 10 =0.0989\n",
      "#Loss 11 =0.0963\n",
      "#Loss 12 =0.0948\n",
      "#Loss 13 =0.0937\n",
      "#Loss 14 =0.0929\n",
      "#Loss 15 =0.0923\n",
      "#Loss 16 =0.0918\n",
      "#Loss 17 =0.0914\n",
      "#Loss 18 =0.0910\n",
      "#Loss 19 =0.0907\n",
      "#Loss 20 =0.0905\n",
      "#Loss 21 =0.0902\n",
      "#Loss 22 =0.0900\n",
      "#Loss 23 =0.0898\n",
      "#Loss 24 =0.0897\n",
      "#Loss 25 =0.0895\n",
      "#Loss 26 =0.0894\n",
      "#Loss 27 =0.0893\n",
      "#Loss 28 =0.0892\n",
      "#Loss 29 =0.0892\n",
      "#Loss 30 =0.0891\n",
      "#Loss 31 =0.0890\n",
      "#Loss 32 =0.0890\n",
      "#Loss 33 =0.0889\n",
      "#Loss 34 =0.0889\n",
      "#Loss 35 =0.0889\n",
      "#Loss 36 =0.0888\n",
      "#Loss 37 =0.0888\n",
      "#Loss 38 =0.0888\n",
      "#Loss 39 =0.0888\n",
      "#Loss 40 =0.0888\n",
      "#Loss 41 =0.0888\n",
      "#Loss 42 =0.0887\n",
      "#Loss 43 =0.0887\n",
      "#Loss 44 =0.0887\n",
      "#Loss 45 =0.0887\n",
      "#Loss 46 =0.0887\n",
      "#Loss 47 =0.0887\n",
      "#Loss 48 =0.0887\n",
      "#Loss 49 =0.0887\n",
      "#Loss 50 =0.0887\n",
      "#Loss 51 =0.0887\n",
      "#Loss 52 =0.0887\n",
      "#Loss 53 =0.0887\n",
      "#Loss 54 =0.0887\n",
      "#Loss 55 =0.0887\n",
      "#Loss 56 =0.0887\n",
      "#Loss 57 =0.0887\n",
      "#Loss 58 =0.0887\n",
      "#Loss 59 =0.0887\n",
      "#Loss 60 =0.0887\n",
      "#Loss 61 =0.0887\n",
      "#Loss 62 =0.0887\n",
      " 2 100 绝对误差 tensor(0.0515, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.2015, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.3596, dtype=torch.float64)   实验回归误差 tensor(0.1134, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.3416, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =58.0789\n",
      "#Loss 1 =57.2497\n",
      "#Loss 2 =25.2923\n",
      "#Loss 3 =14.2163\n",
      "#Loss 4 =9.1026\n",
      "#Loss 5 =6.3234\n",
      "#Loss 6 =4.7063\n",
      "#Loss 7 =3.7225\n",
      "#Loss 8 =3.0754\n",
      "#Loss 9 =2.6196\n",
      "#Loss 10 =2.2934\n",
      "#Loss 11 =2.0394\n",
      "#Loss 12 =1.8448\n",
      "#Loss 13 =1.6901\n",
      "#Loss 14 =1.5675\n",
      "#Loss 15 =1.4590\n",
      "#Loss 16 =1.3772\n",
      "#Loss 17 =1.3077\n",
      "#Loss 18 =1.2425\n",
      "#Loss 19 =1.1877\n",
      "#Loss 20 =1.1443\n",
      "#Loss 21 =1.1117\n",
      "#Loss 22 =1.0859\n",
      "#Loss 23 =1.0642\n",
      "#Loss 24 =1.0429\n",
      "#Loss 25 =1.0203\n",
      "#Loss 26 =0.9967\n",
      "#Loss 27 =0.9704\n",
      "#Loss 28 =0.9483\n",
      "#Loss 29 =0.9309\n",
      "#Loss 30 =0.9191\n",
      "#Loss 31 =0.9108\n",
      "#Loss 32 =0.9047\n",
      "#Loss 33 =0.9001\n",
      "#Loss 34 =0.8945\n",
      "#Loss 35 =0.8895\n",
      "#Loss 36 =0.8851\n",
      "#Loss 37 =0.8791\n",
      "#Loss 38 =0.8742\n",
      "#Loss 39 =0.8653\n",
      "#Loss 40 =0.8540\n",
      "#Loss 41 =0.8467\n",
      "#Loss 42 =0.8415\n",
      "#Loss 43 =0.8370\n",
      "#Loss 44 =0.8299\n",
      "#Loss 45 =0.8240\n",
      "#Loss 46 =0.8188\n",
      "#Loss 47 =0.8136\n",
      "#Loss 48 =0.8101\n",
      "#Loss 49 =0.8077\n",
      "#Loss 50 =0.8049\n",
      "#Loss 51 =0.8024\n",
      "#Loss 52 =0.7966\n",
      "#Loss 53 =0.7868\n",
      "#Loss 54 =0.7775\n",
      "#Loss 55 =0.7661\n",
      "#Loss 56 =0.7569\n",
      "#Loss 57 =0.7463\n",
      "#Loss 58 =0.7382\n",
      "#Loss 59 =0.7299\n",
      "#Loss 60 =0.7191\n",
      "#Loss 61 =0.7095\n",
      "#Loss 62 =0.7031\n",
      "#Loss 63 =0.6991\n",
      "#Loss 64 =0.6963\n",
      "#Loss 65 =0.6941\n",
      "#Loss 66 =0.6915\n",
      "#Loss 67 =0.6855\n",
      "#Loss 68 =0.6813\n",
      "#Loss 69 =0.6759\n",
      "#Loss 70 =0.6648\n",
      "#Loss 71 =0.6559\n",
      "#Loss 72 =0.6477\n",
      "#Loss 73 =0.6415\n",
      "#Loss 74 =0.6379\n",
      "#Loss 75 =0.6356\n",
      "#Loss 76 =0.6341\n",
      "#Loss 77 =0.6329\n",
      "#Loss 78 =0.6314\n",
      "#Loss 79 =0.6300\n",
      "#Loss 80 =0.6290\n",
      "#Loss 81 =0.6279\n",
      "#Loss 82 =0.6261\n",
      "#Loss 83 =0.6250\n",
      "#Loss 84 =0.6239\n",
      "#Loss 85 =0.6233\n",
      "#Loss 86 =0.6227\n",
      "#Loss 87 =0.6224\n",
      "#Loss 88 =0.6220\n",
      "#Loss 89 =0.6216\n",
      "#Loss 90 =0.6214\n",
      "#Loss 91 =0.6212\n",
      "#Loss 92 =0.6211\n",
      "#Loss 93 =0.6210\n",
      "#Loss 94 =0.6210\n",
      "#Loss 95 =0.6210\n",
      "#Loss 96 =0.6210\n",
      "#Loss 97 =0.6209\n",
      "#Loss 98 =0.6209\n",
      " 2 100 绝对误差 tensor(0.2247, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.1545, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1294, dtype=torch.float64)   实验回归误差 tensor(0.1034, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.3638, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =251.3216\n",
      "#Loss 1 =249.1602\n",
      "#Loss 2 =155.6839\n",
      "#Loss 3 =81.1743\n",
      "#Loss 4 =1.7626\n",
      "#Loss 5 =1.3576\n",
      "#Loss 6 =7.6089\n",
      "#Loss 7 =57.0700\n",
      "#Loss 8 =14.1548\n",
      "#Loss 9 =89.6715\n",
      "#Loss 10 =14.1444\n",
      "#Loss 11 =91.1017\n",
      "#Loss 12 =369.8910\n",
      "#Loss 13 =135.2413\n",
      "#Loss 14 =127.1482\n",
      "#Loss 15 =126.8535\n",
      "#Loss 16 =126.8173\n",
      "#Loss 17 =126.8090\n",
      "#Loss 18 =126.8038\n",
      "#Loss 19 =126.8000\n",
      "#Loss 20 =126.7978\n",
      "#Loss 21 =126.7958\n",
      "#Loss 22 =126.7943\n",
      "#Loss 23 =126.7920\n",
      "#Loss 24 =126.7862\n",
      "#Loss 25 =126.7743\n",
      "#Loss 26 =126.7661\n",
      "#Loss 27 =126.7581\n",
      "#Loss 28 =126.7386\n",
      "#Loss 29 =126.7245\n",
      "#Loss 30 =126.7025\n",
      "#Loss 31 =126.6729\n",
      "#Loss 32 =126.6474\n",
      "#Loss 33 =126.6184\n",
      "#Loss 34 =126.5929\n",
      "#Loss 35 =126.5668\n",
      "#Loss 36 =126.5346\n",
      "#Loss 37 =126.4832\n",
      "#Loss 38 =126.3987\n",
      "#Loss 39 =126.3238\n",
      "#Loss 40 =126.2176\n",
      "#Loss 41 =125.9312\n",
      "#Loss 42 =125.3611\n",
      "#Loss 43 =124.5567\n",
      "#Loss 44 =120.5891\n",
      "#Loss 45 =108.3322\n",
      "#Loss 46 =105.2831\n",
      "#Loss 47 =105.1872\n",
      "#Loss 48 =105.1733\n",
      "#Loss 49 =105.1719\n",
      "#Loss 50 =105.1688\n",
      "#Loss 51 =105.1672\n",
      "#Loss 52 =105.1671\n",
      " 2 100 绝对误差 tensor(1.9328, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.6471, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0612, dtype=torch.float64)   实验回归误差 tensor(0.6469, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.3897, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 98   条件数 1.0\n",
      "Loss 0 =19.2814\n",
      "#Loss 1 =19.2134\n",
      "#Loss 2 =7.5473\n",
      "#Loss 3 =3.6653\n",
      "#Loss 4 =2.1815\n",
      "#Loss 5 =1.5176\n",
      "#Loss 6 =1.1907\n",
      "#Loss 7 =1.0221\n",
      "#Loss 8 =0.9337\n",
      "#Loss 9 =0.8871\n",
      "#Loss 10 =0.8626\n",
      "#Loss 11 =0.8498\n",
      "#Loss 12 =0.8430\n",
      "#Loss 13 =0.8395\n",
      "#Loss 14 =0.8376\n",
      "#Loss 15 =0.8365\n",
      "#Loss 16 =0.8359\n",
      "#Loss 17 =0.8356\n",
      "#Loss 18 =0.8354\n",
      "#Loss 19 =0.8353\n",
      "#Loss 20 =0.8352\n",
      "#Loss 21 =0.8351\n",
      "#Loss 22 =0.8351\n",
      "#Loss 23 =0.8351\n",
      "#Loss 24 =0.8351\n",
      "#Loss 25 =0.8351\n",
      "#Loss 26 =0.8350\n",
      "#Loss 27 =0.8350\n",
      " 2 100 绝对误差 tensor(0.9200, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.9613, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.2405, dtype=torch.float64)   实验回归误差 tensor(0.2081, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4071, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =59.2232\n",
      "#Loss 1 =58.8265\n",
      "#Loss 2 =21.8604\n",
      "#Loss 3 =11.2967\n",
      "#Loss 4 =6.4074\n",
      "#Loss 5 =3.8161\n",
      "#Loss 6 =2.4856\n",
      "#Loss 7 =1.7999\n",
      "#Loss 8 =1.4151\n",
      "#Loss 9 =1.1723\n",
      "#Loss 10 =1.0019\n",
      "#Loss 11 =0.8740\n",
      "#Loss 12 =0.7724\n",
      "#Loss 13 =0.6916\n",
      "#Loss 14 =0.6266\n",
      "#Loss 15 =0.5746\n",
      "#Loss 16 =0.5322\n",
      "#Loss 17 =0.4995\n",
      "#Loss 18 =0.4725\n",
      "#Loss 19 =0.4492\n",
      "#Loss 20 =0.4312\n",
      "#Loss 21 =0.4169\n",
      "#Loss 22 =0.4042\n",
      "#Loss 23 =0.3936\n",
      "#Loss 24 =0.3855\n",
      "#Loss 25 =0.3790\n",
      "#Loss 26 =0.3732\n",
      "#Loss 27 =0.3682\n",
      "#Loss 28 =0.3632\n",
      "#Loss 29 =0.3589\n",
      "#Loss 30 =0.3555\n",
      "#Loss 31 =0.3522\n",
      "#Loss 32 =0.3497\n",
      "#Loss 33 =0.3478\n",
      "#Loss 34 =0.3465\n",
      "#Loss 35 =0.3454\n",
      "#Loss 36 =0.3445\n",
      "#Loss 37 =0.3433\n",
      "#Loss 38 =0.3422\n",
      "#Loss 39 =0.3412\n",
      "#Loss 40 =0.3400\n",
      "#Loss 41 =0.3381\n",
      "#Loss 42 =0.3365\n",
      "#Loss 43 =0.3351\n",
      "#Loss 44 =0.3341\n",
      "#Loss 45 =0.3334\n",
      "#Loss 46 =0.3328\n",
      "#Loss 47 =0.3323\n",
      "#Loss 48 =0.3320\n",
      "#Loss 49 =0.3317\n",
      "#Loss 50 =0.3315\n",
      "#Loss 51 =0.3314\n",
      "#Loss 52 =0.3312\n",
      "#Loss 53 =0.3310\n",
      "#Loss 54 =0.3308\n",
      "#Loss 55 =0.3307\n",
      "#Loss 56 =0.3305\n",
      "#Loss 57 =0.3305\n",
      "#Loss 58 =0.3304\n",
      "#Loss 59 =0.3304\n",
      "#Loss 60 =0.3303\n",
      "#Loss 61 =0.3303\n",
      "#Loss 62 =0.3303\n",
      "#Loss 63 =0.3303\n",
      "#Loss 64 =0.3303\n",
      "#Loss 65 =0.3303\n",
      "#Loss 66 =0.3303\n",
      "#Loss 67 =0.3303\n",
      "#Loss 68 =0.3302\n",
      " 2 100 绝对误差 tensor(0.0235, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.0234, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1273, dtype=torch.float64)   实验回归误差 tensor(0.0747, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.3491, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =3.0603\n",
      "#Loss 1 =2.9755\n",
      "#Loss 2 =1.2705\n",
      "#Loss 3 =0.6058\n",
      "#Loss 4 =0.3426\n",
      "#Loss 5 =0.2356\n",
      "#Loss 6 =0.1907\n",
      "#Loss 7 =0.1708\n",
      "#Loss 8 =0.1613\n",
      "#Loss 9 =0.1560\n",
      "#Loss 10 =0.1526\n",
      "#Loss 11 =0.1501\n",
      "#Loss 12 =0.1479\n",
      "#Loss 13 =0.1459\n",
      "#Loss 14 =0.1440\n",
      "#Loss 15 =0.1423\n",
      "#Loss 16 =0.1407\n",
      "#Loss 17 =0.1391\n",
      "#Loss 18 =0.1376\n",
      "#Loss 19 =0.1361\n",
      "#Loss 20 =0.1347\n",
      "#Loss 21 =0.1334\n",
      "#Loss 22 =0.1322\n",
      "#Loss 23 =0.1310\n",
      "#Loss 24 =0.1298\n",
      "#Loss 25 =0.1287\n",
      "#Loss 26 =0.1276\n",
      "#Loss 27 =0.1266\n",
      "#Loss 28 =0.1257\n",
      "#Loss 29 =0.1247\n",
      "#Loss 30 =0.1238\n",
      "#Loss 31 =0.1230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Loss 32 =0.1222\n",
      "#Loss 33 =0.1214\n",
      "#Loss 34 =0.1207\n",
      "#Loss 35 =0.1199\n",
      "#Loss 36 =0.1193\n",
      "#Loss 37 =0.1186\n",
      "#Loss 38 =0.1180\n",
      "#Loss 39 =0.1174\n",
      "#Loss 40 =0.1168\n",
      "#Loss 41 =0.1162\n",
      "#Loss 42 =0.1157\n",
      "#Loss 43 =0.1152\n",
      "#Loss 44 =0.1147\n",
      "#Loss 45 =0.1143\n",
      "#Loss 46 =0.1138\n",
      "#Loss 47 =0.1134\n",
      "#Loss 48 =0.1130\n",
      "#Loss 49 =0.1126\n",
      "#Loss 50 =0.1122\n",
      "#Loss 51 =0.1118\n",
      "#Loss 52 =0.1115\n",
      "#Loss 53 =0.1111\n",
      "#Loss 54 =0.1108\n",
      "#Loss 55 =0.1105\n",
      "#Loss 56 =0.1102\n",
      "#Loss 57 =0.1099\n",
      "#Loss 58 =0.1097\n",
      "#Loss 59 =0.1094\n",
      "#Loss 60 =0.1092\n",
      "#Loss 61 =0.1089\n",
      "#Loss 62 =0.1087\n",
      "#Loss 63 =0.1085\n",
      "#Loss 64 =0.1083\n",
      "#Loss 65 =0.1080\n",
      "#Loss 66 =0.1079\n",
      "#Loss 67 =0.1077\n",
      "#Loss 68 =0.1075\n",
      "#Loss 69 =0.1073\n",
      "#Loss 70 =0.1071\n",
      "#Loss 71 =0.1070\n",
      "#Loss 72 =0.1068\n",
      "#Loss 73 =0.1067\n",
      "#Loss 74 =0.1065\n",
      "#Loss 75 =0.1064\n",
      "#Loss 76 =0.1063\n",
      "#Loss 77 =0.1061\n",
      "#Loss 78 =0.1060\n",
      "#Loss 79 =0.1059\n",
      "#Loss 80 =0.1058\n",
      "#Loss 81 =0.1057\n",
      "#Loss 82 =0.1056\n",
      "#Loss 83 =0.1055\n",
      "#Loss 84 =0.1054\n",
      "#Loss 85 =0.1053\n",
      "#Loss 86 =0.1052\n",
      "#Loss 87 =0.1051\n",
      "#Loss 88 =0.1050\n",
      "#Loss 89 =0.1050\n",
      "#Loss 90 =0.1049\n",
      "#Loss 91 =0.1048\n",
      "#Loss 92 =0.1047\n",
      "#Loss 93 =0.1047\n",
      "#Loss 94 =0.1046\n",
      "#Loss 95 =0.1045\n",
      "#Loss 96 =0.1045\n",
      "#Loss 97 =0.1044\n",
      "#Loss 98 =0.1044\n",
      "#Loss 99 =0.1043\n",
      "#Loss 100 =0.1043\n",
      "#Loss 101 =0.1042\n",
      "#Loss 102 =0.1042\n",
      "#Loss 103 =0.1041\n",
      "#Loss 104 =0.1041\n",
      "#Loss 105 =0.1040\n",
      "#Loss 106 =0.1040\n",
      "#Loss 107 =0.1040\n",
      "#Loss 108 =0.1039\n",
      "#Loss 109 =0.1039\n",
      "#Loss 110 =0.1038\n",
      "#Loss 111 =0.1038\n",
      "#Loss 112 =0.1038\n",
      "#Loss 113 =0.1038\n",
      "#Loss 114 =0.1037\n",
      "#Loss 115 =0.1037\n",
      "#Loss 116 =0.1037\n",
      "#Loss 117 =0.1036\n",
      "#Loss 118 =0.1036\n",
      "#Loss 119 =0.1036\n",
      "#Loss 120 =0.1036\n",
      "#Loss 121 =0.1035\n",
      "#Loss 122 =0.1035\n",
      "#Loss 123 =0.1035\n",
      "#Loss 124 =0.1035\n",
      "#Loss 125 =0.1035\n",
      "#Loss 126 =0.1034\n",
      "#Loss 127 =0.1034\n",
      "#Loss 128 =0.1034\n",
      "#Loss 129 =0.1034\n",
      "#Loss 130 =0.1034\n",
      "#Loss 131 =0.1033\n",
      "#Loss 132 =0.1033\n",
      "#Loss 133 =0.1033\n",
      "#Loss 134 =0.1033\n",
      "#Loss 135 =0.1033\n",
      "#Loss 136 =0.1033\n",
      "#Loss 137 =0.1033\n",
      "#Loss 138 =0.1033\n",
      "#Loss 139 =0.1032\n",
      "#Loss 140 =0.1032\n",
      "#Loss 141 =0.1032\n",
      "#Loss 142 =0.1032\n",
      "#Loss 143 =0.1032\n",
      "#Loss 144 =0.1032\n",
      "#Loss 145 =0.1032\n",
      "#Loss 146 =0.1032\n",
      "#Loss 147 =0.1032\n",
      "#Loss 148 =0.1032\n",
      "#Loss 149 =0.1031\n",
      "#Loss 150 =0.1031\n",
      "超过迭代上限\n",
      " 2 100 绝对误差 tensor(0.0631, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.2843, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.6211, dtype=torch.float64)   实验回归误差 tensor(0.1836, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4071, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =59.0347\n",
      "#Loss 1 =58.9978\n",
      "#Loss 2 =23.7898\n",
      "#Loss 3 =12.7428\n",
      "#Loss 4 =7.8163\n",
      "#Loss 5 =5.1717\n",
      "#Loss 6 =3.6750\n",
      "#Loss 7 =2.8040\n",
      "#Loss 8 =2.2603\n",
      "#Loss 9 =1.9108\n",
      "#Loss 10 =1.6667\n",
      "#Loss 11 =1.4863\n",
      "#Loss 12 =1.3485\n",
      "#Loss 13 =1.2421\n",
      "#Loss 14 =1.1526\n",
      "#Loss 15 =1.0839\n",
      "#Loss 16 =1.0291\n",
      "#Loss 17 =0.9851\n",
      "#Loss 18 =0.9497\n",
      "#Loss 19 =0.9239\n",
      "#Loss 20 =0.9034\n",
      "#Loss 21 =0.8852\n",
      "#Loss 22 =0.8720\n",
      "#Loss 23 =0.8607\n",
      "#Loss 24 =0.8508\n",
      "#Loss 25 =0.8424\n",
      "#Loss 26 =0.8357\n",
      "#Loss 27 =0.8296\n",
      "#Loss 28 =0.8252\n",
      "#Loss 29 =0.8210\n",
      "#Loss 30 =0.8168\n",
      "#Loss 31 =0.8127\n",
      "#Loss 32 =0.7952\n",
      "#Loss 33 =0.7553\n",
      "#Loss 34 =0.7140\n",
      "#Loss 35 =0.6837\n",
      "#Loss 36 =0.6603\n",
      "#Loss 37 =0.6418\n",
      "#Loss 38 =0.6266\n",
      "#Loss 39 =0.6147\n",
      "#Loss 40 =0.6054\n",
      "#Loss 41 =0.5981\n",
      "#Loss 42 =0.5921\n",
      "#Loss 43 =0.5851\n",
      "#Loss 44 =0.5777\n",
      "#Loss 45 =0.5712\n",
      "#Loss 46 =0.5666\n",
      "#Loss 47 =0.5628\n",
      "#Loss 48 =0.5593\n",
      "#Loss 49 =0.5561\n",
      "#Loss 50 =0.5523\n",
      "#Loss 51 =0.5497\n",
      "#Loss 52 =0.5479\n",
      "#Loss 53 =0.5457\n",
      "#Loss 54 =0.5421\n",
      "#Loss 55 =0.5356\n",
      "#Loss 56 =0.5300\n",
      "#Loss 57 =0.5221\n",
      "#Loss 58 =0.5026\n",
      "#Loss 59 =0.4887\n",
      "#Loss 60 =0.4785\n",
      "#Loss 61 =0.4717\n",
      "#Loss 62 =0.4667\n",
      "#Loss 63 =0.4623\n",
      "#Loss 64 =0.4591\n",
      "#Loss 65 =0.4568\n",
      "#Loss 66 =0.4544\n",
      "#Loss 67 =0.4519\n",
      "#Loss 68 =0.4497\n",
      "#Loss 69 =0.4473\n",
      "#Loss 70 =0.4452\n",
      "#Loss 71 =0.4431\n",
      "#Loss 72 =0.4416\n",
      "#Loss 73 =0.4387\n",
      "#Loss 74 =0.4345\n",
      "#Loss 75 =0.4299\n",
      "#Loss 76 =0.4236\n",
      "#Loss 77 =0.4187\n",
      "#Loss 78 =0.4155\n",
      "#Loss 79 =0.4134\n",
      "#Loss 80 =0.4120\n",
      "#Loss 81 =0.4110\n",
      "#Loss 82 =0.4096\n",
      "#Loss 83 =0.4073\n",
      "#Loss 84 =0.4046\n",
      "#Loss 85 =0.4006\n",
      "#Loss 86 =0.3972\n",
      "#Loss 87 =0.3951\n",
      "#Loss 88 =0.3936\n",
      "#Loss 89 =0.3919\n",
      "#Loss 90 =0.3904\n",
      "#Loss 91 =0.3884\n",
      "#Loss 92 =0.3866\n",
      "#Loss 93 =0.3854\n",
      "#Loss 94 =0.3846\n",
      "#Loss 95 =0.3841\n",
      "#Loss 96 =0.3835\n",
      "#Loss 97 =0.3832\n",
      "#Loss 98 =0.3829\n",
      "#Loss 99 =0.3827\n",
      "#Loss 100 =0.3825\n",
      "#Loss 101 =0.3824\n",
      "#Loss 102 =0.3824\n",
      "#Loss 103 =0.3823\n",
      "#Loss 104 =0.3823\n",
      "#Loss 105 =0.3823\n",
      "#Loss 106 =0.3823\n",
      "#Loss 107 =0.3823\n",
      "#Loss 108 =0.3823\n",
      "#Loss 109 =0.3823\n",
      " 2 100 绝对误差 tensor(0.2677, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.1827, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1296, dtype=torch.float64)   实验回归误差 tensor(0.0805, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =7426008.4438\n",
      "#Loss 1 =7425102.2607\n",
      "#Loss 2 =96605998.1735\n",
      "#Loss 3 =nan\n",
      " 2 100 绝对误差 tensor(nan, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(nan, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0004, dtype=torch.float64)   实验回归误差 tensor(nan, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 0   条件数 1.0\n",
      "Loss 0 =211.0820\n",
      "#Loss 1 =186.6737\n",
      "#Loss 2 =81.8042\n",
      "#Loss 3 =18.7579\n",
      "#Loss 4 =9.9864\n",
      "#Loss 5 =25.3125\n",
      "#Loss 6 =2.5653\n",
      "#Loss 7 =5.1827\n",
      "#Loss 8 =5.5110\n",
      "#Loss 9 =14.3281\n",
      "#Loss 10 =4.7712\n",
      "#Loss 11 =11.8084\n",
      "#Loss 12 =5.2785\n",
      "#Loss 13 =13.6385\n",
      "#Loss 14 =4.3959\n",
      "#Loss 15 =11.0510\n",
      "#Loss 16 =4.9582\n",
      "#Loss 17 =11.9142\n",
      "#Loss 18 =4.8008\n",
      "#Loss 19 =12.4563\n",
      "#Loss 20 =4.5330\n",
      "#Loss 21 =11.4926\n",
      "#Loss 22 =4.9535\n",
      "#Loss 23 =12.5262\n",
      "#Loss 24 =4.8199\n",
      "#Loss 25 =12.0673\n",
      "#Loss 26 =4.8209\n",
      "#Loss 27 =11.9685\n",
      "#Loss 28 =4.7831\n",
      "#Loss 29 =12.0153\n",
      "#Loss 30 =4.9159\n",
      "#Loss 31 =12.3497\n",
      "#Loss 32 =4.7644\n",
      "#Loss 33 =11.8059\n",
      "#Loss 34 =4.7348\n",
      "#Loss 35 =12.0432\n",
      "#Loss 36 =4.8655\n",
      "#Loss 37 =12.2911\n",
      "#Loss 38 =4.7830\n",
      "#Loss 39 =12.0564\n",
      "#Loss 40 =4.9363\n",
      "#Loss 41 =12.4551\n",
      "#Loss 42 =4.7141\n",
      "#Loss 43 =11.8347\n",
      "#Loss 44 =4.8826\n",
      "#Loss 45 =12.1272\n",
      "#Loss 46 =4.8098\n",
      "#Loss 47 =11.9462\n",
      "#Loss 48 =4.7704\n",
      "#Loss 49 =12.2557\n",
      "#Loss 50 =4.9379\n",
      "#Loss 51 =12.4460\n",
      "#Loss 52 =4.7511\n",
      "#Loss 53 =11.9992\n",
      "#Loss 54 =4.9232\n",
      "#Loss 55 =12.3738\n",
      "#Loss 56 =4.7696\n",
      "#Loss 57 =11.8388\n",
      "#Loss 58 =4.7477\n",
      "#Loss 59 =12.1127\n",
      "#Loss 60 =4.8887\n",
      "#Loss 61 =12.1714\n",
      "#Loss 62 =4.8260\n",
      "#Loss 63 =12.0350\n",
      "#Loss 64 =4.8001\n",
      "#Loss 65 =12.1142\n",
      "#Loss 66 =4.9488\n",
      "#Loss 67 =12.5337\n",
      "#Loss 68 =4.7396\n",
      "#Loss 69 =11.9736\n",
      "#Loss 70 =4.9200\n",
      "#Loss 71 =12.3482\n",
      "#Loss 72 =4.7607\n",
      "#Loss 73 =12.0347\n",
      "#Loss 74 =4.9329\n",
      "#Loss 75 =12.4308\n",
      "#Loss 76 =4.7481\n",
      "#Loss 77 =11.9818\n",
      "#Loss 78 =4.9275\n",
      "#Loss 79 =12.3842\n",
      "#Loss 80 =4.7710\n",
      "#Loss 81 =11.8494\n",
      "#Loss 82 =4.7521\n",
      "#Loss 83 =12.1360\n",
      "#Loss 84 =4.8963\n",
      "#Loss 85 =12.2146\n",
      "#Loss 86 =4.8404\n",
      "#Loss 87 =12.1159\n",
      "#Loss 88 =4.8279\n",
      "#Loss 89 =12.0180\n",
      "#Loss 90 =4.7918\n",
      "#Loss 91 =12.0745\n",
      "#Loss 92 =4.9380\n",
      "#Loss 93 =12.4702\n",
      "#Loss 94 =4.7196\n",
      "#Loss 95 =11.8649\n",
      "#Loss 96 =4.8933\n",
      "#Loss 97 =12.1868\n",
      "#Loss 98 =4.8296\n",
      "#Loss 99 =12.0573\n",
      "#Loss 100 =4.8083\n",
      "#Loss 101 =12.1587\n",
      "#Loss 102 =4.8091\n",
      "#Loss 103 =12.1249\n",
      "#Loss 104 =4.9474\n",
      "#Loss 105 =12.5326\n",
      "#Loss 106 =4.7402\n",
      "#Loss 107 =11.9754\n",
      "#Loss 108 =4.9204\n",
      "#Loss 109 =12.3510\n",
      "#Loss 110 =4.7615\n",
      "#Loss 111 =11.7942\n",
      "#Loss 112 =4.7311\n",
      "#Loss 113 =12.0221\n",
      "#Loss 114 =4.9015\n",
      "#Loss 115 =12.2007\n",
      "#Loss 116 =4.8297\n",
      "#Loss 117 =12.0638\n",
      "#Loss 118 =4.8114\n",
      "#Loss 119 =12.1749\n",
      "#Loss 120 =4.8143\n",
      "#Loss 121 =12.1517\n",
      "#Loss 122 =4.8008\n",
      "#Loss 123 =12.0861\n",
      "#Loss 124 =4.9363\n",
      "#Loss 125 =12.4681\n",
      "#Loss 126 =4.7199\n",
      "#Loss 127 =11.8655\n",
      "#Loss 128 =4.8933\n",
      "#Loss 129 =12.1871\n",
      "#Loss 130 =4.8297\n",
      "#Loss 131 =12.0580\n",
      "#Loss 132 =4.8085\n",
      "#Loss 133 =12.1600\n",
      "#Loss 134 =4.8095\n",
      "#Loss 135 =12.1274\n",
      "#Loss 136 =4.9483\n",
      "#Loss 137 =12.5373\n",
      "#Loss 138 =4.7416\n",
      "#Loss 139 =11.7393\n",
      "#Loss 140 =4.7186\n",
      "#Loss 141 =11.9448\n",
      "#Loss 142 =4.8733\n",
      "#Loss 143 =12.2991\n",
      "#Loss 144 =4.7812\n",
      "#Loss 145 =12.0522\n",
      "#Loss 146 =4.9358\n",
      "#Loss 147 =12.4509\n",
      "#Loss 148 =4.7126\n",
      "#Loss 149 =11.8265\n",
      "#Loss 150 =4.8796\n",
      "超过迭代上限\n",
      " 2 100 绝对误差 tensor(0.1675, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.1281, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0602, dtype=torch.float64)   实验回归误差 tensor(0.2889, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.3266, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =35.0569\n",
      "#Loss 1 =34.9589\n",
      "#Loss 2 =14.9410\n",
      "#Loss 3 =7.5425\n",
      "#Loss 4 =4.4522\n",
      "#Loss 5 =2.9449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Loss 6 =2.1203\n",
      "#Loss 7 =1.6287\n",
      "#Loss 8 =1.3159\n",
      "#Loss 9 =1.1071\n",
      "#Loss 10 =0.9629\n",
      "#Loss 11 =0.8592\n",
      "#Loss 12 =0.7799\n",
      "#Loss 13 =0.7180\n",
      "#Loss 14 =0.6714\n",
      "#Loss 15 =0.6332\n",
      "#Loss 16 =0.5976\n",
      "#Loss 17 =0.5710\n",
      "#Loss 18 =0.5510\n",
      "#Loss 19 =0.5359\n",
      "#Loss 20 =0.5244\n",
      "#Loss 21 =0.5155\n",
      "#Loss 22 =0.5082\n",
      "#Loss 23 =0.5025\n",
      "#Loss 24 =0.4981\n",
      "#Loss 25 =0.4944\n",
      "#Loss 26 =0.4916\n",
      "#Loss 27 =0.4894\n",
      "#Loss 28 =0.4877\n",
      "#Loss 29 =0.4864\n",
      "#Loss 30 =0.4854\n",
      "#Loss 31 =0.4846\n",
      "#Loss 32 =0.4840\n",
      "#Loss 33 =0.4836\n",
      "#Loss 34 =0.4832\n",
      "#Loss 35 =0.4829\n",
      "#Loss 36 =0.4823\n",
      "#Loss 37 =0.4818\n",
      "#Loss 38 =0.4814\n",
      "#Loss 39 =0.4811\n",
      "#Loss 40 =0.4809\n",
      "#Loss 41 =0.4807\n",
      "#Loss 42 =0.4805\n",
      "#Loss 43 =0.4804\n",
      "#Loss 44 =0.4803\n",
      "#Loss 45 =0.4802\n",
      "#Loss 46 =0.4797\n",
      "#Loss 47 =0.4793\n",
      "#Loss 48 =0.4789\n",
      "#Loss 49 =0.4780\n",
      "#Loss 50 =0.4773\n",
      "#Loss 51 =0.4768\n",
      "#Loss 52 =0.4764\n",
      "#Loss 53 =0.4761\n",
      "#Loss 54 =0.4759\n",
      "#Loss 55 =0.4757\n",
      "#Loss 56 =0.4755\n",
      "#Loss 57 =0.4754\n",
      "#Loss 58 =0.4753\n",
      "#Loss 59 =0.4752\n",
      "#Loss 60 =0.4751\n",
      "#Loss 61 =0.4750\n",
      "#Loss 62 =0.4749\n",
      "#Loss 63 =0.4748\n",
      "#Loss 64 =0.4748\n",
      "#Loss 65 =0.4747\n",
      "#Loss 66 =0.4747\n",
      "#Loss 67 =0.4747\n",
      "#Loss 68 =0.4747\n",
      "#Loss 69 =0.4746\n",
      "#Loss 70 =0.4746\n",
      "#Loss 71 =0.4746\n",
      "#Loss 72 =0.4746\n",
      "#Loss 73 =0.4746\n",
      "#Loss 74 =0.4746\n",
      "#Loss 75 =0.4746\n",
      " 2 100 绝对误差 tensor(1.5828, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.9915, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1777, dtype=torch.float64)   实验回归误差 tensor(0.1164, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4070, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =23.6326\n",
      "#Loss 1 =23.4554\n",
      "#Loss 2 =9.1882\n",
      "#Loss 3 =4.1944\n",
      "#Loss 4 =2.1842\n",
      "#Loss 5 =1.2328\n",
      "#Loss 6 =0.7377\n",
      "#Loss 7 =0.4676\n",
      "#Loss 8 =0.3171\n",
      "#Loss 9 =0.2320\n",
      "#Loss 10 =0.1835\n",
      "#Loss 11 =0.1555\n",
      "#Loss 12 =0.1391\n",
      "#Loss 13 =0.1293\n",
      "#Loss 14 =0.1233\n",
      "#Loss 15 =0.1196\n",
      "#Loss 16 =0.1172\n",
      "#Loss 17 =0.1157\n",
      "#Loss 18 =0.1147\n",
      "#Loss 19 =0.1140\n",
      "#Loss 20 =0.1136\n",
      "#Loss 21 =0.1132\n",
      "#Loss 22 =0.1130\n",
      "#Loss 23 =0.1129\n",
      "#Loss 24 =0.1128\n",
      "#Loss 25 =0.1127\n",
      "#Loss 26 =0.1126\n",
      "#Loss 27 =0.1126\n",
      "#Loss 28 =0.1126\n",
      "#Loss 29 =0.1125\n",
      "#Loss 30 =0.1125\n",
      "#Loss 31 =0.1125\n",
      "#Loss 32 =0.1125\n",
      "#Loss 33 =0.1125\n",
      "#Loss 34 =0.1125\n",
      "#Loss 35 =0.1125\n",
      "#Loss 36 =0.1125\n",
      "#Loss 37 =0.1125\n",
      "#Loss 38 =0.1125\n",
      " 2 100 绝对误差 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.0129, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.2036, dtype=torch.float64)   实验回归误差 tensor(0.0690, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.2961, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =24.9110\n",
      "#Loss 1 =24.6430\n",
      "#Loss 2 =9.1653\n",
      "#Loss 3 =4.7123\n",
      "#Loss 4 =2.4365\n",
      "#Loss 5 =1.2463\n",
      "#Loss 6 =0.7701\n",
      "#Loss 7 =0.6406\n",
      "#Loss 8 =0.6151\n",
      "#Loss 9 =0.6096\n",
      "#Loss 10 =0.6078\n",
      "#Loss 11 =0.6069\n",
      "#Loss 12 =0.6064\n",
      "#Loss 13 =0.6061\n",
      "#Loss 14 =0.6059\n",
      "#Loss 15 =0.6058\n",
      "#Loss 16 =0.6057\n",
      "#Loss 17 =0.6057\n",
      "#Loss 18 =0.6056\n",
      "#Loss 19 =0.6056\n",
      "#Loss 20 =0.6056\n",
      "#Loss 21 =0.6056\n",
      " 2 100 绝对误差 tensor(0.9601, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.8526, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1764, dtype=torch.float64)   实验回归误差 tensor(0.1559, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4071, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =16.7333\n",
      "#Loss 1 =16.7064\n",
      "#Loss 2 =5.3322\n",
      "#Loss 3 =2.1540\n",
      "#Loss 4 =1.0893\n",
      "#Loss 5 =0.6615\n",
      "#Loss 6 =0.4706\n",
      "#Loss 7 =0.3807\n",
      "#Loss 8 =0.3371\n",
      "#Loss 9 =0.3155\n",
      "#Loss 10 =0.3045\n",
      "#Loss 11 =0.2987\n",
      "#Loss 12 =0.2955\n",
      "#Loss 13 =0.2936\n",
      "#Loss 14 =0.2924\n",
      "#Loss 15 =0.2917\n",
      "#Loss 16 =0.2912\n",
      "#Loss 17 =0.2908\n",
      "#Loss 18 =0.2906\n",
      "#Loss 19 =0.2904\n",
      "#Loss 20 =0.2903\n",
      "#Loss 21 =0.2902\n",
      "#Loss 22 =0.2901\n",
      "#Loss 23 =0.2901\n",
      "#Loss 24 =0.2900\n",
      "#Loss 25 =0.2900\n",
      "#Loss 26 =0.2900\n",
      "#Loss 27 =0.2900\n",
      "#Loss 28 =0.2900\n",
      "#Loss 29 =0.2900\n",
      "#Loss 30 =0.2900\n",
      "#Loss 31 =0.2899\n",
      "#Loss 32 =0.2899\n",
      " 2 100 绝对误差 tensor(0.8157, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.9528, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.2259, dtype=torch.float64)   实验回归误差 tensor(0.1316, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =1033.1134\n",
      "#Loss 1 =1032.9928\n",
      "#Loss 2 =841.2037\n",
      "#Loss 3 =807.6128\n",
      "#Loss 4 =644.6995\n",
      "#Loss 5 =1984.5666\n",
      "#Loss 6 =953.2007\n",
      "#Loss 7 =979.2874\n",
      "#Loss 8 =984.0551\n",
      "#Loss 9 =952.0181\n",
      "#Loss 10 =940.8298\n",
      "#Loss 11 =977.1551\n",
      "#Loss 12 =933.9570\n",
      "#Loss 13 =965.1972\n",
      "#Loss 14 =981.0856\n",
      "#Loss 15 =981.3433\n",
      "#Loss 16 =981.5769\n",
      "#Loss 17 =1003.0865\n",
      "#Loss 18 =994.2177\n",
      "#Loss 19 =993.6692\n",
      "#Loss 20 =990.6055\n",
      "#Loss 21 =969.3802\n",
      "#Loss 22 =990.3100\n",
      "#Loss 23 =988.3951\n",
      "#Loss 24 =986.6917\n",
      "#Loss 25 =985.7388\n",
      "#Loss 26 =987.5080\n",
      "#Loss 27 =976.4124\n",
      "#Loss 28 =987.8635\n",
      "#Loss 29 =974.6347\n",
      "#Loss 30 =985.5836\n",
      "#Loss 31 =971.1418\n",
      "#Loss 32 =978.5168\n",
      "#Loss 33 =981.9158\n",
      "#Loss 34 =980.7179\n",
      "#Loss 35 =945.4815\n",
      "#Loss 36 =988.4456\n",
      "#Loss 37 =1002.5294\n",
      "#Loss 38 =1006.0000\n",
      "#Loss 39 =1005.6243\n",
      "#Loss 40 =1005.2414\n",
      "#Loss 41 =1004.8573\n",
      "#Loss 42 =1004.5943\n",
      "#Loss 43 =1003.9951\n",
      "#Loss 44 =1003.4737\n",
      "#Loss 45 =1003.2005\n",
      "#Loss 46 =1002.7695\n",
      "#Loss 47 =1002.5516\n",
      "#Loss 48 =1002.2577\n",
      "#Loss 49 =1001.8242\n",
      "#Loss 50 =1001.4683\n",
      "#Loss 51 =1009.0470\n",
      "#Loss 52 =1000.6783\n",
      "#Loss 53 =1000.2036\n",
      "#Loss 54 =999.7727\n",
      "#Loss 55 =999.4139\n",
      "#Loss 56 =999.0301\n",
      "#Loss 57 =998.3320\n",
      "#Loss 58 =966.3603\n",
      "#Loss 59 =996.7563\n",
      "#Loss 60 =996.1387\n",
      "#Loss 61 =961.3734\n",
      "#Loss 62 =994.6297\n",
      "#Loss 63 =969.1795\n",
      "#Loss 64 =992.4969\n",
      "#Loss 65 =991.6043\n",
      "#Loss 66 =965.9686\n",
      "#Loss 67 =991.0180\n",
      "#Loss 68 =980.1705\n",
      "#Loss 69 =978.4605\n",
      "#Loss 70 =988.6189\n",
      "#Loss 71 =977.3066\n",
      "#Loss 72 =946.2198\n",
      "#Loss 73 =984.3368\n",
      "#Loss 74 =993.8557\n",
      "#Loss 75 =993.4859\n",
      "#Loss 76 =994.0484\n",
      "#Loss 77 =1016.3288\n",
      "#Loss 78 =1016.1239\n",
      "#Loss 79 =1016.4897\n",
      "#Loss 80 =1013.0928\n",
      "#Loss 81 =1012.7844\n",
      "#Loss 82 =1014.7087\n",
      "#Loss 83 =1015.0887\n",
      "#Loss 84 =1013.2225\n",
      "#Loss 85 =1015.1348\n",
      "#Loss 86 =1013.6558\n",
      "#Loss 87 =1012.4786\n",
      "#Loss 88 =1012.2063\n",
      "#Loss 89 =1013.6982\n",
      "#Loss 90 =1014.0066\n",
      "#Loss 91 =1011.7182\n",
      "#Loss 92 =1013.5535\n",
      "#Loss 93 =1013.3410\n",
      "#Loss 94 =1011.7018\n",
      "#Loss 95 =1010.4145\n",
      "#Loss 96 =1012.6291\n",
      "#Loss 97 =1009.8892\n",
      "#Loss 98 =1010.6610\n",
      "#Loss 99 =1009.3152\n",
      "#Loss 100 =1009.0261\n",
      "#Loss 101 =1008.0734\n",
      "#Loss 102 =1011.0790\n",
      "#Loss 103 =1006.5300\n",
      "#Loss 104 =1007.8110\n",
      "#Loss 105 =1006.8276\n",
      "#Loss 106 =1009.9947\n",
      "#Loss 107 =1008.0228\n",
      "#Loss 108 =1009.5178\n",
      "#Loss 109 =1009.2265\n",
      "#Loss 110 =1006.0424\n",
      "#Loss 111 =1005.7111\n",
      "#Loss 112 =1008.3375\n",
      "#Loss 113 =1006.3117\n",
      "#Loss 114 =1007.8380\n",
      "#Loss 115 =1005.7057\n",
      "#Loss 116 =1004.1036\n",
      "#Loss 117 =1001.8319\n",
      "#Loss 118 =1004.6243\n",
      "#Loss 119 =1004.3967\n",
      "#Loss 120 =1006.0476\n",
      "#Loss 121 =1003.7068\n",
      "#Loss 122 =1005.4345\n",
      "#Loss 123 =999.5768\n",
      "#Loss 124 =1001.3301\n",
      "#Loss 125 =1000.9333\n",
      "#Loss 126 =1000.5963\n",
      "#Loss 127 =1003.8406\n",
      "#Loss 128 =1003.5397\n",
      "#Loss 129 =1001.0534\n",
      "#Loss 130 =1000.7106\n",
      "#Loss 131 =1000.2914\n",
      "#Loss 132 =999.8457\n",
      "#Loss 133 =1001.7077\n",
      "#Loss 134 =1001.3588\n",
      "#Loss 135 =1001.0464\n",
      "#Loss 136 =1000.6534\n",
      "#Loss 137 =1000.2727\n",
      "#Loss 138 =999.9239\n",
      "#Loss 139 =999.5570\n",
      "#Loss 140 =999.1255\n",
      "#Loss 141 =998.7719\n",
      "#Loss 142 =998.3487\n",
      "#Loss 143 =1001.9184\n",
      "#Loss 144 =1004.2351\n",
      "#Loss 145 =997.2854\n",
      "#Loss 146 =996.8496\n",
      "#Loss 147 =1003.4220\n",
      "#Loss 148 =1000.4298\n",
      "#Loss 149 =1000.1499\n",
      "#Loss 150 =999.8383\n",
      "超过迭代上限\n",
      " 2 100 绝对误差 tensor(36.7127, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(15.1031, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0316, dtype=torch.float64)   实验回归误差 tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 0   条件数 1.0\n",
      "Loss 0 =37.7798\n",
      "#Loss 1 =37.6748\n",
      "#Loss 2 =18.1438\n",
      "#Loss 3 =10.0556\n",
      "#Loss 4 =6.2350\n",
      "#Loss 5 =4.1542\n",
      "#Loss 6 =2.9313\n",
      "#Loss 7 =2.1824\n",
      "#Loss 8 =1.7085\n",
      "#Loss 9 =1.3973\n",
      "#Loss 10 =1.1802\n",
      "#Loss 11 =1.0226\n",
      "#Loss 12 =0.9067\n",
      "#Loss 13 =0.8185\n",
      "#Loss 14 =0.7468\n",
      "#Loss 15 =0.6912\n",
      "#Loss 16 =0.6479\n",
      "#Loss 17 =0.6138\n",
      "#Loss 18 =0.5839\n",
      "#Loss 19 =0.5591\n",
      "#Loss 20 =0.5393\n",
      "#Loss 21 =0.5232\n",
      "#Loss 22 =0.5083\n",
      "#Loss 23 =0.4932\n",
      "#Loss 24 =0.4811\n",
      "#Loss 25 =0.4710\n",
      "#Loss 26 =0.4629\n",
      "#Loss 27 =0.4565\n",
      "#Loss 28 =0.4514\n",
      "#Loss 29 =0.4473\n",
      "#Loss 30 =0.4439\n",
      "#Loss 31 =0.4413\n",
      "#Loss 32 =0.4391\n",
      "#Loss 33 =0.4374\n",
      "#Loss 34 =0.4351\n",
      "#Loss 35 =0.4311\n",
      "#Loss 36 =0.4278\n",
      "#Loss 37 =0.4251\n",
      "#Loss 38 =0.4230\n",
      "#Loss 39 =0.4212\n",
      "#Loss 40 =0.4199\n",
      "#Loss 41 =0.4187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Loss 42 =0.4176\n",
      "#Loss 43 =0.4158\n",
      "#Loss 44 =0.4143\n",
      "#Loss 45 =0.4131\n",
      "#Loss 46 =0.4121\n",
      "#Loss 47 =0.4114\n",
      "#Loss 48 =0.4107\n",
      "#Loss 49 =0.4102\n",
      "#Loss 50 =0.4098\n",
      "#Loss 51 =0.4095\n",
      "#Loss 52 =0.4093\n",
      "#Loss 53 =0.4091\n",
      "#Loss 54 =0.4089\n",
      "#Loss 55 =0.4088\n",
      "#Loss 56 =0.4087\n",
      "#Loss 57 =0.4086\n",
      "#Loss 58 =0.4085\n",
      "#Loss 59 =0.4084\n",
      "#Loss 60 =0.4084\n",
      "#Loss 61 =0.4084\n",
      "#Loss 62 =0.4083\n",
      "#Loss 63 =0.4083\n",
      "#Loss 64 =0.4083\n",
      "#Loss 65 =0.4083\n",
      "#Loss 66 =0.4083\n",
      "#Loss 67 =0.4083\n",
      "#Loss 68 =0.4083\n",
      "#Loss 69 =0.4082\n",
      "#Loss 70 =0.4082\n",
      "#Loss 71 =0.4082\n",
      "#Loss 72 =0.4082\n",
      "#Loss 73 =0.4082\n",
      "#Loss 74 =0.4082\n",
      " 2 100 绝对误差 tensor(1.8020, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.9605, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1842, dtype=torch.float64)   实验回归误差 tensor(0.1039, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4000, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =146.1893\n",
      "#Loss 1 =145.5979\n",
      "#Loss 2 =93.3722\n",
      "#Loss 3 =61.4042\n",
      "#Loss 4 =23.9826\n",
      "#Loss 5 =1.3777\n",
      "#Loss 6 =0.8847\n",
      "#Loss 7 =0.6332\n",
      "#Loss 8 =0.5175\n",
      "#Loss 9 =0.4733\n",
      "#Loss 10 =0.5168\n",
      "#Loss 11 =0.6472\n",
      "#Loss 12 =1.0957\n",
      "#Loss 13 =1.7794\n",
      "#Loss 14 =4.3428\n",
      "#Loss 15 =5.4935\n",
      "#Loss 16 =14.7348\n",
      "#Loss 17 =5.0025\n",
      "#Loss 18 =11.2875\n",
      "#Loss 19 =4.4639\n",
      "#Loss 20 =9.5911\n",
      "#Loss 21 =4.5792\n",
      "#Loss 22 =9.9806\n",
      "#Loss 23 =4.5802\n",
      "#Loss 24 =9.9715\n",
      "#Loss 25 =4.5747\n",
      "#Loss 26 =9.9538\n",
      "#Loss 27 =4.5752\n",
      "#Loss 28 =9.9559\n",
      "#Loss 29 =4.5754\n",
      "#Loss 30 =9.9564\n",
      "#Loss 31 =4.5753\n",
      "#Loss 32 =9.9563\n",
      "#Loss 33 =4.5753\n",
      "#Loss 34 =9.9562\n",
      "#Loss 35 =4.5753\n",
      "#Loss 36 =9.9563\n",
      "#Loss 37 =4.5753\n",
      "#Loss 38 =9.9563\n",
      "#Loss 39 =4.5753\n",
      "#Loss 40 =9.9563\n",
      "#Loss 41 =4.5753\n",
      "#Loss 42 =9.9563\n",
      "#Loss 43 =4.5753\n",
      "#Loss 44 =9.9563\n",
      "#Loss 45 =4.5753\n",
      "#Loss 46 =9.9563\n",
      "#Loss 47 =4.5753\n",
      "#Loss 48 =9.9563\n",
      "#Loss 49 =4.5753\n",
      "#Loss 50 =9.9563\n",
      "#Loss 51 =4.5753\n",
      "#Loss 52 =9.9563\n",
      "#Loss 53 =4.5753\n",
      "#Loss 54 =9.9563\n",
      "#Loss 55 =4.5753\n",
      "#Loss 56 =9.9563\n",
      "#Loss 57 =4.5753\n",
      "#Loss 58 =9.9563\n",
      "#Loss 59 =4.5753\n",
      "#Loss 60 =9.9563\n",
      "#Loss 61 =4.5753\n",
      "#Loss 62 =9.9563\n",
      "#Loss 63 =4.5753\n",
      "#Loss 64 =9.9563\n",
      "#Loss 65 =4.5753\n",
      "#Loss 66 =9.9563\n",
      "#Loss 67 =4.5753\n",
      "#Loss 68 =9.9563\n",
      "#Loss 69 =4.5753\n",
      "#Loss 70 =9.9563\n",
      "#Loss 71 =4.5753\n",
      "#Loss 72 =9.9563\n",
      "#Loss 73 =4.5753\n",
      "#Loss 74 =9.9563\n",
      "#Loss 75 =4.5753\n",
      "#Loss 76 =9.9563\n",
      "#Loss 77 =4.5753\n",
      "#Loss 78 =9.9563\n",
      "#Loss 79 =4.5753\n",
      "#Loss 80 =9.9563\n",
      "#Loss 81 =4.5753\n",
      "#Loss 82 =9.9563\n",
      "#Loss 83 =4.5753\n",
      "#Loss 84 =9.9563\n",
      "#Loss 85 =4.5753\n",
      "#Loss 86 =9.9563\n",
      "#Loss 87 =4.5753\n",
      "#Loss 88 =9.9563\n",
      "#Loss 89 =4.5753\n",
      "#Loss 90 =9.9563\n",
      "#Loss 91 =4.5753\n",
      "#Loss 92 =9.9563\n",
      "#Loss 93 =4.5753\n",
      "#Loss 94 =9.9563\n",
      "#Loss 95 =4.5753\n",
      "#Loss 96 =9.9563\n",
      "#Loss 97 =4.5753\n",
      "#Loss 98 =9.9563\n",
      "#Loss 99 =4.5753\n",
      "#Loss 100 =9.9563\n",
      "#Loss 101 =4.5753\n",
      "#Loss 102 =9.9563\n",
      "#Loss 103 =4.5753\n",
      "#Loss 104 =9.9563\n",
      "#Loss 105 =4.5753\n",
      "#Loss 106 =9.9563\n",
      "#Loss 107 =4.5753\n",
      "#Loss 108 =9.9563\n",
      "#Loss 109 =4.5753\n",
      "#Loss 110 =9.9563\n",
      "#Loss 111 =4.5753\n",
      "#Loss 112 =9.9563\n",
      "#Loss 113 =4.5753\n",
      "#Loss 114 =9.9563\n",
      "#Loss 115 =4.5753\n",
      "#Loss 116 =9.9563\n",
      "#Loss 117 =4.5753\n",
      "#Loss 118 =9.9563\n",
      "#Loss 119 =4.5753\n",
      "#Loss 120 =9.9563\n",
      "#Loss 121 =4.5753\n",
      "#Loss 122 =9.9563\n",
      "#Loss 123 =4.5753\n",
      "#Loss 124 =9.9563\n",
      "#Loss 125 =4.5753\n",
      "#Loss 126 =9.9563\n",
      "#Loss 127 =4.5753\n",
      "#Loss 128 =9.9563\n",
      "#Loss 129 =4.5753\n",
      "#Loss 130 =9.9563\n",
      "#Loss 131 =4.5753\n",
      "#Loss 132 =9.9563\n",
      "#Loss 133 =4.5753\n",
      "#Loss 134 =9.9563\n",
      "#Loss 135 =4.5753\n",
      "#Loss 136 =9.9563\n",
      "#Loss 137 =4.5753\n",
      "#Loss 138 =9.9563\n",
      "#Loss 139 =4.5753\n",
      "#Loss 140 =9.9563\n",
      "#Loss 141 =4.5753\n",
      "#Loss 142 =9.9563\n",
      "#Loss 143 =4.5753\n",
      "#Loss 144 =9.9563\n",
      "#Loss 145 =4.5753\n",
      "#Loss 146 =9.9563\n",
      "#Loss 147 =4.5753\n",
      "#Loss 148 =9.9563\n",
      "#Loss 149 =4.5753\n",
      "#Loss 150 =9.9563\n",
      "超过迭代上限\n",
      " 2 100 绝对误差 tensor(0.1063, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.1163, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0830, dtype=torch.float64)   实验回归误差 tensor(0.1775, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.2849, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 98   条件数 1.0\n",
      "Loss 0 =32.7014\n",
      "#Loss 1 =29.7243\n",
      "#Loss 2 =15.4245\n",
      "#Loss 3 =9.3648\n",
      "#Loss 4 =5.6496\n",
      "#Loss 5 =3.2233\n",
      "#Loss 6 =1.8360\n",
      "#Loss 7 =1.1569\n",
      "#Loss 8 =0.8120\n",
      "#Loss 9 =0.6001\n",
      "#Loss 10 =0.4574\n",
      "#Loss 11 =0.3593\n",
      "#Loss 12 =0.2912\n",
      "#Loss 13 =0.2434\n",
      "#Loss 14 =0.2097\n",
      "#Loss 15 =0.1858\n",
      "#Loss 16 =0.1687\n",
      "#Loss 17 =0.1564\n",
      "#Loss 18 =0.1475\n",
      "#Loss 19 =0.1411\n",
      "#Loss 20 =0.1364\n",
      "#Loss 21 =0.1331\n",
      "#Loss 22 =0.1306\n",
      "#Loss 23 =0.1288\n",
      "#Loss 24 =0.1275\n",
      "#Loss 25 =0.1265\n",
      "#Loss 26 =0.1258\n",
      "#Loss 27 =0.1253\n",
      "#Loss 28 =0.1249\n",
      "#Loss 29 =0.1246\n",
      "#Loss 30 =0.1244\n",
      "#Loss 31 =0.1243\n",
      "#Loss 32 =0.1242\n",
      "#Loss 33 =0.1241\n",
      "#Loss 34 =0.1240\n",
      "#Loss 35 =0.1240\n",
      "#Loss 36 =0.1239\n",
      "#Loss 37 =0.1239\n",
      "#Loss 38 =0.1239\n",
      "#Loss 39 =0.1239\n",
      "#Loss 40 =0.1239\n",
      "#Loss 41 =0.1239\n",
      "#Loss 42 =0.1239\n",
      "#Loss 43 =0.1238\n",
      "#Loss 44 =0.1238\n",
      "#Loss 45 =0.1238\n",
      "#Loss 46 =0.1238\n",
      "#Loss 47 =0.1238\n",
      " 2 100 绝对误差 tensor(0.0269, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.0506, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1496, dtype=torch.float64)   实验回归误差 tensor(0.0615, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.3115, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =197.1738\n",
      "#Loss 1 =190.5283\n",
      "#Loss 2 =110.8007\n",
      "#Loss 3 =56.5408\n",
      "#Loss 4 =4.8411\n",
      "#Loss 5 =6.7321\n",
      "#Loss 6 =15.6895\n",
      "#Loss 7 =15.3808\n",
      "#Loss 8 =47.8404\n",
      "#Loss 9 =3.5983\n",
      "#Loss 10 =3.7326\n",
      "#Loss 11 =4.2327\n",
      "#Loss 12 =5.5215\n",
      "#Loss 13 =11.0628\n",
      "#Loss 14 =14.6842\n",
      "#Loss 15 =45.4596\n",
      "#Loss 16 =3.5455\n",
      "#Loss 17 =3.5693\n",
      "#Loss 18 =3.6489\n",
      "#Loss 19 =3.9360\n",
      "#Loss 20 =4.7291\n",
      "#Loss 21 =7.9833\n",
      "#Loss 22 =12.1959\n",
      "#Loss 23 =36.6018\n",
      "#Loss 24 =5.4487\n",
      "#Loss 25 =10.7667\n",
      "#Loss 26 =14.5215\n",
      "#Loss 27 =44.8901\n",
      "#Loss 28 =3.5696\n",
      "#Loss 29 =3.6516\n",
      "#Loss 30 =3.9039\n",
      "#Loss 31 =4.8578\n",
      "#Loss 32 =7.0222\n",
      "#Loss 33 =16.9460\n",
      "#Loss 34 =15.1744\n",
      "#Loss 35 =47.1483\n",
      "#Loss 36 =3.5568\n",
      "#Loss 37 =3.6032\n",
      "#Loss 38 =3.7732\n",
      "#Loss 39 =4.2662\n",
      "#Loss 40 =6.2134\n",
      "#Loss 41 =9.6803\n",
      "#Loss 42 =27.2330\n",
      "#Loss 43 =10.2410\n",
      "#Loss 44 =29.3410\n",
      "#Loss 45 =8.9784\n",
      "#Loss 46 =24.5332\n",
      "#Loss 47 =11.7893\n",
      "#Loss 48 =35.0978\n",
      "#Loss 49 =6.0572\n",
      "#Loss 50 =13.1479\n",
      "#Loss 51 =15.3497\n",
      "#Loss 52 =47.7497\n",
      "#Loss 53 =3.5908\n",
      "#Loss 54 =3.7105\n",
      "#Loss 55 =4.1537\n",
      "#Loss 56 =5.3158\n",
      "#Loss 57 =10.2589\n",
      "#Loss 58 =14.2236\n",
      "#Loss 59 =43.8528\n",
      "#Loss 60 =3.6509\n",
      "#Loss 61 =3.9366\n",
      "#Loss 62 =4.7289\n",
      "#Loss 63 =7.9811\n",
      "#Loss 64 =12.1921\n",
      "#Loss 65 =36.5873\n",
      "#Loss 66 =5.4541\n",
      "#Loss 67 =10.7876\n",
      "#Loss 68 =14.5331\n",
      "#Loss 69 =44.9303\n",
      "#Loss 70 =3.5675\n",
      "#Loss 71 =3.6442\n",
      "#Loss 72 =3.8812\n",
      "#Loss 73 =4.7743\n",
      "#Loss 74 =6.8348\n",
      "#Loss 75 =16.2114\n",
      "#Loss 76 =15.3254\n",
      "#Loss 77 =47.6677\n",
      "#Loss 78 =3.5852\n",
      "#Loss 79 =3.6932\n",
      "#Loss 80 =4.0917\n",
      "#Loss 81 =5.1517\n",
      "#Loss 82 =9.6192\n",
      "#Loss 83 =13.7674\n",
      "#Loss 84 =42.2460\n",
      "#Loss 85 =3.8728\n",
      "#Loss 86 =4.7357\n",
      "#Loss 87 =6.7416\n",
      "#Loss 88 =15.8429\n",
      "#Loss 89 =15.3793\n",
      "#Loss 90 =47.8516\n",
      "#Loss 91 =3.5980\n",
      "#Loss 92 =3.7332\n",
      "#Loss 93 =4.2352\n",
      "#Loss 94 =5.5282\n",
      "#Loss 95 =11.0890\n",
      "#Loss 96 =14.6975\n",
      "#Loss 97 =45.5058\n",
      "#Loss 98 =3.5441\n",
      "#Loss 99 =3.5647\n",
      "#Loss 100 =3.6342\n",
      "#Loss 101 =3.8841\n",
      "#Loss 102 =4.5839\n",
      "#Loss 103 =7.4241\n",
      "#Loss 104 =11.4985\n",
      "#Loss 105 =34.0444\n",
      "#Loss 106 =6.5382\n",
      "#Loss 107 =15.0343\n",
      "#Loss 108 =15.4506\n",
      "#Loss 109 =48.0941\n",
      "#Loss 110 =3.6175\n",
      "#Loss 111 =3.7933\n",
      "#Loss 112 =4.4519\n",
      "#Loss 113 =6.0713\n",
      "#Loss 114 =13.2156\n",
      "#Loss 115 =15.3689\n",
      "#Loss 116 =47.8193\n",
      "#Loss 117 =3.5955\n",
      "#Loss 118 =3.7255\n",
      "#Loss 119 =4.2076\n",
      "#Loss 120 =5.4568\n",
      "#Loss 121 =10.8098\n",
      "#Loss 122 =14.5513\n",
      "#Loss 123 =44.9973\n",
      "#Loss 124 =3.5640\n",
      "#Loss 125 =3.6326\n",
      "#Loss 126 =3.8459\n",
      "#Loss 127 =4.6450\n",
      "#Loss 128 =6.5360\n",
      "#Loss 129 =15.0387\n",
      "#Loss 130 =15.4575\n",
      "#Loss 131 =48.1210\n",
      "#Loss 132 =3.6193\n",
      "#Loss 133 =3.7995\n",
      "#Loss 134 =4.4740\n",
      "#Loss 135 =6.1256\n",
      "#Loss 136 =13.4287\n",
      "#Loss 137 =15.4011\n",
      "#Loss 138 =47.9294\n",
      "#Loss 139 =3.6037\n",
      "#Loss 140 =3.7511\n",
      "#Loss 141 =4.2990\n",
      "#Loss 142 =5.6910\n",
      "#Loss 143 =11.7255\n",
      "#Loss 144 =14.9759\n",
      "#Loss 145 =46.4694\n",
      "#Loss 146 =3.5369\n",
      "#Loss 147 =3.5398\n",
      "#Loss 148 =3.5550\n",
      "#Loss 149 =3.6050\n",
      "#Loss 150 =3.7819\n",
      "超过迭代上限\n",
      " 2 100 绝对误差 tensor(2.4442, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.7640, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0687, dtype=torch.float64)   实验回归误差 tensor(0.1476, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =55.3392\n",
      "#Loss 1 =54.5585\n",
      "#Loss 2 =21.0515\n",
      "#Loss 3 =11.6248\n",
      "#Loss 4 =8.0382\n",
      "#Loss 5 =6.2404\n",
      "#Loss 6 =5.1341\n",
      "#Loss 7 =4.3830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Loss 8 =3.8394\n",
      "#Loss 9 =3.4467\n",
      "#Loss 10 =3.1573\n",
      "#Loss 11 =2.9335\n",
      "#Loss 12 =2.7516\n",
      "#Loss 13 =2.6149\n",
      "#Loss 14 =2.5023\n",
      "#Loss 15 =2.4006\n",
      "#Loss 16 =2.3084\n",
      "#Loss 17 =2.2205\n",
      "#Loss 18 =2.1358\n",
      "#Loss 19 =2.0557\n",
      "#Loss 20 =1.9780\n",
      "#Loss 21 =1.9074\n",
      "#Loss 22 =1.8368\n",
      "#Loss 23 =1.7796\n",
      "#Loss 24 =1.7308\n",
      "#Loss 25 =1.6744\n",
      "#Loss 26 =1.6246\n",
      "#Loss 27 =1.5848\n",
      "#Loss 28 =1.5493\n",
      "#Loss 29 =1.5155\n",
      "#Loss 30 =1.4848\n",
      "#Loss 31 =1.4571\n",
      "#Loss 32 =1.4345\n",
      "#Loss 33 =1.4138\n",
      "#Loss 34 =1.3959\n",
      "#Loss 35 =1.3804\n",
      "#Loss 36 =1.3671\n",
      "#Loss 37 =1.3555\n",
      "#Loss 38 =1.3465\n",
      "#Loss 39 =1.3396\n",
      "#Loss 40 =1.3334\n",
      "#Loss 41 =1.3270\n",
      "#Loss 42 =1.3215\n",
      "#Loss 43 =1.3158\n",
      "#Loss 44 =1.3116\n",
      "#Loss 45 =1.3082\n",
      "#Loss 46 =1.3052\n",
      "#Loss 47 =1.3028\n",
      "#Loss 48 =1.3010\n",
      "#Loss 49 =1.2998\n",
      "#Loss 50 =1.2988\n",
      "#Loss 51 =1.2980\n",
      "#Loss 52 =1.2973\n",
      "#Loss 53 =1.2969\n",
      "#Loss 54 =1.2965\n",
      "#Loss 55 =1.2962\n",
      "#Loss 56 =1.2958\n",
      "#Loss 57 =1.2952\n",
      "#Loss 58 =1.2948\n",
      "#Loss 59 =1.2944\n",
      "#Loss 60 =1.2939\n",
      "#Loss 61 =1.2936\n",
      "#Loss 62 =1.2933\n",
      "#Loss 63 =1.2930\n",
      "#Loss 64 =1.2927\n",
      "#Loss 65 =1.2925\n",
      "#Loss 66 =1.2924\n",
      "#Loss 67 =1.2923\n",
      "#Loss 68 =1.2922\n",
      "#Loss 69 =1.2921\n",
      "#Loss 70 =1.2920\n",
      "#Loss 71 =1.2919\n",
      "#Loss 72 =1.2919\n",
      "#Loss 73 =1.2918\n",
      "#Loss 74 =1.2917\n",
      "#Loss 75 =1.2914\n",
      "#Loss 76 =1.2911\n",
      "#Loss 77 =1.2908\n",
      "#Loss 78 =1.2905\n",
      "#Loss 79 =1.2902\n",
      "#Loss 80 =1.2901\n",
      "#Loss 81 =1.2899\n",
      "#Loss 82 =1.2896\n",
      "#Loss 83 =1.2894\n",
      "#Loss 84 =1.2892\n",
      "#Loss 85 =1.2891\n",
      "#Loss 86 =1.2889\n",
      "#Loss 87 =1.2889\n",
      "#Loss 88 =1.2888\n",
      "#Loss 89 =1.2887\n",
      "#Loss 90 =1.2886\n",
      "#Loss 91 =1.2884\n",
      "#Loss 92 =1.2871\n",
      "#Loss 93 =1.2859\n",
      "#Loss 94 =1.2848\n",
      "#Loss 95 =1.2840\n",
      "#Loss 96 =1.2834\n",
      "#Loss 97 =1.2828\n",
      "#Loss 98 =1.2817\n",
      "#Loss 99 =1.2801\n",
      "#Loss 100 =1.2790\n",
      "#Loss 101 =1.2780\n",
      "#Loss 102 =1.2767\n",
      "#Loss 103 =1.2754\n",
      "#Loss 104 =1.2742\n",
      "#Loss 105 =1.2733\n",
      "#Loss 106 =1.2727\n",
      "#Loss 107 =1.2722\n",
      "#Loss 108 =1.2719\n",
      "#Loss 109 =1.2716\n",
      "#Loss 110 =1.2713\n",
      "#Loss 111 =1.2711\n",
      "#Loss 112 =1.2710\n",
      "#Loss 113 =1.2709\n",
      "#Loss 114 =1.2708\n",
      "#Loss 115 =1.2708\n",
      "#Loss 116 =1.2707\n",
      "#Loss 117 =1.2707\n",
      "#Loss 118 =1.2707\n",
      "#Loss 119 =1.2706\n",
      "#Loss 120 =1.2706\n",
      "#Loss 121 =1.2706\n",
      " 2 100 绝对误差 tensor(3.6346, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.4770, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1428, dtype=torch.float64)   实验回归误差 tensor(0.1515, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4000, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =3075.8898\n",
      "#Loss 1 =3049.1328\n",
      "#Loss 2 =2740.8525\n",
      "#Loss 3 =1952.0192\n",
      "#Loss 4 =3075.6507\n",
      "#Loss 5 =3075.6491\n",
      " 2 100 绝对误差 tensor(2.3176, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.3776, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0175, dtype=torch.float64)   实验回归误差 tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 0   条件数 1.0\n",
      "Loss 0 =44.0127\n",
      "#Loss 1 =43.8963\n",
      "#Loss 2 =25.7693\n",
      "#Loss 3 =17.0022\n",
      "#Loss 4 =11.7224\n",
      "#Loss 5 =7.9295\n",
      "#Loss 6 =5.1164\n",
      "#Loss 7 =3.1245\n",
      "#Loss 8 =1.8146\n",
      "#Loss 9 =1.0256\n",
      "#Loss 10 =0.5938\n",
      "#Loss 11 =0.3757\n",
      "#Loss 12 =0.2731\n",
      "#Loss 13 =0.2265\n",
      "#Loss 14 =0.2067\n",
      "#Loss 15 =0.1985\n",
      "#Loss 16 =0.1952\n",
      "#Loss 17 =0.1939\n",
      "#Loss 18 =0.1934\n",
      "#Loss 19 =0.1932\n",
      "#Loss 20 =0.1931\n",
      "#Loss 21 =0.1930\n",
      "#Loss 22 =0.1930\n",
      "#Loss 23 =0.1930\n",
      "#Loss 24 =0.1930\n",
      "#Loss 25 =0.1930\n",
      "#Loss 26 =0.1930\n",
      "#Loss 27 =0.1930\n",
      "#Loss 28 =0.1930\n",
      "#Loss 29 =0.1930\n",
      "#Loss 30 =0.1930\n",
      "#Loss 31 =0.1930\n",
      "#Loss 32 =0.1930\n",
      "#Loss 33 =0.1930\n",
      " 2 100 绝对误差 tensor(0.0202, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.0246, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1399, dtype=torch.float64)   实验回归误差 tensor(0.0662, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.3491, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =60.9416\n",
      "#Loss 1 =60.6613\n",
      "#Loss 2 =33.1411\n",
      "#Loss 3 =21.0717\n",
      "#Loss 4 =14.9530\n",
      "#Loss 5 =11.3179\n",
      "#Loss 6 =8.7147\n",
      "#Loss 7 =6.9527\n",
      "#Loss 8 =5.7465\n",
      "#Loss 9 =4.8826\n",
      "#Loss 10 =4.2799\n",
      "#Loss 11 =3.8543\n",
      "#Loss 12 =3.5494\n",
      "#Loss 13 =3.3401\n",
      "#Loss 14 =3.1858\n",
      "#Loss 15 =3.0568\n",
      "#Loss 16 =2.9489\n",
      "#Loss 17 =2.8689\n",
      "#Loss 18 =2.7732\n",
      "#Loss 19 =2.6889\n",
      "#Loss 20 =2.6180\n",
      "#Loss 21 =2.5204\n",
      "#Loss 22 =2.4089\n",
      "#Loss 23 =2.3259\n",
      "#Loss 24 =2.2699\n",
      "#Loss 25 =2.1767\n",
      "#Loss 26 =2.0958\n",
      "#Loss 27 =2.0037\n",
      "#Loss 28 =1.9279\n",
      "#Loss 29 =1.8676\n",
      "#Loss 30 =1.8289\n",
      "#Loss 31 =1.8015\n",
      "#Loss 32 =1.7662\n",
      "#Loss 33 =1.7144\n",
      "#Loss 34 =1.6593\n",
      "#Loss 35 =1.5136\n",
      "#Loss 36 =1.0156\n",
      "#Loss 37 =0.7174\n",
      "#Loss 38 =0.5447\n",
      "#Loss 39 =0.4477\n",
      "#Loss 40 =0.3913\n",
      "#Loss 41 =0.3587\n",
      "#Loss 42 =0.3388\n",
      "#Loss 43 =0.3256\n",
      "#Loss 44 =0.3162\n",
      "#Loss 45 =0.3104\n",
      "#Loss 46 =0.3064\n",
      "#Loss 47 =0.3033\n",
      "#Loss 48 =0.3009\n",
      "#Loss 49 =0.2994\n",
      "#Loss 50 =0.2982\n",
      "#Loss 51 =0.2973\n",
      "#Loss 52 =0.2966\n",
      "#Loss 53 =0.2960\n",
      "#Loss 54 =0.2957\n",
      "#Loss 55 =0.2954\n",
      "#Loss 56 =0.2953\n",
      "#Loss 57 =0.2952\n",
      "#Loss 58 =0.2951\n",
      "#Loss 59 =0.2951\n",
      "#Loss 60 =0.2951\n",
      "#Loss 61 =0.2950\n",
      "#Loss 62 =0.2950\n",
      "#Loss 63 =0.2950\n",
      "#Loss 64 =0.2950\n",
      "#Loss 65 =0.2950\n",
      "#Loss 66 =0.2950\n",
      " 2 100 绝对误差 tensor(0.0264, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.0188, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1165, dtype=torch.float64)   实验回归误差 tensor(0.0696, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.3342, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =96.7197\n",
      "#Loss 1 =94.0842\n",
      "#Loss 2 =50.6385\n",
      "#Loss 3 =31.2321\n",
      "#Loss 4 =15.0354\n",
      "#Loss 5 =6.0396\n",
      "#Loss 6 =4.7941\n",
      "#Loss 7 =4.7035\n",
      "#Loss 8 =4.6577\n",
      "#Loss 9 =4.6332\n",
      "#Loss 10 =4.6199\n",
      "#Loss 11 =4.6124\n",
      "#Loss 12 =4.6083\n",
      "#Loss 13 =4.6060\n",
      "#Loss 14 =4.6047\n",
      "#Loss 15 =4.6040\n",
      "#Loss 16 =4.6036\n",
      "#Loss 17 =4.6034\n",
      "#Loss 18 =4.6033\n",
      "#Loss 19 =4.6032\n",
      "#Loss 20 =4.6032\n",
      " 2 100 绝对误差 tensor(1.7349, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.8453, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0917, dtype=torch.float64)   实验回归误差 tensor(0.2182, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =494.9486\n",
      "#Loss 1 =470.5417\n",
      "#Loss 2 =339.0858\n",
      "#Loss 3 =86.3265\n",
      "#Loss 4 =7676.1040\n",
      "#Loss 5 =489.3151\n",
      "#Loss 6 =489.3105\n",
      " 2 100 绝对误差 tensor(95.3891, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(60.1206, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0445, dtype=torch.float64)   实验回归误差 tensor(1.0001, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 0   条件数 1.0\n",
      "Loss 0 =61.1770\n",
      "#Loss 1 =60.2852\n",
      "#Loss 2 =32.5747\n",
      "#Loss 3 =20.1014\n",
      "#Loss 4 =13.1009\n",
      "#Loss 5 =8.7733\n",
      "#Loss 6 =6.2645\n",
      "#Loss 7 =4.9814\n",
      "#Loss 8 =4.3673\n",
      "#Loss 9 =4.0445\n",
      "#Loss 10 =3.8390\n",
      "#Loss 11 =3.6892\n",
      "#Loss 12 =3.5739\n",
      "#Loss 13 =3.4843\n",
      "#Loss 14 =3.4148\n",
      "#Loss 15 =3.3607\n",
      "#Loss 16 =3.3183\n",
      "#Loss 17 =3.2808\n",
      "#Loss 18 =3.2515\n",
      "#Loss 19 =3.2273\n",
      "#Loss 20 =3.2088\n",
      "#Loss 21 =3.1944\n",
      "#Loss 22 =3.1832\n",
      "#Loss 23 =3.1746\n",
      "#Loss 24 =3.1678\n",
      "#Loss 25 =3.1616\n",
      "#Loss 26 =3.1558\n",
      "#Loss 27 =3.1514\n",
      "#Loss 28 =3.1480\n",
      "#Loss 29 =3.1454\n",
      "#Loss 30 =3.1433\n",
      "#Loss 31 =3.1415\n",
      "#Loss 32 =3.1401\n",
      "#Loss 33 =3.1391\n",
      "#Loss 34 =3.1383\n",
      "#Loss 35 =3.1377\n",
      "#Loss 36 =3.1373\n",
      "#Loss 37 =3.1370\n",
      "#Loss 38 =3.1367\n",
      "#Loss 39 =3.1365\n",
      "#Loss 40 =3.1364\n",
      "#Loss 41 =3.1363\n",
      "#Loss 42 =3.1362\n",
      "#Loss 43 =3.1361\n",
      "#Loss 44 =3.1360\n",
      "#Loss 45 =3.1360\n",
      "#Loss 46 =3.1360\n",
      "#Loss 47 =3.1359\n",
      " 2 100 绝对误差 tensor(2.2532, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.7919, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1353, dtype=torch.float64)   实验回归误差 tensor(0.2264, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =33.3459\n",
      "#Loss 1 =31.5828\n",
      "#Loss 2 =10.8079\n",
      "#Loss 3 =4.7540\n",
      "#Loss 4 =2.2117\n",
      "#Loss 5 =1.0208\n",
      "#Loss 6 =0.4974\n",
      "#Loss 7 =0.2894\n",
      "#Loss 8 =0.2121\n",
      "#Loss 9 =0.1847\n",
      "#Loss 10 =0.1745\n",
      "#Loss 11 =0.1702\n",
      "#Loss 12 =0.1681\n",
      "#Loss 13 =0.1668\n",
      "#Loss 14 =0.1660\n",
      "#Loss 15 =0.1654\n",
      "#Loss 16 =0.1649\n",
      "#Loss 17 =0.1646\n",
      "#Loss 18 =0.1644\n",
      "#Loss 19 =0.1642\n",
      "#Loss 20 =0.1640\n",
      "#Loss 21 =0.1639\n",
      "#Loss 22 =0.1639\n",
      "#Loss 23 =0.1638\n",
      "#Loss 24 =0.1638\n",
      "#Loss 25 =0.1637\n",
      "#Loss 26 =0.1637\n",
      "#Loss 27 =0.1637\n",
      "#Loss 28 =0.1637\n",
      "#Loss 29 =0.1637\n",
      "#Loss 30 =0.1637\n",
      "#Loss 31 =0.1637\n",
      "#Loss 32 =0.1636\n",
      "#Loss 33 =0.1636\n",
      "#Loss 34 =0.1636\n",
      "#Loss 35 =0.1636\n",
      "#Loss 36 =0.1636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 100 绝对误差 tensor(0.0128, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.0217, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1610, dtype=torch.float64)   实验回归误差 tensor(0.0701, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.3115, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =58.7963\n",
      "#Loss 1 =57.5065\n",
      "#Loss 2 =29.6470\n",
      "#Loss 3 =19.2145\n",
      "#Loss 4 =13.2063\n",
      "#Loss 5 =9.7574\n",
      "#Loss 6 =8.2832\n",
      "#Loss 7 =7.9012\n",
      "#Loss 8 =7.8479\n",
      "#Loss 9 =7.8433\n",
      "#Loss 10 =7.8429\n",
      "#Loss 11 =7.8429\n",
      " 2 100 绝对误差 tensor(1.6747, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.6703, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1412, dtype=torch.float64)   实验回归误差 tensor(0.3652, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =11392.9969\n",
      "#Loss 1 =11361.5830\n",
      "#Loss 2 =10880.3486\n",
      "#Loss 3 =10454.8588\n",
      "#Loss 4 =11849.2924\n",
      "#Loss 5 =11325.9988\n",
      "#Loss 6 =11263.7695\n",
      "#Loss 7 =11310.8248\n",
      "#Loss 8 =11290.6240\n",
      "#Loss 9 =11269.2112\n",
      "#Loss 10 =11391.9089\n",
      "#Loss 11 =11391.9215\n",
      " 2 100 绝对误差 tensor(30.6171, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(16.2536, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0087, dtype=torch.float64)   实验回归误差 tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 0   条件数 1.0\n",
      "Loss 0 =42223.2762\n",
      "#Loss 1 =41704.3671\n",
      "#Loss 2 =4379615.0239\n",
      "#Loss 3 =nan\n",
      " 2 100 绝对误差 tensor(nan, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(nan, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0045, dtype=torch.float64)   实验回归误差 tensor(nan, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 0   条件数 1.0\n",
      "Loss 0 =1.7751\n",
      "#Loss 1 =1.7636\n",
      "#Loss 2 =0.7384\n",
      "#Loss 3 =0.3182\n",
      "#Loss 4 =0.1452\n",
      "#Loss 5 =0.0732\n",
      "#Loss 6 =0.0431\n",
      "#Loss 7 =0.0304\n",
      "#Loss 8 =0.0250\n",
      "#Loss 9 =0.0227\n",
      "#Loss 10 =0.0217\n",
      "#Loss 11 =0.0213\n",
      "#Loss 12 =0.0211\n",
      "#Loss 13 =0.0210\n",
      "#Loss 14 =0.0209\n",
      "#Loss 15 =0.0209\n",
      "#Loss 16 =0.0209\n",
      "#Loss 17 =0.0208\n",
      "#Loss 18 =0.0208\n",
      "#Loss 19 =0.0208\n",
      "#Loss 20 =0.0208\n",
      "#Loss 21 =0.0208\n",
      "#Loss 22 =0.0208\n",
      "#Loss 23 =0.0208\n",
      "#Loss 24 =0.0207\n",
      "#Loss 25 =0.0207\n",
      "#Loss 26 =0.0207\n",
      "#Loss 27 =0.0207\n",
      "#Loss 28 =0.0207\n",
      "#Loss 29 =0.0207\n",
      "#Loss 30 =0.0207\n",
      "#Loss 31 =0.0207\n",
      "#Loss 32 =0.0206\n",
      "#Loss 33 =0.0206\n",
      "#Loss 34 =0.0206\n",
      "#Loss 35 =0.0206\n",
      "#Loss 36 =0.0206\n",
      "#Loss 37 =0.0206\n",
      "#Loss 38 =0.0206\n",
      "#Loss 39 =0.0206\n",
      "#Loss 40 =0.0206\n",
      "#Loss 41 =0.0206\n",
      "#Loss 42 =0.0205\n",
      "#Loss 43 =0.0205\n",
      "#Loss 44 =0.0205\n",
      "#Loss 45 =0.0205\n",
      "#Loss 46 =0.0205\n",
      "#Loss 47 =0.0205\n",
      "#Loss 48 =0.0205\n",
      "#Loss 49 =0.0205\n",
      "#Loss 50 =0.0205\n",
      "#Loss 51 =0.0205\n",
      "#Loss 52 =0.0205\n",
      "#Loss 53 =0.0205\n",
      "#Loss 54 =0.0205\n",
      "#Loss 55 =0.0205\n",
      "#Loss 56 =0.0204\n",
      "#Loss 57 =0.0204\n",
      "#Loss 58 =0.0204\n",
      "#Loss 59 =0.0204\n",
      "#Loss 60 =0.0204\n",
      "#Loss 61 =0.0204\n",
      "#Loss 62 =0.0204\n",
      "#Loss 63 =0.0204\n",
      "#Loss 64 =0.0204\n",
      "#Loss 65 =0.0204\n",
      "#Loss 66 =0.0204\n",
      "#Loss 67 =0.0204\n",
      "#Loss 68 =0.0204\n",
      "#Loss 69 =0.0204\n",
      "#Loss 70 =0.0204\n",
      "#Loss 71 =0.0204\n",
      "#Loss 72 =0.0204\n",
      "#Loss 73 =0.0204\n",
      "#Loss 74 =0.0204\n",
      "#Loss 75 =0.0204\n",
      "#Loss 76 =0.0204\n",
      "#Loss 77 =0.0203\n",
      "#Loss 78 =0.0203\n",
      "#Loss 79 =0.0203\n",
      "#Loss 80 =0.0203\n",
      "#Loss 81 =0.0203\n",
      "#Loss 82 =0.0203\n",
      "#Loss 83 =0.0203\n",
      "#Loss 84 =0.0203\n",
      "#Loss 85 =0.0203\n",
      "#Loss 86 =0.0203\n",
      "#Loss 87 =0.0203\n",
      "#Loss 88 =0.0203\n",
      "#Loss 89 =0.0203\n",
      "#Loss 90 =0.0203\n",
      "#Loss 91 =0.0203\n",
      "#Loss 92 =0.0203\n",
      "#Loss 93 =0.0203\n",
      "#Loss 94 =0.0203\n",
      "#Loss 95 =0.0203\n",
      "#Loss 96 =0.0203\n",
      "#Loss 97 =0.0203\n",
      "#Loss 98 =0.0203\n",
      "#Loss 99 =0.0203\n",
      "#Loss 100 =0.0203\n",
      "#Loss 101 =0.0203\n",
      "#Loss 102 =0.0203\n",
      "#Loss 103 =0.0203\n",
      "#Loss 104 =0.0203\n",
      "#Loss 105 =0.0203\n",
      "#Loss 106 =0.0203\n",
      "#Loss 107 =0.0203\n",
      "#Loss 108 =0.0203\n",
      "#Loss 109 =0.0203\n",
      "#Loss 110 =0.0203\n",
      "#Loss 111 =0.0203\n",
      "#Loss 112 =0.0203\n",
      "#Loss 113 =0.0203\n",
      "#Loss 114 =0.0203\n",
      "#Loss 115 =0.0203\n",
      "#Loss 116 =0.0203\n",
      "#Loss 117 =0.0203\n",
      "#Loss 118 =0.0203\n",
      "#Loss 119 =0.0203\n",
      "#Loss 120 =0.0203\n",
      "#Loss 121 =0.0203\n",
      "#Loss 122 =0.0203\n",
      "#Loss 123 =0.0203\n",
      "#Loss 124 =0.0203\n",
      "#Loss 125 =0.0203\n",
      "#Loss 126 =0.0203\n",
      "#Loss 127 =0.0203\n",
      "#Loss 128 =0.0203\n",
      "#Loss 129 =0.0203\n",
      "#Loss 130 =0.0203\n",
      "#Loss 131 =0.0202\n",
      "#Loss 132 =0.0202\n",
      "#Loss 133 =0.0202\n",
      "#Loss 134 =0.0202\n",
      "#Loss 135 =0.0202\n",
      "#Loss 136 =0.0202\n",
      "#Loss 137 =0.0202\n",
      "#Loss 138 =0.0202\n",
      "#Loss 139 =0.0202\n",
      "#Loss 140 =0.0202\n",
      "#Loss 141 =0.0202\n",
      "#Loss 142 =0.0202\n",
      "#Loss 143 =0.0202\n",
      "#Loss 144 =0.0202\n",
      "#Loss 145 =0.0202\n",
      "#Loss 146 =0.0202\n",
      "#Loss 147 =0.0202\n",
      "#Loss 148 =0.0202\n",
      "#Loss 149 =0.0202\n",
      "#Loss 150 =0.0202\n",
      "超过迭代上限\n",
      " 2 100 绝对误差 tensor(0.3565, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.3487, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.6549, dtype=torch.float64)   实验回归误差 tensor(0.1068, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.3982, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 92   条件数 1.0\n",
      "Loss 0 =37.7081\n",
      "#Loss 1 =37.6402\n",
      "#Loss 2 =8.4413\n",
      "#Loss 3 =3.0888\n",
      "#Loss 4 =1.5139\n",
      "#Loss 5 =0.8866\n",
      "#Loss 6 =0.5968\n",
      "#Loss 7 =0.4513\n",
      "#Loss 8 =0.3740\n",
      "#Loss 9 =0.3312\n",
      "#Loss 10 =0.3069\n",
      "#Loss 11 =0.2926\n",
      "#Loss 12 =0.2841\n",
      "#Loss 13 =0.2790\n",
      "#Loss 14 =0.2759\n",
      "#Loss 15 =0.2739\n",
      "#Loss 16 =0.2727\n",
      "#Loss 17 =0.2720\n",
      "#Loss 18 =0.2715\n",
      "#Loss 19 =0.2712\n",
      "#Loss 20 =0.2710\n",
      "#Loss 21 =0.2709\n",
      "#Loss 22 =0.2708\n",
      "#Loss 23 =0.2708\n",
      "#Loss 24 =0.2708\n",
      "#Loss 25 =0.2707\n",
      "#Loss 26 =0.2707\n",
      "#Loss 27 =0.2707\n",
      "#Loss 28 =0.2707\n",
      "#Loss 29 =0.2707\n",
      "#Loss 30 =0.2707\n",
      " 2 100 绝对误差 tensor(0.0154, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.0258, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1740, dtype=torch.float64)   实验回归误差 tensor(0.0847, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.3266, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =320.7833\n",
      "#Loss 1 =319.7695\n",
      "#Loss 2 =219.1424\n",
      "#Loss 3 =144.0595\n",
      "#Loss 4 =38.6220\n",
      "#Loss 5 =65.1125\n",
      "#Loss 6 =202.1228\n",
      "#Loss 7 =179.6643\n",
      "#Loss 8 =153.2120\n",
      "#Loss 9 =118.9188\n",
      "#Loss 10 =74.2787\n",
      "#Loss 11 =23.0768\n",
      "#Loss 12 =113.1289\n",
      "#Loss 13 =197.7076\n",
      "#Loss 14 =171.9436\n",
      "#Loss 15 =132.0111\n",
      "#Loss 16 =79.1453\n",
      "#Loss 17 =77.8080\n",
      "#Loss 18 =103.0127\n",
      "#Loss 19 =76.6989\n",
      "#Loss 20 =104.5211\n",
      "#Loss 21 =76.2866\n",
      "#Loss 22 =101.9627\n",
      "#Loss 23 =76.8018\n",
      "#Loss 24 =104.9459\n",
      "#Loss 25 =76.1980\n",
      "#Loss 26 =101.5961\n",
      "#Loss 27 =76.8892\n",
      "#Loss 28 =105.3035\n",
      "#Loss 29 =75.8691\n",
      "#Loss 30 =100.2289\n",
      "#Loss 31 =77.1709\n",
      "#Loss 32 =106.4184\n",
      "#Loss 33 =74.9490\n",
      "#Loss 34 =97.1669\n",
      "#Loss 35 =80.3157\n",
      "#Loss 36 =117.5127\n",
      "#Loss 37 =69.6465\n",
      "#Loss 38 =69.7426\n",
      "#Loss 39 =70.4075\n",
      "#Loss 40 =73.7751\n",
      "#Loss 41 =87.6327\n",
      "#Loss 42 =134.8735\n",
      "#Loss 43 =76.5047\n",
      "#Loss 44 =88.9535\n",
      "#Loss 45 =138.7273\n",
      "#Loss 46 =79.2588\n",
      "#Loss 47 =91.9352\n",
      "#Loss 48 =145.8710\n",
      "#Loss 49 =92.7018\n",
      "#Loss 50 =88.4698\n",
      "#Loss 51 =137.3142\n",
      "#Loss 52 =78.1872\n",
      "#Loss 53 =90.9247\n",
      "#Loss 54 =143.5400\n",
      "#Loss 55 =89.7669\n",
      "#Loss 56 =89.2717\n",
      "#Loss 57 =140.4956\n",
      "#Loss 58 =85.8945\n",
      "#Loss 59 =90.1792\n",
      "#Loss 60 =143.4993\n",
      "#Loss 61 =89.2600\n",
      "#Loss 62 =92.0255\n",
      "#Loss 63 =146.8777\n",
      "#Loss 64 =94.2365\n",
      "#Loss 65 =89.0160\n",
      "#Loss 66 =138.3091\n",
      "#Loss 67 =79.2451\n",
      "#Loss 68 =92.0089\n",
      "#Loss 69 =146.0458\n",
      "#Loss 70 =92.8981\n",
      "#Loss 71 =88.3603\n",
      "#Loss 72 =136.9527\n",
      "#Loss 73 =77.9571\n",
      "#Loss 74 =90.7285\n",
      "#Loss 75 =143.0678\n",
      "#Loss 76 =88.6831\n",
      "#Loss 77 =88.5229\n",
      "#Loss 78 =138.6958\n",
      "#Loss 79 =83.0287\n",
      "#Loss 80 =88.1792\n",
      "#Loss 81 =139.2692\n",
      "#Loss 82 =83.8475\n",
      "#Loss 83 =92.3948\n",
      "#Loss 84 =149.2561\n",
      "#Loss 85 =95.7722\n",
      "#Loss 86 =87.7580\n",
      "#Loss 87 =134.6947\n",
      "#Loss 88 =76.6526\n",
      "#Loss 89 =89.2494\n",
      "#Loss 90 =139.5550\n",
      "#Loss 91 =79.9349\n",
      "#Loss 92 =92.4928\n",
      "#Loss 93 =147.1177\n",
      "#Loss 94 =94.1305\n",
      "#Loss 95 =87.5996\n",
      "#Loss 96 =134.6131\n",
      "#Loss 97 =76.4363\n",
      "#Loss 98 =88.9649\n",
      "#Loss 99 =138.7600\n",
      "#Loss 100 =79.2836\n",
      "#Loss 101 =91.9534\n",
      "#Loss 102 =145.9123\n",
      "#Loss 103 =92.7475\n",
      "#Loss 104 =88.4425\n",
      "#Loss 105 =137.2305\n",
      "#Loss 106 =78.1271\n",
      "#Loss 107 =90.8644\n",
      "#Loss 108 =143.3953\n",
      "#Loss 109 =89.6215\n",
      "#Loss 110 =89.3347\n",
      "#Loss 111 =140.6721\n",
      "#Loss 112 =86.1287\n",
      "#Loss 113 =90.3215\n",
      "#Loss 114 =143.8098\n",
      "#Loss 115 =89.6442\n",
      "#Loss 116 =91.9015\n",
      "#Loss 117 =146.5430\n",
      "#Loss 118 =93.8830\n",
      "#Loss 119 =89.2155\n",
      "#Loss 120 =138.9260\n",
      "#Loss 121 =79.6853\n",
      "#Loss 122 =92.2848\n",
      "#Loss 123 =146.6581\n",
      "#Loss 124 =93.5997\n",
      "#Loss 125 =87.9335\n",
      "#Loss 126 =135.6410\n",
      "#Loss 127 =77.0909\n",
      "#Loss 128 =89.8302\n",
      "#Loss 129 =140.8401\n",
      "#Loss 130 =85.4557\n",
      "#Loss 131 =87.5912\n",
      "#Loss 132 =137.4423\n",
      "#Loss 133 =82.3315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Loss 134 =91.3752\n",
      "#Loss 135 =147.7498\n",
      "#Loss 136 =96.8960\n",
      "#Loss 137 =83.3245\n",
      "#Loss 138 =124.8594\n",
      "#Loss 139 =70.2635\n",
      "#Loss 140 =72.6675\n",
      "#Loss 141 =85.8624\n",
      "#Loss 142 =96.5158\n",
      "#Loss 143 =157.3553\n",
      "#Loss 144 =108.0938\n",
      "#Loss 145 =78.0601\n",
      "#Loss 146 =100.4597\n",
      "#Loss 147 =82.9545\n",
      "#Loss 148 =119.3459\n",
      "#Loss 149 =69.9092\n",
      "#Loss 150 =69.5581\n",
      "超过迭代上限\n",
      " 2 100 绝对误差 tensor(4.0461, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(2.7195, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0580, dtype=torch.float64)   实验回归误差 tensor(0.4657, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4000, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =124.2547\n",
      "#Loss 1 =122.6196\n",
      "#Loss 2 =91.1249\n",
      "#Loss 3 =72.1351\n",
      "#Loss 4 =48.8895\n",
      "#Loss 5 =27.3938\n",
      "#Loss 6 =19.0448\n",
      "#Loss 7 =15.2552\n",
      "#Loss 8 =13.1399\n",
      "#Loss 9 =16.7147\n",
      "#Loss 10 =51.2098\n",
      "#Loss 11 =115.5624\n",
      "#Loss 12 =103.3947\n",
      "#Loss 13 =92.7816\n",
      "#Loss 14 =76.0374\n",
      "#Loss 15 =51.6238\n",
      "#Loss 16 =24.6851\n",
      "#Loss 17 =14.0243\n",
      "#Loss 18 =11.0425\n",
      "#Loss 19 =10.2286\n",
      "#Loss 20 =21.0861\n",
      "#Loss 21 =60.1807\n",
      "#Loss 22 =112.5419\n",
      "#Loss 23 =88.4358\n",
      "#Loss 24 =67.9289\n",
      "#Loss 25 =40.8644\n",
      "#Loss 26 =17.7842\n",
      "#Loss 27 =12.1455\n",
      "#Loss 28 =10.1262\n",
      "#Loss 29 =11.2624\n",
      "#Loss 30 =33.3230\n",
      "#Loss 31 =68.1435\n",
      "#Loss 32 =98.5732\n",
      "#Loss 33 =83.6966\n",
      "#Loss 34 =62.1098\n",
      "#Loss 35 =34.5039\n",
      "#Loss 36 =15.6345\n",
      "#Loss 37 =10.9356\n",
      "#Loss 38 =8.2358\n",
      "#Loss 39 =6.6622\n",
      "#Loss 40 =8.2472\n",
      "#Loss 41 =36.7052\n",
      "#Loss 42 =114.8716\n",
      "#Loss 43 =106.5036\n",
      "#Loss 44 =98.3169\n",
      "#Loss 45 =85.4984\n",
      "#Loss 46 =66.7504\n",
      "#Loss 47 =43.4347\n",
      "#Loss 48 =24.4955\n",
      "#Loss 49 =17.9710\n",
      "#Loss 50 =14.5724\n",
      "#Loss 51 =12.8873\n",
      "#Loss 52 =18.4619\n",
      "#Loss 53 =65.9247\n",
      "#Loss 54 =108.5406\n",
      "#Loss 55 =85.5428\n",
      "#Loss 56 =63.7561\n",
      "#Loss 57 =36.0293\n",
      "#Loss 58 =15.9910\n",
      "#Loss 59 =11.1424\n",
      "#Loss 60 =8.5769\n",
      "#Loss 61 =7.6265\n",
      "#Loss 62 =14.8943\n",
      "#Loss 63 =74.8304\n",
      "#Loss 64 =104.8364\n",
      "#Loss 65 =55.5151\n",
      "#Loss 66 =30.4284\n",
      "#Loss 67 =19.6486\n",
      "#Loss 68 =15.4849\n",
      "#Loss 69 =12.8976\n",
      "#Loss 70 =13.7741\n",
      "#Loss 71 =33.1808\n",
      "#Loss 72 =121.7160\n",
      "#Loss 73 =116.3480\n",
      "#Loss 74 =114.0125\n",
      "#Loss 75 =110.4664\n",
      "#Loss 76 =104.7280\n",
      "#Loss 77 =95.2879\n",
      "#Loss 78 =80.0218\n",
      "#Loss 79 =57.1372\n",
      "#Loss 80 =29.5595\n",
      "#Loss 81 =14.6878\n",
      "#Loss 82 =10.6388\n",
      "#Loss 83 =8.2205\n",
      "#Loss 84 =8.6256\n",
      "#Loss 85 =22.3264\n",
      "#Loss 86 =108.3491\n",
      "#Loss 87 =25.9171\n",
      "#Loss 88 =15.4417\n",
      "#Loss 89 =11.2224\n",
      "#Loss 90 =8.7671\n",
      "#Loss 91 =10.0680\n",
      "#Loss 92 =27.6624\n",
      "#Loss 93 =113.1722\n",
      "#Loss 94 =33.9913\n",
      "#Loss 95 =15.8975\n",
      "#Loss 96 =11.0873\n",
      "#Loss 97 =8.3070\n",
      "#Loss 98 =6.6162\n",
      "#Loss 99 =7.3522\n",
      "#Loss 100 =27.6769\n",
      "#Loss 101 =119.5400\n",
      "#Loss 102 =114.3404\n",
      "#Loss 103 =110.9371\n",
      "#Loss 104 =105.5332\n",
      "#Loss 105 =96.8445\n",
      "#Loss 106 =83.2580\n",
      "#Loss 107 =63.6957\n",
      "#Loss 108 =40.2167\n",
      "#Loss 109 =23.0389\n",
      "#Loss 110 =17.2412\n",
      "#Loss 111 =13.9368\n",
      "#Loss 112 =12.1193\n",
      "#Loss 113 =15.9211\n",
      "#Loss 114 =59.9238\n",
      "#Loss 115 =106.1759\n",
      "#Loss 116 =90.0549\n",
      "#Loss 117 =71.2539\n",
      "#Loss 118 =45.1749\n",
      "#Loss 119 =20.0923\n",
      "#Loss 120 =13.0642\n",
      "#Loss 121 =11.0863\n",
      "#Loss 122 =12.2301\n",
      "#Loss 123 =34.2757\n",
      "#Loss 124 =57.7369\n",
      "#Loss 125 =89.8626\n",
      "#Loss 126 =66.8602\n",
      "#Loss 127 =39.1319\n",
      "#Loss 128 =16.9595\n",
      "#Loss 129 =11.7149\n",
      "#Loss 130 =9.5381\n",
      "#Loss 131 =10.1537\n",
      "#Loss 132 =29.0580\n",
      "#Loss 133 =78.6932\n",
      "#Loss 134 =108.6893\n",
      "#Loss 135 =100.7143\n",
      "#Loss 136 =88.6853\n",
      "#Loss 137 =69.7766\n",
      "#Loss 138 =43.5405\n",
      "#Loss 139 =19.2347\n",
      "#Loss 140 =12.7715\n",
      "#Loss 141 =10.8153\n",
      "#Loss 142 =12.0632\n",
      "#Loss 143 =34.6782\n",
      "#Loss 144 =59.7782\n",
      "#Loss 145 =91.0411\n",
      "#Loss 146 =69.7363\n",
      "#Loss 147 =42.7681\n",
      "#Loss 148 =18.6290\n",
      "#Loss 149 =12.5289\n",
      "#Loss 150 =10.6410\n",
      "超过迭代上限\n",
      " 2 100 绝对误差 tensor(0.5237, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.4161, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0918, dtype=torch.float64)   实验回归误差 tensor(0.3121, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.3244, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 96   条件数 1.0\n",
      "Loss 0 =184.2463\n",
      "#Loss 1 =182.7523\n",
      "#Loss 2 =108.6743\n",
      "#Loss 3 =65.3842\n",
      "#Loss 4 =21.3483\n",
      "#Loss 5 =2.2480\n",
      "#Loss 6 =1.5672\n",
      "#Loss 7 =1.1030\n",
      "#Loss 8 =0.8972\n",
      "#Loss 9 =0.7582\n",
      "#Loss 10 =0.7208\n",
      "#Loss 11 =0.6816\n",
      "#Loss 12 =0.6956\n",
      "#Loss 13 =0.6765\n",
      "#Loss 14 =0.7056\n",
      "#Loss 15 =0.6920\n",
      "#Loss 16 =0.7295\n",
      "#Loss 17 =0.7165\n",
      "#Loss 18 =0.7627\n",
      "#Loss 19 =0.7468\n",
      "#Loss 20 =0.8034\n",
      "#Loss 21 =0.7820\n",
      "#Loss 22 =0.8513\n",
      "#Loss 23 =0.8220\n",
      "#Loss 24 =0.9061\n",
      "#Loss 25 =0.8664\n",
      "#Loss 26 =0.9678\n",
      "#Loss 27 =0.9148\n",
      "#Loss 28 =1.0357\n",
      "#Loss 29 =0.9664\n",
      "#Loss 30 =1.1091\n",
      "#Loss 31 =1.0201\n",
      "#Loss 32 =1.1865\n",
      "#Loss 33 =1.0746\n",
      "#Loss 34 =1.2656\n",
      "#Loss 35 =1.1284\n",
      "#Loss 36 =1.3447\n",
      "#Loss 37 =1.1800\n",
      "#Loss 38 =1.4209\n",
      "#Loss 39 =1.2274\n",
      "#Loss 40 =1.4912\n",
      "#Loss 41 =1.2704\n",
      "#Loss 42 =1.5558\n",
      "#Loss 43 =1.3087\n",
      "#Loss 44 =1.6135\n",
      "#Loss 45 =1.3421\n",
      "#Loss 46 =1.6641\n",
      "#Loss 47 =1.3698\n",
      "#Loss 48 =1.7060\n",
      "#Loss 49 =1.3929\n",
      "#Loss 50 =1.7411\n",
      "#Loss 51 =1.4119\n",
      "#Loss 52 =1.7701\n",
      "#Loss 53 =1.4274\n",
      "#Loss 54 =1.7937\n",
      "#Loss 55 =1.4399\n",
      "#Loss 56 =1.8128\n",
      "#Loss 57 =1.4499\n",
      "#Loss 58 =1.8281\n",
      "#Loss 59 =1.4578\n",
      "#Loss 60 =1.8403\n",
      "#Loss 61 =1.4641\n",
      "#Loss 62 =1.8499\n",
      "#Loss 63 =1.4691\n",
      "#Loss 64 =1.8575\n",
      "#Loss 65 =1.4729\n",
      "#Loss 66 =1.8634\n",
      "#Loss 67 =1.4759\n",
      "#Loss 68 =1.8680\n",
      "#Loss 69 =1.4783\n",
      "#Loss 70 =1.8716\n",
      "#Loss 71 =1.4801\n",
      "#Loss 72 =1.8745\n",
      "#Loss 73 =1.4816\n",
      "#Loss 74 =1.8766\n",
      "#Loss 75 =1.4827\n",
      "#Loss 76 =1.8783\n",
      "#Loss 77 =1.4835\n",
      "#Loss 78 =1.8796\n",
      "#Loss 79 =1.4842\n",
      "#Loss 80 =1.8807\n",
      "#Loss 81 =1.4847\n",
      "#Loss 82 =1.8814\n",
      "#Loss 83 =1.4851\n",
      "#Loss 84 =1.8821\n",
      "#Loss 85 =1.4854\n",
      "#Loss 86 =1.8825\n",
      "#Loss 87 =1.4856\n",
      "#Loss 88 =1.8829\n",
      "#Loss 89 =1.4858\n",
      "#Loss 90 =1.8832\n",
      "#Loss 91 =1.4860\n",
      "#Loss 92 =1.8834\n",
      "#Loss 93 =1.4861\n",
      "#Loss 94 =1.8836\n",
      "#Loss 95 =1.4861\n",
      "#Loss 96 =1.8837\n",
      "#Loss 97 =1.4862\n",
      "#Loss 98 =1.8838\n",
      "#Loss 99 =1.4863\n",
      "#Loss 100 =1.8839\n",
      "#Loss 101 =1.4863\n",
      "#Loss 102 =1.8839\n",
      "#Loss 103 =1.4863\n",
      "#Loss 104 =1.8840\n",
      "#Loss 105 =1.4864\n",
      "#Loss 106 =1.8840\n",
      "#Loss 107 =1.4864\n",
      "#Loss 108 =1.8840\n",
      "#Loss 109 =1.4864\n",
      "#Loss 110 =1.8841\n",
      "#Loss 111 =1.4864\n",
      "#Loss 112 =1.8841\n",
      "#Loss 113 =1.4864\n",
      "#Loss 114 =1.8841\n",
      "#Loss 115 =1.4864\n",
      "#Loss 116 =1.8841\n",
      "#Loss 117 =1.4864\n",
      "#Loss 118 =1.8841\n",
      "#Loss 119 =1.4864\n",
      "#Loss 120 =1.8841\n",
      "#Loss 121 =1.4864\n",
      "#Loss 122 =1.8841\n",
      "#Loss 123 =1.4864\n",
      "#Loss 124 =1.8841\n",
      "#Loss 125 =1.4864\n",
      "#Loss 126 =1.8841\n",
      "#Loss 127 =1.4864\n",
      "#Loss 128 =1.8841\n",
      "#Loss 129 =1.4864\n",
      "#Loss 130 =1.8841\n",
      "#Loss 131 =1.4864\n",
      "#Loss 132 =1.8841\n",
      "#Loss 133 =1.4864\n",
      "#Loss 134 =1.8841\n",
      "#Loss 135 =1.4864\n",
      "#Loss 136 =1.8841\n",
      "#Loss 137 =1.4864\n",
      "#Loss 138 =1.8841\n",
      "#Loss 139 =1.4864\n",
      "#Loss 140 =1.8841\n",
      "#Loss 141 =1.4864\n",
      "#Loss 142 =1.8841\n",
      "#Loss 143 =1.4864\n",
      "#Loss 144 =1.8841\n",
      "#Loss 145 =1.4864\n",
      "#Loss 146 =1.8841\n",
      "#Loss 147 =1.4864\n",
      "#Loss 148 =1.8841\n",
      "#Loss 149 =1.4864\n",
      "#Loss 150 =1.8841\n",
      "超过迭代上限\n",
      " 2 100 绝对误差 tensor(0.0341, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(0.0272, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0814, dtype=torch.float64)   实验回归误差 tensor(0.0903, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.2800, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 98   条件数 1.0\n",
      "Loss 0 =3.2524\n",
      "#Loss 1 =3.2305\n",
      "#Loss 2 =1.3247\n",
      "#Loss 3 =0.5798\n",
      "#Loss 4 =0.2850\n",
      "#Loss 5 =0.1659\n",
      "#Loss 6 =0.1169\n",
      "#Loss 7 =0.0964\n",
      "#Loss 8 =0.0878\n",
      "#Loss 9 =0.0840\n",
      "#Loss 10 =0.0823\n",
      "#Loss 11 =0.0815\n",
      "#Loss 12 =0.0811\n",
      "#Loss 13 =0.0809\n",
      "#Loss 14 =0.0807\n",
      "#Loss 15 =0.0805\n",
      "#Loss 16 =0.0804\n",
      "#Loss 17 =0.0803\n",
      "#Loss 18 =0.0802\n",
      "#Loss 19 =0.0801\n",
      "#Loss 20 =0.0800\n",
      "#Loss 21 =0.0799\n",
      "#Loss 22 =0.0798\n",
      "#Loss 23 =0.0797\n",
      "#Loss 24 =0.0796\n",
      "#Loss 25 =0.0796\n",
      "#Loss 26 =0.0795\n",
      "#Loss 27 =0.0794\n",
      "#Loss 28 =0.0794\n",
      "#Loss 29 =0.0793\n",
      "#Loss 30 =0.0793\n",
      "#Loss 31 =0.0792\n",
      "#Loss 32 =0.0792\n",
      "#Loss 33 =0.0791\n",
      "#Loss 34 =0.0791\n",
      "#Loss 35 =0.0790\n",
      "#Loss 36 =0.0790\n",
      "#Loss 37 =0.0790\n",
      "#Loss 38 =0.0789\n",
      "#Loss 39 =0.0789\n",
      "#Loss 40 =0.0789\n",
      "#Loss 41 =0.0788\n",
      "#Loss 42 =0.0788\n",
      "#Loss 43 =0.0788\n",
      "#Loss 44 =0.0787\n",
      "#Loss 45 =0.0787\n",
      "#Loss 46 =0.0787\n",
      "#Loss 47 =0.0787\n",
      "#Loss 48 =0.0787\n",
      "#Loss 49 =0.0786\n",
      "#Loss 50 =0.0786\n",
      "#Loss 51 =0.0786\n",
      "#Loss 52 =0.0786\n",
      "#Loss 53 =0.0786\n",
      "#Loss 54 =0.0786\n",
      "#Loss 55 =0.0785\n",
      "#Loss 56 =0.0785\n",
      "#Loss 57 =0.0785\n",
      "#Loss 58 =0.0785\n",
      "#Loss 59 =0.0785\n",
      "#Loss 60 =0.0785\n",
      "#Loss 61 =0.0785\n",
      "#Loss 62 =0.0785\n",
      "#Loss 63 =0.0785\n",
      "#Loss 64 =0.0784\n",
      "#Loss 65 =0.0784\n",
      "#Loss 66 =0.0784\n",
      "#Loss 67 =0.0784\n",
      "#Loss 68 =0.0784\n",
      "#Loss 69 =0.0784\n",
      "#Loss 70 =0.0784\n",
      "#Loss 71 =0.0784\n",
      "#Loss 72 =0.0784\n",
      "#Loss 73 =0.0784\n",
      "#Loss 74 =0.0784\n",
      "#Loss 75 =0.0784\n",
      "#Loss 76 =0.0784\n",
      "#Loss 77 =0.0784\n",
      "#Loss 78 =0.0784\n",
      "#Loss 79 =0.0784\n",
      "#Loss 80 =0.0784\n",
      "#Loss 81 =0.0784\n",
      "#Loss 82 =0.0784\n",
      "#Loss 83 =0.0783\n",
      "#Loss 84 =0.0783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Loss 85 =0.0783\n",
      "#Loss 86 =0.0783\n",
      "#Loss 87 =0.0783\n",
      "#Loss 88 =0.0783\n",
      "#Loss 89 =0.0783\n",
      "#Loss 90 =0.0783\n",
      "#Loss 91 =0.0783\n",
      "#Loss 92 =0.0783\n",
      "#Loss 93 =0.0783\n",
      "#Loss 94 =0.0783\n",
      "#Loss 95 =0.0783\n",
      "#Loss 96 =0.0783\n",
      "#Loss 97 =0.0783\n",
      "#Loss 98 =0.0783\n",
      "#Loss 99 =0.0783\n",
      "#Loss 100 =0.0783\n",
      "#Loss 101 =0.0783\n",
      "#Loss 102 =0.0783\n",
      "#Loss 103 =0.0783\n",
      "#Loss 104 =0.0783\n",
      "#Loss 105 =0.0783\n",
      "#Loss 106 =0.0783\n",
      "#Loss 107 =0.0783\n",
      "#Loss 108 =0.0783\n",
      " 2 100 绝对误差 tensor(0.4479, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.5086, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.5424, dtype=torch.float64)   实验回归误差 tensor(0.1552, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4070, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =14.5087\n",
      "#Loss 1 =13.9934\n",
      "#Loss 2 =6.8718\n",
      "#Loss 3 =4.6590\n",
      "#Loss 4 =3.6191\n",
      "#Loss 5 =2.9582\n",
      "#Loss 6 =2.4860\n",
      "#Loss 7 =2.1363\n",
      "#Loss 8 =1.8718\n",
      "#Loss 9 =1.6664\n",
      "#Loss 10 =1.5024\n",
      "#Loss 11 =1.3683\n",
      "#Loss 12 =1.2566\n",
      "#Loss 13 =1.1624\n",
      "#Loss 14 =1.0821\n",
      "#Loss 15 =1.0130\n",
      "#Loss 16 =0.9533\n",
      "#Loss 17 =0.9011\n",
      "#Loss 18 =0.8553\n",
      "#Loss 19 =0.8150\n",
      "#Loss 20 =0.7791\n",
      "#Loss 21 =0.7472\n",
      "#Loss 22 =0.7186\n",
      "#Loss 23 =0.6930\n",
      "#Loss 24 =0.6698\n",
      "#Loss 25 =0.6489\n",
      "#Loss 26 =0.6299\n",
      "#Loss 27 =0.6125\n",
      "#Loss 28 =0.5967\n",
      "#Loss 29 =0.5823\n",
      "#Loss 30 =0.5690\n",
      "#Loss 31 =0.5568\n",
      "#Loss 32 =0.5456\n",
      "#Loss 33 =0.5352\n",
      "#Loss 34 =0.5256\n",
      "#Loss 35 =0.5168\n",
      "#Loss 36 =0.5085\n",
      "#Loss 37 =0.5009\n",
      "#Loss 38 =0.4937\n",
      "#Loss 39 =0.4871\n",
      "#Loss 40 =0.4809\n",
      "#Loss 41 =0.4751\n",
      "#Loss 42 =0.4697\n",
      "#Loss 43 =0.4646\n",
      "#Loss 44 =0.4599\n",
      "#Loss 45 =0.4554\n",
      "#Loss 46 =0.4512\n",
      "#Loss 47 =0.4473\n",
      "#Loss 48 =0.4435\n",
      "#Loss 49 =0.4400\n",
      "#Loss 50 =0.4367\n",
      "#Loss 51 =0.4336\n",
      "#Loss 52 =0.4307\n",
      "#Loss 53 =0.4279\n",
      "#Loss 54 =0.4253\n",
      "#Loss 55 =0.4228\n",
      "#Loss 56 =0.4205\n",
      "#Loss 57 =0.4182\n",
      "#Loss 58 =0.4161\n",
      "#Loss 59 =0.4141\n",
      "#Loss 60 =0.4122\n",
      "#Loss 61 =0.4104\n",
      "#Loss 62 =0.4087\n",
      "#Loss 63 =0.4071\n",
      "#Loss 64 =0.4055\n",
      "#Loss 65 =0.4041\n",
      "#Loss 66 =0.4026\n",
      "#Loss 67 =0.4013\n",
      "#Loss 68 =0.4000\n",
      "#Loss 69 =0.3988\n",
      "#Loss 70 =0.3977\n",
      "#Loss 71 =0.3966\n",
      "#Loss 72 =0.3955\n",
      "#Loss 73 =0.3945\n",
      "#Loss 74 =0.3935\n",
      "#Loss 75 =0.3926\n",
      "#Loss 76 =0.3917\n",
      "#Loss 77 =0.3909\n",
      "#Loss 78 =0.3901\n",
      "#Loss 79 =0.3893\n",
      "#Loss 80 =0.3886\n",
      "#Loss 81 =0.3879\n",
      "#Loss 82 =0.3872\n",
      "#Loss 83 =0.3866\n",
      "#Loss 84 =0.3860\n",
      "#Loss 85 =0.3854\n",
      "#Loss 86 =0.3848\n",
      "#Loss 87 =0.3842\n",
      "#Loss 88 =0.3837\n",
      "#Loss 89 =0.3832\n",
      "#Loss 90 =0.3827\n",
      "#Loss 91 =0.3823\n",
      "#Loss 92 =0.3818\n",
      "#Loss 93 =0.3814\n",
      "#Loss 94 =0.3810\n",
      "#Loss 95 =0.3806\n",
      "#Loss 96 =0.3802\n",
      "#Loss 97 =0.3799\n",
      "#Loss 98 =0.3795\n",
      "#Loss 99 =0.3792\n",
      "#Loss 100 =0.3789\n",
      "#Loss 101 =0.3786\n",
      "#Loss 102 =0.3783\n",
      "#Loss 103 =0.3780\n",
      "#Loss 104 =0.3777\n",
      "#Loss 105 =0.3774\n",
      "#Loss 106 =0.3772\n",
      "#Loss 107 =0.3769\n",
      "#Loss 108 =0.3767\n",
      "#Loss 109 =0.3765\n",
      "#Loss 110 =0.3763\n",
      "#Loss 111 =0.3761\n",
      "#Loss 112 =0.3759\n",
      "#Loss 113 =0.3757\n",
      "#Loss 114 =0.3755\n",
      "#Loss 115 =0.3753\n",
      "#Loss 116 =0.3751\n",
      "#Loss 117 =0.3749\n",
      "#Loss 118 =0.3748\n",
      "#Loss 119 =0.3746\n",
      "#Loss 120 =0.3745\n",
      "#Loss 121 =0.3743\n",
      "#Loss 122 =0.3742\n",
      "#Loss 123 =0.3741\n",
      "#Loss 124 =0.3739\n",
      "#Loss 125 =0.3738\n",
      "#Loss 126 =0.3737\n",
      "#Loss 127 =0.3736\n",
      "#Loss 128 =0.3735\n",
      "#Loss 129 =0.3734\n",
      "#Loss 130 =0.3732\n",
      "#Loss 131 =0.3731\n",
      "#Loss 132 =0.3730\n",
      "#Loss 133 =0.3730\n",
      "#Loss 134 =0.3729\n",
      "#Loss 135 =0.3728\n",
      "#Loss 136 =0.3727\n",
      "#Loss 137 =0.3726\n",
      "#Loss 138 =0.3725\n",
      "#Loss 139 =0.3724\n",
      "#Loss 140 =0.3724\n",
      "#Loss 141 =0.3723\n",
      "#Loss 142 =0.3722\n",
      "#Loss 143 =0.3722\n",
      "#Loss 144 =0.3721\n",
      "#Loss 145 =0.3720\n",
      "#Loss 146 =0.3720\n",
      "#Loss 147 =0.3719\n",
      "#Loss 148 =0.3719\n",
      "#Loss 149 =0.3718\n",
      "#Loss 150 =0.3717\n",
      "超过迭代上限\n",
      " 2 100 绝对误差 tensor(1.3578, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(1.7027, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.2438, dtype=torch.float64)   实验回归误差 tensor(0.1601, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.4000, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 100   条件数 1.0\n",
      "Loss 0 =10153.2613\n",
      "#Loss 1 =10099.4021\n",
      "#Loss 2 =9495.3270\n",
      "#Loss 3 =25913503.7800\n",
      "#Loss 4 =nan\n",
      " 2 100 绝对误差 tensor(nan, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(nan, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0091, dtype=torch.float64)   实验回归误差 tensor(nan, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 0   条件数 1.0\n",
      "Loss 0 =12542785.1523\n",
      "#Loss 1 =12530185.1019\n",
      "#Loss 2 =12538092.4131\n",
      "#Loss 3 =12537800.6465\n",
      "#Loss 4 =12541536.8166\n",
      "#Loss 5 =12541332.8768\n",
      "#Loss 6 =12541009.5300\n",
      "#Loss 7 =12540912.0532\n",
      " 2 100 绝对误差 tensor(31.2430, dtype=torch.float64, grad_fn=<NormBackward0>)   相对误差 tensor(11.5433, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0003, dtype=torch.float64)   实验回归误差 tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)   置换矩阵误差 tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)   离置换矩阵距离 0   条件数 1.0\n"
     ]
    }
   ],
   "source": [
    "gama_=1\n",
    "eta=0\n",
    "starts=1\n",
    "for i_____ in range(5):\n",
    "    for num_example in range(100,101,100): \n",
    "        for num_X2feature in [2]:\n",
    "            for i____ in range(10):\n",
    "                (y_,X2_,true_w2,true_P,error_reg1,conditionnumber)=generatedata(noise=0.1)\n",
    "                y=y_\n",
    "                X2=X2_\n",
    "                results_Loss = []\n",
    "                results_w2=[]\n",
    "                results_error=[]\n",
    "                for i__ in range(starts):\n",
    "    #                     P_array=np.random.permutation(num_example)\n",
    "    #                     P=torch.zeros(num_example,num_example,dtype=torch.float64)\n",
    "    #                     for i in range(num_example):\n",
    "    #                         P[i][P_array[i]]=1\n",
    "    #                     X_=torch.cat([X1,X2],1)\n",
    "    #                     X=torch.mm(P,X_)\n",
    "    #                     w=torch.mm(torch.mm(torch.tensor(np.linalg.inv(torch.mm(X.transpose(1,0),X))),X.transpose(1,0)),y)\n",
    "    #                     w1,w2=w.split([num_X1feature,num_X2feature],dim=0)\n",
    "    #                     w1=torch.from_numpy(np.random.normal(0, 0,(num_X1feature,1)))\n",
    "    #                     w2=torch.from_numpy(np.random.normal(0, 0,(num_X2feature,1)))\n",
    "                    w2=generateinitialw(method='zeros')\n",
    "                    #w2=true_w2\n",
    "                    w2.requires_grad_(requires_grad=True)\n",
    "    #                 results_Loss = []\n",
    "                    lr=0.002\n",
    "                    results_S=[]\n",
    "                    t=0\n",
    "                    before1=0\n",
    "                    while True:                     \n",
    "                        Y1=y\n",
    "                        Y2=torch.exp(w2[0]*X2)*torch.sin(w2[1]*X2)\n",
    "                        C=torch.zeros(num_example,num_example,dtype=torch.float64)\n",
    "                        for i in range(num_example):\n",
    "                            for j in range(num_example):\n",
    "                                C[i][j]=(Y1[i]-Y2[j])**2            \n",
    "\n",
    "                        #S=SinkhornIPOT(C)\n",
    "                        a=torch.ones(num_example,1,dtype=torch.float64)\n",
    "                        b=torch.ones(num_example,1,dtype=torch.float64)\n",
    "                        S=sinkhorn_epsilon_scaling(a, b, C, 0.00000001)\n",
    "                        #print(S.transpose(1,0).half())\n",
    "                        #results_S.append(S)\n",
    "                        #if t>0:\n",
    "                            #print('        S变化',(torch.norm(results_S[t]-results_S[t-1]))/(torch.norm(results_S[t-1])))\n",
    "                        #Loss=torch.sum(S*C)\n",
    "                        Loss=torch.norm(Y1-torch.mm(S,Y2))**2\n",
    "                        if Loss<1e-2:\n",
    "                            break\n",
    "                        Loss.backward()\n",
    "    #                         results_Loss.append(Loss)\n",
    "    #                         for i_ in range(num_X1features):\n",
    "    #                             results_w1[t][i_]=(w1[i_].data)\n",
    "    #                         for i_ in range(num_X2features):\n",
    "    #                             results_w2[t][i_]=(w2[i_].data)\n",
    "                        w2.data-=lr*(w2.grad+np.random.normal(0,np.sqrt(eta/(1+t)**gama_)))\n",
    "                        #print(w2.grad)\n",
    "    #                     if t==num_epochs-1:\n",
    "    #                         print('最终w1梯度：',w1.grad)\n",
    "    #                         print('最终w2梯度：',w2.grad)\n",
    "                        w2.grad.data.zero_() \n",
    "                        print('Loss',t,'={:.4f}'.format(Loss.data))\n",
    "                        #print('Loss',t,'=',Loss)\n",
    "    #                     if t%6==0:\n",
    "    #                         if torch.norm(Loss-before1)<1e-4:\n",
    "    #                             break\n",
    "    #                         before1=Loss\n",
    "                        if torch.norm(Loss-before1)/before1<1e-5:\n",
    "                            break\n",
    "                        before1=Loss\n",
    "                        if t>=150:\n",
    "                            print('超过迭代上限')\n",
    "                            break\n",
    "                        if math.isnan(Loss):\n",
    "                            break\n",
    "                        t+=1\n",
    "                        print('#',end='')\n",
    "\n",
    "\n",
    "\n",
    "                    print(' ',end='')\n",
    "                    error_each=(torch.norm(w2-true_w2))\n",
    "                    #results_error.append(error_each)\n",
    "                    error_each2=(torch.norm(w2-true_w2))/torch.norm(true_w2)\n",
    "                    \n",
    "                    #results_Loss.append(Loss)\n",
    "                    #results_w1.append(w1.data)\n",
    "                    #results_w2.append(w2.data)\n",
    "\n",
    "\n",
    "                #w1=results_w1[results_Loss.index(min(results_Loss))]\n",
    "                #w2=results_w2[results_Loss.index(min(results_Loss))]\n",
    "\n",
    "    #                     for i_ in range(starts):\n",
    "    #                         results_w1[i_]=(w1[i_].data)\n",
    "    #                     for i_ in range(starts):\n",
    "    #                         results_w2[i_]=(w2[i_].data)\n",
    "\n",
    "\n",
    "                #error_w=((torch.norm(w1-true_w1))/(torch.norm(true_w1))+(torch.norm(w2-true_w2))/(torch.norm(true_w2)))/2\n",
    "                #print(num_X1feature,num_X2feature,num_example,'平均相对误差1：',error_w)\n",
    "                #print(num_X2feature,num_example,'平均相对误差2：',np.min(results_error),end='   ')\n",
    "                print(num_X2feature,num_example,'绝对误差',error_each,end='   ')\n",
    "                print('相对误差',error_each2,end='   ')\n",
    "                #print('真实置换矩阵为：',true_P)    \n",
    "                \n",
    "                error_reg2=(torch.norm(y_-torch.exp(w2[0]*torch.mm(S,X2_))*torch.sin(w2[1]*torch.mm(S,X2_)))/torch.norm(y_))\n",
    "                print('真实回归误差',error_reg1,end='   ')\n",
    "                print('实验回归误差',error_reg2,end='   ')\n",
    "                error_P=(torch.norm(S.transpose(1,0)-true_P))/(torch.norm(true_P))\n",
    "                print('置换矩阵误差',error_P,end='   ')\n",
    "                matrix_count=0\n",
    "                for i_2 in range(num_example):\n",
    "                    for j_2 in range(num_example):\n",
    "                        if (abs(S[i_2][j_2]-1)<0.02):\n",
    "                            matrix_count+=1\n",
    "                print('离置换矩阵距离',matrix_count,end='   ')\n",
    "                print('条件数',conditionnumber)\n",
    "                #print('双随机矩阵S为：',S.transpose(1,0).half())\n",
    "                #print(results)\n",
    "    #                 plt.figure(figsize=(6,6))\n",
    "    #                 plt.plot(results_w1_0,results_Loss, '-o',label='$w1[0]$')\n",
    "    #                 plt.plot(results_w1_1,results_Loss, '-o',label='$w1[1]$')\n",
    "    #                 plt.plot(results_w1_2,results_Loss, '-o',label='$w1[2]$')\n",
    "    #                 plt.plot(results_w2_0,results_Loss, '-o',label='$w2[0]$')\n",
    "    #                 plt.plot(results_w2_1,results_Loss, '-o',label='$w2[1]$')\n",
    "    #                 plt.plot(results_w2_2,results_Loss, '-o',label='$w2[2]$')\n",
    "    #                 plt.legend()\n",
    "    #                 plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
