{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline \n",
    "import torch \n",
    "#from IPython import display \n",
    "#from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "import random\n",
    "import math\n",
    "#from time import time\n",
    "#from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatedata(noise,showpermutation=False,showtrue_w=False):\n",
    "    true_w2 = torch.from_numpy(np.random.normal(0, 1,(num_X2feature,1)))\n",
    "    if showtrue_w:\n",
    "        print('true_w2:',true_w2)\n",
    "    X2_before_ =5* torch.from_numpy(np.random.normal(0, 1, (num_example, num_X2feature)))\n",
    "    y_ = torch.mm(X2_before_,true_w2)\n",
    "    y_ += torch.from_numpy(np.random.normal(0, noise ,size=y_.size()))\n",
    "    P_array=np.random.permutation(num_example)\n",
    "    P=torch.zeros(num_example,num_example,dtype=torch.float64)\n",
    "    for i in range(num_example):\n",
    "        P[i][P_array[i]]=1\n",
    "    if showpermutation:\n",
    "        print('打乱X2的置换矩阵为',P)\n",
    "    X2_=torch.mm(P,X2_before_)\n",
    "    #X2_=X2_before_\n",
    "    error_reg=(torch.norm(y_-torch.mm(X2_before_,true_w2))/torch.norm(y_))\n",
    "    return y_,X2_,true_w2,P,error_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateinitialw(method,showinitialw=False):\n",
    "    if method=='normal':\n",
    "        w2 = torch.from_numpy(np.random.normal(0, 1,(num_X2feature,1)))\n",
    "    if method=='zeros':\n",
    "        w2=torch.zeros(num_X2feature,1,dtype=torch.float64)\n",
    "    if showinitialw:\n",
    "        print('initial w2:',w2)\n",
    "    return w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_stabilized(a, b, M, reg, numItermax=1000, tau=1e3, stopThr=1e-9,\n",
    "                        warmstart=None, verbose=False, print_period=20,\n",
    "                        log=False, **kwargs):\n",
    "\n",
    "#     a = np.asarray(a, dtype=np.float64)\n",
    "#     b = np.asarray(b, dtype=np.float64)\n",
    "#     M = np.asarray(M, dtype=np.float64)\n",
    "    a=a\n",
    "    b=b\n",
    "    M=M\n",
    "\n",
    "#     if len(a) == 0:\n",
    "#         a = np.ones((M.shape[0],), dtype=np.float64) / M.shape[0]\n",
    "#     if len(b) == 0:\n",
    "#         b = np.ones((M.shape[1],), dtype=np.float64) / M.shape[1]\n",
    "\n",
    "    # test if multiple target\n",
    "#     if len(b.shape) > 1:\n",
    "#         n_hists = b.shape[1]\n",
    "#         a = a[:, np.newaxis]\n",
    "#     else:\n",
    "#         n_hists = 0\n",
    "    n_hists = 0\n",
    "    # init data\n",
    "    dim_a = len(a)\n",
    "    dim_b = len(b)\n",
    "\n",
    "    cpt = 0\n",
    "    if log:\n",
    "        log = {'err': []}\n",
    "\n",
    "    # we assume that no distances are null except those of the diagonal of\n",
    "    # distances\n",
    "    if warmstart is None:\n",
    "        alpha, beta = torch.zeros(dim_a,1,dtype=torch.float64), torch.zeros(dim_b,1,dtype=torch.float64)\n",
    "    else:\n",
    "        alpha, beta = warmstart\n",
    "\n",
    "    if n_hists:\n",
    "        u = torch.ones((dim_a, n_hists)) / dim_a\n",
    "        v = torch.ones((dim_b, n_hists)) / dim_b\n",
    "    else:\n",
    "        u, v = torch.ones(dim_a,1,dtype=torch.float64) / dim_a, torch.ones(dim_b,1,dtype=torch.float64) / dim_b\n",
    "\n",
    "    def get_K(alpha, beta):\n",
    "        \"\"\"log space computation\"\"\"\n",
    "        return torch.exp(-(M - alpha.reshape((dim_a, 1))- beta.reshape((1, dim_b))) / reg)\n",
    "\n",
    "    def get_Gamma(alpha, beta, u, v):\n",
    "        \"\"\"log space gamma computation\"\"\"\n",
    "        return torch.exp(-(M - alpha.reshape((dim_a, 1)) - beta.reshape((1, dim_b)))\n",
    "                      / reg + torch.log(u.reshape((dim_a, 1))) + torch.log(v.reshape((1, dim_b))))\n",
    "\n",
    "    # print(np.min(K))\n",
    "\n",
    "    K = get_K(alpha, beta)\n",
    "    transp = K\n",
    "    loop = 1\n",
    "    cpt = 0\n",
    "    err = 1\n",
    "    while loop:\n",
    "\n",
    "        uprev = u\n",
    "        vprev = v\n",
    "        # sinkhornrn update\n",
    "        v = b / (torch.mm(K.transpose(1,0), u) + 1e-16)\n",
    "        u = a / (torch.mm(K, v) + 1e-16)\n",
    "        # remove numerical problems and store them in K\n",
    "        if torch.abs(u).max() > tau or torch.abs(v).max() > tau:\n",
    "            if n_hists:\n",
    "                alpha, beta = alpha + reg * \\\n",
    "                    torch.max(torch.log(u), 1), beta + reg * torch.max(np.log(v))\n",
    "            else:\n",
    "                alpha, beta = alpha + reg * torch.log(u), beta + reg * torch.log(v)\n",
    "                if n_hists:\n",
    "                    u, v = torch.ones((dim_a, n_hists)) / dim_a, torch.ones((dim_b, n_hists)) / dim_b\n",
    "                else:\n",
    "                    u, v = torch.ones(dim_a,1,dtype=torch.float64) / dim_a, torch.ones(dim_b,1,dtype=torch.float64) / dim_b\n",
    "            K = get_K(alpha, beta)\n",
    "            \n",
    "\n",
    "        if cpt % print_period == 0:\n",
    "            # we can speed up the process by checking for the error only all\n",
    "            # the 10th iterations\n",
    "            if n_hists:\n",
    "                err_u = abs(u - uprev).max()\n",
    "                err_u /= max(abs(u).max(), abs(uprev).max(), 1.)\n",
    "                err_v = abs(v - vprev).max()\n",
    "                err_v /= max(abs(v).max(), abs(vprev).max(), 1.)\n",
    "                err = 0.5 * (err_u + err_v)\n",
    "            else:\n",
    "                transp = get_Gamma(alpha, beta, u, v)\n",
    "                err = torch.norm((torch.sum(transp, axis=0) - b))\n",
    "            if log:\n",
    "                log['err'].append(err)\n",
    "\n",
    "            if verbose:\n",
    "                if cpt % (print_period * 20) == 0:\n",
    "                    print(\n",
    "                        '{:5s}|{:12s}'.format('It.', 'Err') + '\\n' + '-' * 19)\n",
    "                print('{:5d}|{:8e}|'.format(cpt, err))\n",
    "\n",
    "        if err <= stopThr:\n",
    "            loop = False\n",
    "\n",
    "        if cpt >= numItermax:\n",
    "            loop = False\n",
    "\n",
    "        if np.any(np.isnan(u.detach().numpy())) or np.any(np.isnan(v.detach().numpy())):\n",
    "            # we have reached the machine precision\n",
    "            # come back to previous solution and quit loop\n",
    "            print('Warning: numerical errors at iteration', cpt)\n",
    "            u = uprev\n",
    "            v = vprev\n",
    "            break\n",
    "\n",
    "        cpt = cpt + 1\n",
    "    #print(cpt)\n",
    "    if log:\n",
    "        if n_hists:\n",
    "            alpha = alpha[:, None]\n",
    "            beta = beta[:, None]\n",
    "        logu = alpha / reg + torch.log(u)\n",
    "        logv = beta / reg + torch.log(v)\n",
    "        log['logu'] = logu\n",
    "        log['logv'] = logv\n",
    "        log['alpha'] = alpha + reg * torch.log(u)\n",
    "        log['beta'] = beta + reg * torch.log(v)\n",
    "        log['warmstart'] = (log['alpha'], log['beta'])\n",
    "        if n_hists:\n",
    "            res = torch.zeros((n_hists))\n",
    "            for i in range(n_hists):\n",
    "                res[i] = torch.sum(get_Gamma(alpha, beta, u[:, i], v[:, i]) * M)\n",
    "            return res, log\n",
    "\n",
    "        else:\n",
    "            return get_Gamma(alpha, beta, u, v), log\n",
    "    else:\n",
    "        if n_hists:\n",
    "            res = torch.zeros((n_hists))\n",
    "            for i in range(n_hists):\n",
    "                res[i] = torch.sum(get_Gamma(alpha, beta, u[:, i], v[:, i]) * M)\n",
    "            return res\n",
    "        else:\n",
    "            return get_Gamma(alpha, beta, u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_epsilon_scaling(a, b, M, reg, numItermax=100, epsilon0=1e4,\n",
    "                             numInnerItermax=100, tau=1e3, stopThr=1e-9,\n",
    "                             warmstart=None, verbose=False, print_period=10,\n",
    "                             log=False, **kwargs):\n",
    "    #a = np.asarray(a, dtype=np.float64)\n",
    "    #b = np.asarray(b, dtype=np.float64)\n",
    "    #M = np.asarray(M, dtype=np.float64)\n",
    "    a=a\n",
    "    b=b\n",
    "    M=M\n",
    "#     if len(a) == 0:\n",
    "#         a = np.ones((M.shape[0],), dtype=np.float64) / M.shape[0]\n",
    "#     if len(b) == 0:\n",
    "#         b = np.ones((M.shape[1],), dtype=np.float64) / M.shape[1]\n",
    "\n",
    "    # init data\n",
    "    dim_a = len(a)\n",
    "    #dim_a=num_example\n",
    "    dim_b = len(b)\n",
    "    #dim_b=num_example\n",
    "    # nrelative umerical precision with 64 bits\n",
    "    numItermin = 35\n",
    "    numItermax = max(numItermin, numItermax)  # ensure that last velue is exact\n",
    "\n",
    "    cpt = 0\n",
    "    if log:\n",
    "        log = {'err': []}\n",
    "\n",
    "    # we assume that no distances are null except those of the diagonal of\n",
    "    # distances\n",
    "    if warmstart is None:\n",
    "        alpha, beta = torch.zeros(dim_a,1,dtype=torch.float64), torch.zeros(dim_b,1,dtype=torch.float64)\n",
    "    else:\n",
    "        alpha, beta = warmstart\n",
    "\n",
    "    def get_K(alpha, beta):\n",
    "        \"\"\"log space computation\"\"\"\n",
    "        return torch.exp(-(M - alpha.reshape((dim_a, 1))\n",
    "                        - beta.reshape((1, dim_b))) / reg)\n",
    "\n",
    "    # print(np.min(K))\n",
    "    def get_reg(n):  # exponential decreasing\n",
    "        return (epsilon0 - reg) * np.exp(-n) + reg\n",
    "\n",
    "    loop = 1\n",
    "    cpt = 0\n",
    "    err = 1\n",
    "    while loop:\n",
    "\n",
    "        regi = get_reg(cpt)\n",
    "\n",
    "        G, logi = sinkhorn_stabilized(a, b, M, regi,\n",
    "                                      numItermax=numInnerItermax, stopThr=1e-9,\n",
    "                                      warmstart=(alpha, beta), verbose=False,\n",
    "                                      print_period=20, tau=tau, log=True)\n",
    "\n",
    "        alpha = logi['alpha']\n",
    "        beta = logi['beta']\n",
    "\n",
    "        if cpt >= numItermax:\n",
    "            loop = False\n",
    "\n",
    "        if cpt % (print_period) == 0:  # spsion nearly converged\n",
    "            # we can speed up the process by checking for the error only all\n",
    "            # the 10th iterations\n",
    "            transp = G\n",
    "            err = torch.norm(\n",
    "                (torch.sum(transp, axis=0) - b))**2 + torch.norm((torch.sum(transp, axis=1) - a))**2\n",
    "            if log:\n",
    "                log['err'].append(err)\n",
    "\n",
    "            if verbose:\n",
    "                if cpt % (print_period * 10) == 0:\n",
    "                    print(\n",
    "                        '{:5s}|{:12s}'.format('It.', 'Err') + '\\n' + '-' * 19)\n",
    "                print('{:5d}|{:8e}|'.format(cpt, err))\n",
    "\n",
    "        if err <= stopThr and cpt > numItermin:\n",
    "            loop = False\n",
    "\n",
    "        cpt = cpt + 1\n",
    "    # print('err=',err,' cpt=',cpt)\n",
    "    if log:\n",
    "        log['alpha'] = alpha\n",
    "        log['beta'] = beta\n",
    "        log['warmstart'] = (log['alpha'], log['beta'])\n",
    "        return G, log\n",
    "    else:\n",
    "        return G\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ 1 60 平均相对误差2： tensor(0.0251, dtype=torch.float64, grad_fn=<DivBackward0>)   1 60 置换矩阵误差： tensor(1.3540, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.3410, dtype=torch.float64)   实验回归误差 tensor(0.1454, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################ 1 60 平均相对误差2： tensor(0.0229, dtype=torch.float64, grad_fn=<DivBackward0>)   1 60 置换矩阵误差： tensor(1.2247, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1052, dtype=torch.float64)   实验回归误差 tensor(0.0654, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################# 1 60 平均相对误差2： tensor(0.0601, dtype=torch.float64, grad_fn=<DivBackward0>)   1 60 置换矩阵误差： tensor(1.3540, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.2231, dtype=torch.float64)   实验回归误差 tensor(0.1004, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################## 1 60 平均相对误差2： tensor(0.0278, dtype=torch.float64, grad_fn=<DivBackward0>)   1 60 置换矩阵误差： tensor(1.2111, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1218, dtype=torch.float64)   实验回归误差 tensor(0.0684, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######################## 1 60 平均相对误差2： tensor(0.0065, dtype=torch.float64, grad_fn=<DivBackward0>)   1 60 置换矩阵误差： tensor(1.3166, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1567, dtype=torch.float64)   实验回归误差 tensor(0.0890, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############### 1 60 平均相对误差2： tensor(0.0033, dtype=torch.float64, grad_fn=<DivBackward0>)   1 60 置换矩阵误差： tensor(0.8563, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0355, dtype=torch.float64)   实验回归误差 tensor(0.0308, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "##################### 1 60 平均相对误差2： tensor(0.0196, dtype=torch.float64, grad_fn=<DivBackward0>)   1 60 置换矩阵误差： tensor(1.1690, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1527, dtype=torch.float64)   实验回归误差 tensor(0.0958, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################### 1 60 平均相对误差2： tensor(2.0008, dtype=torch.float64, grad_fn=<DivBackward0>)   1 60 置换矩阵误差： tensor(1.4024, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1908, dtype=torch.float64)   实验回归误差 tensor(0.1391, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############## 1 60 平均相对误差2： tensor(0.1465, dtype=torch.float64, grad_fn=<DivBackward0>)   1 60 置换矩阵误差： tensor(1.3663, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.4642, dtype=torch.float64)   实验回归误差 tensor(0.1800, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "#################### 1 60 平均相对误差2： tensor(0.0334, dtype=torch.float64, grad_fn=<DivBackward0>)   1 60 置换矩阵误差： tensor(1.2517, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1303, dtype=torch.float64)   实验回归误差 tensor(0.0836, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################### 2 60 平均相对误差2： tensor(0.9839, dtype=torch.float64, grad_fn=<DivBackward0>)   2 60 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0324, dtype=torch.float64)   实验回归误差 tensor(0.1960, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################## 2 60 平均相对误差2： tensor(0.2870, dtype=torch.float64, grad_fn=<DivBackward0>)   2 60 置换矩阵误差： tensor(1.3038, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.5499, dtype=torch.float64)   实验回归误差 tensor(0.1316, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################################################## 2 60 平均相对误差2： tensor(0.0174, dtype=torch.float64, grad_fn=<DivBackward0>)   2 60 置换矩阵误差： tensor(1.1402, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0963, dtype=torch.float64)   实验回归误差 tensor(0.0723, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################# 2 60 平均相对误差2： tensor(0.7767, dtype=torch.float64, grad_fn=<DivBackward0>)   2 60 置换矩阵误差： tensor(1.3416, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0915, dtype=torch.float64)   实验回归误差 tensor(0.1170, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############### 2 60 平均相对误差2： tensor(0.6818, dtype=torch.float64, grad_fn=<DivBackward0>)   2 60 置换矩阵误差： tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.2835, dtype=torch.float64)   实验回归误差 tensor(0.1538, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################################################### 2 60 平均相对误差2： tensor(0.9228, dtype=torch.float64, grad_fn=<DivBackward0>)   2 60 置换矩阵误差： tensor(1.3904, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.2018, dtype=torch.float64)   实验回归误差 tensor(0.1388, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################################## 2 60 平均相对误差2： tensor(0.7568, dtype=torch.float64, grad_fn=<DivBackward0>)   2 60 置换矩阵误差： tensor(1.4024, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.2096, dtype=torch.float64)   实验回归误差 tensor(0.1737, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################# 2 60 平均相对误差2： tensor(0.8329, dtype=torch.float64, grad_fn=<DivBackward0>)   2 60 置换矩阵误差： tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1213, dtype=torch.float64)   实验回归误差 tensor(0.1649, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################### 2 60 平均相对误差2： tensor(0.7748, dtype=torch.float64, grad_fn=<DivBackward0>)   2 60 置换矩阵误差： tensor(1.3663, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0370, dtype=torch.float64)   实验回归误差 tensor(0.1265, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################################### 2 60 平均相对误差2： tensor(0.7918, dtype=torch.float64, grad_fn=<DivBackward0>)   2 60 置换矩阵误差： tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0847, dtype=torch.float64)   实验回归误差 tensor(0.1506, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "####################### 3 60 平均相对误差2： tensor(1.2624, dtype=torch.float64, grad_fn=<DivBackward0>)   3 60 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0480, dtype=torch.float64)   实验回归误差 tensor(0.1332, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################ 3 60 平均相对误差2： tensor(0.6307, dtype=torch.float64, grad_fn=<DivBackward0>)   3 60 置换矩阵误差： tensor(1.3904, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0768, dtype=torch.float64)   实验回归误差 tensor(0.1597, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################### 3 60 平均相对误差2： tensor(0.0029, dtype=torch.float64, grad_fn=<DivBackward0>)   3 60 置换矩阵误差： tensor(0.6583, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0248, dtype=torch.float64)   实验回归误差 tensor(0.0232, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################### 3 60 平均相对误差2： tensor(1.0298, dtype=torch.float64, grad_fn=<DivBackward0>)   3 60 置换矩阵误差： tensor(1.4024, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0620, dtype=torch.float64)   实验回归误差 tensor(0.1829, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################### 3 60 平均相对误差2： tensor(1.2835, dtype=torch.float64, grad_fn=<DivBackward0>)   3 60 置换矩阵误差： tensor(1.4024, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0560, dtype=torch.float64)   实验回归误差 tensor(0.2064, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################################# 3 60 平均相对误差2： tensor(0.0162, dtype=torch.float64, grad_fn=<DivBackward0>)   3 60 置换矩阵误差： tensor(1.2111, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1003, dtype=torch.float64)   实验回归误差 tensor(0.0637, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################################# 3 60 平均相对误差2： tensor(0.7330, dtype=torch.float64, grad_fn=<DivBackward0>)   3 60 置换矩阵误差： tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0732, dtype=torch.float64)   实验回归误差 tensor(0.1223, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################### 3 60 平均相对误差2： tensor(1.0319, dtype=torch.float64, grad_fn=<DivBackward0>)   3 60 置换矩阵误差： tensor(1.3904, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0304, dtype=torch.float64)   实验回归误差 tensor(0.1806, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################### 3 60 平均相对误差2： tensor(1.0576, dtype=torch.float64, grad_fn=<DivBackward0>)   3 60 置换矩阵误差： tensor(1.4024, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0458, dtype=torch.float64)   实验回归误差 tensor(0.1306, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "#################################### 3 60 平均相对误差2： tensor(0.8777, dtype=torch.float64, grad_fn=<DivBackward0>)   3 60 置换矩阵误差： tensor(1.3904, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0429, dtype=torch.float64)   实验回归误差 tensor(0.1537, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "####################################################### 4 60 平均相对误差2： tensor(1.6077, dtype=torch.float64, grad_fn=<DivBackward0>)   4 60 置换矩阵误差： tensor(1.4024, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0733, dtype=torch.float64)   实验回归误差 tensor(0.1357, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################################################ 4 60 平均相对误差2： tensor(1.3764, dtype=torch.float64, grad_fn=<DivBackward0>)   4 60 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0328, dtype=torch.float64)   实验回归误差 tensor(0.1249, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################ 4 60 平均相对误差2： tensor(0.0654, dtype=torch.float64, grad_fn=<DivBackward0>)   4 60 置换矩阵误差： tensor(1.1547, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0385, dtype=torch.float64)   实验回归误差 tensor(0.0324, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "####################################### 4 60 平均相对误差2： tensor(1.1269, dtype=torch.float64, grad_fn=<DivBackward0>)   4 60 置换矩阵误差： tensor(1.3663, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0556, dtype=torch.float64)   实验回归误差 tensor(0.1761, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################ 4 60 平均相对误差2： tensor(1.1546, dtype=torch.float64, grad_fn=<DivBackward0>)   4 60 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0527, dtype=torch.float64)   实验回归误差 tensor(0.1869, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######################### 4 60 平均相对误差2： tensor(0.4636, dtype=torch.float64, grad_fn=<DivBackward0>)   4 60 置换矩阵误差： tensor(1.3904, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0487, dtype=torch.float64)   实验回归误差 tensor(0.0773, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################################################################## 4 60 平均相对误差2： tensor(0.5372, dtype=torch.float64, grad_fn=<DivBackward0>)   4 60 置换矩阵误差： tensor(1.3038, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0494, dtype=torch.float64)   实验回归误差 tensor(0.0966, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################# 4 60 平均相对误差2： tensor(0.1999, dtype=torch.float64, grad_fn=<DivBackward0>)   4 60 置换矩阵误差： tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0570, dtype=torch.float64)   实验回归误差 tensor(0.0944, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "#################################################### 4 60 平均相对误差2： tensor(0.9203, dtype=torch.float64, grad_fn=<DivBackward0>)   4 60 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0747, dtype=torch.float64)   实验回归误差 tensor(0.1724, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################################################## 5 60 平均相对误差2： tensor(0.8083, dtype=torch.float64, grad_fn=<DivBackward0>)   5 60 置换矩阵误差： tensor(1.3663, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0491, dtype=torch.float64)   实验回归误差 tensor(0.0911, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################### 5 60 平均相对误差2： tensor(1.1214, dtype=torch.float64, grad_fn=<DivBackward0>)   5 60 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0579, dtype=torch.float64)   实验回归误差 tensor(0.2548, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################### 5 60 平均相对误差2： tensor(1.0102, dtype=torch.float64, grad_fn=<DivBackward0>)   5 60 置换矩阵误差： tensor(1.3904, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0341, dtype=torch.float64)   实验回归误差 tensor(0.1809, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######################################################## 5 60 平均相对误差2： tensor(1.2799, dtype=torch.float64, grad_fn=<DivBackward0>)   5 60 置换矩阵误差： tensor(1.4024, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0466, dtype=torch.float64)   实验回归误差 tensor(0.1807, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######################################################## 5 60 平均相对误差2： tensor(0.9363, dtype=torch.float64, grad_fn=<DivBackward0>)   5 60 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0574, dtype=torch.float64)   实验回归误差 tensor(0.1701, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######################################################################################## 5 60 平均相对误差2： tensor(1.4713, dtype=torch.float64, grad_fn=<DivBackward0>)   5 60 置换矩阵误差： tensor(1.3904, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0491, dtype=torch.float64)   实验回归误差 tensor(0.2002, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################################### 5 60 平均相对误差2： tensor(0.6097, dtype=torch.float64, grad_fn=<DivBackward0>)   5 60 置换矩阵误差： tensor(1.3663, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0446, dtype=torch.float64)   实验回归误差 tensor(0.0973, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################ 5 60 平均相对误差2： tensor(1.4442, dtype=torch.float64, grad_fn=<DivBackward0>)   5 60 置换矩阵误差： tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0496, dtype=torch.float64)   实验回归误差 tensor(0.0842, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################## 5 60 平均相对误差2： tensor(0.9733, dtype=torch.float64, grad_fn=<DivBackward0>)   5 60 置换矩阵误差： tensor(1.4024, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0336, dtype=torch.float64)   实验回归误差 tensor(0.1916, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################# 5 60 平均相对误差2： tensor(0.7977, dtype=torch.float64, grad_fn=<DivBackward0>)   5 60 置换矩阵误差： tensor(1.3904, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0412, dtype=torch.float64)   实验回归误差 tensor(0.1045, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############ 1 80 平均相对误差2： tensor(0.0105, dtype=torch.float64, grad_fn=<DivBackward0>)   1 80 置换矩阵误差： tensor(1.1619, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0765, dtype=torch.float64)   实验回归误差 tensor(0.0492, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########## 1 80 平均相对误差2： tensor(0.1900, dtype=torch.float64, grad_fn=<DivBackward0>)   1 80 置换矩阵误差： tensor(1.3693, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.4460, dtype=torch.float64)   实验回归误差 tensor(0.1505, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############# 1 80 平均相对误差2： tensor(4.0755e-05, dtype=torch.float64, grad_fn=<DivBackward0>)   1 80 置换矩阵误差： tensor(1.2942, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1149, dtype=torch.float64)   实验回归误差 tensor(0.0601, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############## 1 80 平均相对误差2： tensor(0.0149, dtype=torch.float64, grad_fn=<DivBackward0>)   1 80 置换矩阵误差： tensor(1.2349, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0934, dtype=torch.float64)   实验回归误差 tensor(0.0504, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################# 1 80 平均相对误差2： tensor(0.0139, dtype=torch.float64, grad_fn=<DivBackward0>)   1 80 置换矩阵误差： tensor(1.2042, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1112, dtype=torch.float64)   实验回归误差 tensor(0.0653, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############### 1 80 平均相对误差2： tensor(8.1087, dtype=torch.float64, grad_fn=<DivBackward0>)   1 80 置换矩阵误差： tensor(1.4053, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(1.0188, dtype=torch.float64)   实验回归误差 tensor(0.3087, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### 1 80 平均相对误差2： tensor(0.0278, dtype=torch.float64, grad_fn=<DivBackward0>)   1 80 置换矩阵误差： tensor(1.2145, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1157, dtype=torch.float64)   实验回归误差 tensor(0.0619, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################## 1 80 平均相对误差2： tensor(0.0288, dtype=torch.float64, grad_fn=<DivBackward0>)   1 80 置换矩阵误差： tensor(1.3134, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1610, dtype=torch.float64)   实验回归误差 tensor(0.0858, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############## 1 80 平均相对误差2： tensor(0.0073, dtype=torch.float64, grad_fn=<DivBackward0>)   1 80 置换矩阵误差： tensor(1.3134, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1280, dtype=torch.float64)   实验回归误差 tensor(0.0625, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############ 1 80 平均相对误差2： tensor(0.0085, dtype=torch.float64, grad_fn=<DivBackward0>)   1 80 置换矩阵误差： tensor(1.1726, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0817, dtype=torch.float64)   实验回归误差 tensor(0.0466, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "####################### 2 80 平均相对误差2： tensor(1.3667, dtype=torch.float64, grad_fn=<DivBackward0>)   2 80 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0902, dtype=torch.float64)   实验回归误差 tensor(0.1344, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############## 2 80 平均相对误差2： tensor(1.0958, dtype=torch.float64, grad_fn=<DivBackward0>)   2 80 置换矩阵误差： tensor(1.3964, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.2680, dtype=torch.float64)   实验回归误差 tensor(0.2244, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "##################################################### 2 80 平均相对误差2： tensor(0.0031, dtype=torch.float64, grad_fn=<DivBackward0>)   2 80 置换矩阵误差： tensor(1.1180, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0585, dtype=torch.float64)   实验回归误差 tensor(0.0410, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "###################### 2 80 平均相对误差2： tensor(0.8764, dtype=torch.float64, grad_fn=<DivBackward0>)   2 80 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.2025, dtype=torch.float64)   实验回归误差 tensor(0.1198, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############# 2 80 平均相对误差2： tensor(0.0264, dtype=torch.float64, grad_fn=<DivBackward0>)   2 80 置换矩阵误差： tensor(1.2145, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0899, dtype=torch.float64)   实验回归误差 tensor(0.0551, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######################################################################## 2 80 平均相对误差2： tensor(1.7193, dtype=torch.float64, grad_fn=<DivBackward0>)   2 80 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0685, dtype=torch.float64)   实验回归误差 tensor(0.0935, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################################# 2 80 平均相对误差2： tensor(0.9122, dtype=torch.float64, grad_fn=<DivBackward0>)   2 80 置换矩阵误差： tensor(1.3874, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0877, dtype=torch.float64)   实验回归误差 tensor(0.1068, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################## 2 80 平均相对误差2： tensor(0.6795, dtype=torch.float64, grad_fn=<DivBackward0>)   2 80 置换矩阵误差： tensor(1.3693, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1091, dtype=torch.float64)   实验回归误差 tensor(0.1636, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################# 2 80 平均相对误差2： tensor(0.6461, dtype=torch.float64, grad_fn=<DivBackward0>)   2 80 置换矩阵误差： tensor(1.3964, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1364, dtype=torch.float64)   实验回归误差 tensor(0.0995, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################ 2 80 平均相对误差2： tensor(0.2183, dtype=torch.float64, grad_fn=<DivBackward0>)   2 80 置换矩阵误差： tensor(1.3416, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0521, dtype=torch.float64)   实验回归误差 tensor(0.0660, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################## 3 80 平均相对误差2： tensor(0.6578, dtype=torch.float64, grad_fn=<DivBackward0>)   3 80 置换矩阵误差： tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0532, dtype=torch.float64)   实验回归误差 tensor(0.1163, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "###################################################### 3 80 平均相对误差2： tensor(0.0233, dtype=torch.float64, grad_fn=<DivBackward0>)   3 80 置换矩阵误差： tensor(1.1832, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0632, dtype=torch.float64)   实验回归误差 tensor(0.0431, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############## 3 80 平均相对误差2： tensor(0.6293, dtype=torch.float64, grad_fn=<DivBackward0>)   3 80 置换矩阵误差： tensor(1.3874, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0544, dtype=torch.float64)   实验回归误差 tensor(0.1194, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######################################### 3 80 平均相对误差2： tensor(0.0192, dtype=torch.float64, grad_fn=<DivBackward0>)   3 80 置换矩阵误差： tensor(1.1068, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0520, dtype=torch.float64)   实验回归误差 tensor(0.0353, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################################### 3 80 平均相对误差2： tensor(0.0203, dtype=torch.float64, grad_fn=<DivBackward0>)   3 80 置换矩阵误差： tensor(1.1292, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0380, dtype=torch.float64)   实验回归误差 tensor(0.0294, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######################################################################### 3 80 平均相对误差2： tensor(0.9680, dtype=torch.float64, grad_fn=<DivBackward0>)   3 80 置换矩阵误差： tensor(1.4053, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0736, dtype=torch.float64)   实验回归误差 tensor(0.1446, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################### 3 80 平均相对误差2： tensor(0.7795, dtype=torch.float64, grad_fn=<DivBackward0>)   3 80 置换矩阵误差： tensor(1.3964, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0845, dtype=torch.float64)   实验回归误差 tensor(0.1421, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "####################### 3 80 平均相对误差2： tensor(0.4254, dtype=torch.float64, grad_fn=<DivBackward0>)   3 80 置换矩阵误差： tensor(1.3693, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.3756, dtype=torch.float64)   实验回归误差 tensor(0.1710, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################### 4 80 平均相对误差2： tensor(0.9918, dtype=torch.float64, grad_fn=<DivBackward0>)   4 80 置换矩阵误差： tensor(1.3964, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0275, dtype=torch.float64)   实验回归误差 tensor(0.1906, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################################## 4 80 平均相对误差2： tensor(1.1430, dtype=torch.float64, grad_fn=<DivBackward0>)   4 80 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0587, dtype=torch.float64)   实验回归误差 tensor(0.1295, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################################################################## 4 80 平均相对误差2： tensor(0.0145, dtype=torch.float64, grad_fn=<DivBackward0>)   4 80 置换矩阵误差： tensor(1.1180, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0540, dtype=torch.float64)   实验回归误差 tensor(0.0356, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################## 4 80 平均相对误差2： tensor(1.1641, dtype=torch.float64, grad_fn=<DivBackward0>)   4 80 置换矩阵误差： tensor(1.3874, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0304, dtype=torch.float64)   实验回归误差 tensor(0.1535, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "#################################################################################### 4 80 平均相对误差2： tensor(1.1373, dtype=torch.float64, grad_fn=<DivBackward0>)   4 80 置换矩阵误差： tensor(1.3964, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0438, dtype=torch.float64)   实验回归误差 tensor(0.1015, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "###################################### 4 80 平均相对误差2： tensor(0.2954, dtype=torch.float64, grad_fn=<DivBackward0>)   4 80 置换矩阵误差： tensor(1.3601, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0593, dtype=torch.float64)   实验回归误差 tensor(0.0719, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################### 4 80 平均相对误差2： tensor(1.0018, dtype=torch.float64, grad_fn=<DivBackward0>)   4 80 置换矩阵误差： tensor(1.4053, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0328, dtype=torch.float64)   实验回归误差 tensor(0.1275, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################# 4 80 平均相对误差2： tensor(1.1638, dtype=torch.float64, grad_fn=<DivBackward0>)   4 80 置换矩阵误差： tensor(1.4053, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0583, dtype=torch.float64)   实验回归误差 tensor(0.1926, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "##################################################### 4 80 平均相对误差2： tensor(0.0054, dtype=torch.float64, grad_fn=<DivBackward0>)   4 80 置换矩阵误差： tensor(1.0607, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0338, dtype=torch.float64)   实验回归误差 tensor(0.0219, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################### 4 80 平均相对误差2： tensor(0.7810, dtype=torch.float64, grad_fn=<DivBackward0>)   4 80 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0698, dtype=torch.float64)   实验回归误差 tensor(0.1356, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################################### 5 80 平均相对误差2： tensor(0.8643, dtype=torch.float64, grad_fn=<DivBackward0>)   5 80 置换矩阵误差： tensor(1.3509, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1296, dtype=torch.float64)   实验回归误差 tensor(0.1202, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######################################################### 5 80 平均相对误差2： tensor(0.5221, dtype=torch.float64, grad_fn=<DivBackward0>)   5 80 置换矩阵误差： tensor(1.3693, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0328, dtype=torch.float64)   实验回归误差 tensor(0.0884, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################################# 5 80 平均相对误差2： tensor(0.4662, dtype=torch.float64, grad_fn=<DivBackward0>)   5 80 置换矩阵误差： tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0533, dtype=torch.float64)   实验回归误差 tensor(0.0878, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################### 5 80 平均相对误差2： tensor(1.1044, dtype=torch.float64, grad_fn=<DivBackward0>)   5 80 置换矩阵误差： tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0429, dtype=torch.float64)   实验回归误差 tensor(0.1268, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################# 5 80 平均相对误差2： tensor(1.2546, dtype=torch.float64, grad_fn=<DivBackward0>)   5 80 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0268, dtype=torch.float64)   实验回归误差 tensor(0.1212, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################################## 5 80 平均相对误差2： tensor(1.0074, dtype=torch.float64, grad_fn=<DivBackward0>)   5 80 置换矩阵误差： tensor(1.4053, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0645, dtype=torch.float64)   实验回归误差 tensor(0.1884, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "####################################################################### 5 80 平均相对误差2： tensor(0.5678, dtype=torch.float64, grad_fn=<DivBackward0>)   5 80 置换矩阵误差： tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0899, dtype=torch.float64)   实验回归误差 tensor(0.0720, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################ 5 80 平均相对误差2： tensor(0.9579, dtype=torch.float64, grad_fn=<DivBackward0>)   5 80 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0363, dtype=torch.float64)   实验回归误差 tensor(0.1291, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################################ 5 80 平均相对误差2： tensor(1.4307, dtype=torch.float64, grad_fn=<DivBackward0>)   5 80 置换矩阵误差： tensor(1.3964, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0327, dtype=torch.float64)   实验回归误差 tensor(0.1208, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################# 5 80 平均相对误差2： tensor(1.0837, dtype=torch.float64, grad_fn=<DivBackward0>)   5 80 置换矩阵误差： tensor(1.3874, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0372, dtype=torch.float64)   实验回归误差 tensor(0.1207, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######## 1 100 平均相对误差2： tensor(0.0027, dtype=torch.float64, grad_fn=<DivBackward0>)   1 100 置换矩阵误差： tensor(1.2000, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0821, dtype=torch.float64)   实验回归误差 tensor(0.0543, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######## 1 100 平均相对误差2： tensor(0.0038, dtype=torch.float64, grad_fn=<DivBackward0>)   1 100 置换矩阵误差： tensor(1.2083, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0475, dtype=torch.float64)   实验回归误差 tensor(0.0318, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######## 1 100 平均相对误差2： tensor(0.1052, dtype=torch.float64, grad_fn=<DivBackward0>)   1 100 置换矩阵误差： tensor(1.3416, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.3267, dtype=torch.float64)   实验回归误差 tensor(0.1067, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######## 1 100 平均相对误差2： tensor(0.0160, dtype=torch.float64, grad_fn=<DivBackward0>)   1 100 置换矩阵误差： tensor(1.1489, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0681, dtype=torch.float64)   实验回归误差 tensor(0.0402, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "###### 1 100 平均相对误差2： tensor(0.0030, dtype=torch.float64, grad_fn=<DivBackward0>)   1 100 置换矩阵误差： tensor(1.0583, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0343, dtype=torch.float64)   实验回归误差 tensor(0.0263, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########## 1 100 平均相对误差2： tensor(0.0220, dtype=torch.float64, grad_fn=<DivBackward0>)   1 100 置换矩阵误差： tensor(1.3038, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1613, dtype=torch.float64)   实验回归误差 tensor(0.0755, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "####### 1 100 平均相对误差2： tensor(0.0012, dtype=torch.float64, grad_fn=<DivBackward0>)   1 100 置换矩阵误差： tensor(1.0863, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0438, dtype=torch.float64)   实验回归误差 tensor(0.0328, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "####### 1 100 平均相对误差2： tensor(0.0093, dtype=torch.float64, grad_fn=<DivBackward0>)   1 100 置换矩阵误差： tensor(1.2961, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1128, dtype=torch.float64)   实验回归误差 tensor(0.0607, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "####### 1 100 平均相对误差2： tensor(0.1021, dtype=torch.float64, grad_fn=<DivBackward0>)   1 100 置换矩阵误差： tensor(1.3711, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.4574, dtype=torch.float64)   实验回归误差 tensor(0.1397, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "###### 1 100 平均相对误差2： tensor(2.2335, dtype=torch.float64, grad_fn=<DivBackward0>)   1 100 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.4646, dtype=torch.float64)   实验回归误差 tensor(0.1995, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################## 2 100 平均相对误差2： tensor(0.2696, dtype=torch.float64, grad_fn=<DivBackward0>)   2 100 置换矩阵误差： tensor(1.3416, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1043, dtype=torch.float64)   实验回归误差 tensor(0.0724, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "##################### 2 100 平均相对误差2： tensor(0.1473, dtype=torch.float64, grad_fn=<DivBackward0>)   2 100 置换矩阵误差： tensor(1.3711, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.2999, dtype=torch.float64)   实验回归误差 tensor(0.1150, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############ 2 100 平均相对误差2： tensor(0.5517, dtype=torch.float64, grad_fn=<DivBackward0>)   2 100 置换矩阵误差： tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1926, dtype=torch.float64)   实验回归误差 tensor(0.1139, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############### 2 100 平均相对误差2： tensor(1.3531, dtype=torch.float64, grad_fn=<DivBackward0>)   2 100 置换矩阵误差： tensor(1.3856, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0595, dtype=torch.float64)   实验回归误差 tensor(0.2072, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################### 2 100 平均相对误差2： tensor(0.0062, dtype=torch.float64, grad_fn=<DivBackward0>)   2 100 置换矩阵误差： tensor(1.0954, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0400, dtype=torch.float64)   实验回归误差 tensor(0.0291, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################## 2 100 平均相对误差2： tensor(0.8052, dtype=torch.float64, grad_fn=<DivBackward0>)   2 100 置换矩阵误差： tensor(1.4000, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0764, dtype=torch.float64)   实验回归误差 tensor(0.1459, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################ 2 100 平均相对误差2： tensor(0.7414, dtype=torch.float64, grad_fn=<DivBackward0>)   2 100 置换矩阵误差： tensor(1.4000, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1299, dtype=torch.float64)   实验回归误差 tensor(0.1367, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########## 2 100 平均相对误差2： tensor(1.1160, dtype=torch.float64, grad_fn=<DivBackward0>)   2 100 置换矩阵误差： tensor(1.4000, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.2887, dtype=torch.float64)   实验回归误差 tensor(0.1102, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################### 2 100 平均相对误差2： tensor(1.2643, dtype=torch.float64, grad_fn=<DivBackward0>)   2 100 置换矩阵误差： tensor(1.4071, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0993, dtype=torch.float64)   实验回归误差 tensor(0.1974, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######################## 2 100 平均相对误差2： tensor(1.2546, dtype=torch.float64, grad_fn=<DivBackward0>)   2 100 置换矩阵误差： tensor(1.4071, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1007, dtype=torch.float64)   实验回归误差 tensor(0.1464, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############# 3 100 平均相对误差2： tensor(0.7494, dtype=torch.float64, grad_fn=<DivBackward0>)   3 100 置换矩阵误差： tensor(1.4071, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0688, dtype=torch.float64)   实验回归误差 tensor(0.1466, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################### 3 100 平均相对误差2： tensor(1.2148, dtype=torch.float64, grad_fn=<DivBackward0>)   3 100 置换矩阵误差： tensor(1.4071, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0631, dtype=torch.float64)   实验回归误差 tensor(0.1478, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################## 3 100 平均相对误差2： tensor(0.8938, dtype=torch.float64, grad_fn=<DivBackward0>)   3 100 置换矩阵误差： tensor(1.4000, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0371, dtype=torch.float64)   实验回归误差 tensor(0.0897, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################### 3 100 平均相对误差2： tensor(0.9629, dtype=torch.float64, grad_fn=<DivBackward0>)   3 100 置换矩阵误差： tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0733, dtype=torch.float64)   实验回归误差 tensor(0.1374, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################ 3 100 平均相对误差2： tensor(1.2677, dtype=torch.float64, grad_fn=<DivBackward0>)   3 100 置换矩阵误差： tensor(1.4071, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0450, dtype=torch.float64)   实验回归误差 tensor(0.1575, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############### 3 100 平均相对误差2： tensor(0.7954, dtype=torch.float64, grad_fn=<DivBackward0>)   3 100 置换矩阵误差： tensor(1.3928, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0348, dtype=torch.float64)   实验回归误差 tensor(0.1010, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################### 3 100 平均相对误差2： tensor(0.2098, dtype=torch.float64, grad_fn=<DivBackward0>)   3 100 置换矩阵误差： tensor(1.3115, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0530, dtype=torch.float64)   实验回归误差 tensor(0.0672, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############## 3 100 平均相对误差2： tensor(1.3083, dtype=torch.float64, grad_fn=<DivBackward0>)   3 100 置换矩阵误差： tensor(1.4000, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0426, dtype=torch.float64)   实验回归误差 tensor(0.1525, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################ 3 100 平均相对误差2： tensor(1.3031, dtype=torch.float64, grad_fn=<DivBackward0>)   3 100 置换矩阵误差： tensor(1.4000, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0521, dtype=torch.float64)   实验回归误差 tensor(0.2437, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################### 3 100 平均相对误差2： tensor(0.6064, dtype=torch.float64, grad_fn=<DivBackward0>)   3 100 置换矩阵误差： tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0263, dtype=torch.float64)   实验回归误差 tensor(0.1080, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################################### 4 100 平均相对误差2： tensor(0.5902, dtype=torch.float64, grad_fn=<DivBackward0>)   4 100 置换矩阵误差： tensor(1.3856, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0430, dtype=torch.float64)   实验回归误差 tensor(0.0947, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "#################################### 4 100 平均相对误差2： tensor(0.4762, dtype=torch.float64, grad_fn=<DivBackward0>)   4 100 置换矩阵误差： tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0459, dtype=torch.float64)   实验回归误差 tensor(0.1097, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################# 4 100 平均相对误差2： tensor(1.2249, dtype=torch.float64, grad_fn=<DivBackward0>)   4 100 置换矩阵误差： tensor(1.4000, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0749, dtype=torch.float64)   实验回归误差 tensor(0.1203, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "##################################### 4 100 平均相对误差2： tensor(0.0142, dtype=torch.float64, grad_fn=<DivBackward0>)   4 100 置换矩阵误差： tensor(1.0954, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0408, dtype=torch.float64)   实验回归误差 tensor(0.0263, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "##################################################################### 4 100 平均相对误差2： tensor(1.4825, dtype=torch.float64, grad_fn=<DivBackward0>)   4 100 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0677, dtype=torch.float64)   实验回归误差 tensor(0.2021, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######################################################################################## 4 100 平均相对误差2： tensor(1.4391, dtype=torch.float64, grad_fn=<DivBackward0>)   4 100 置换矩阵误差： tensor(1.4071, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0563, dtype=torch.float64)   实验回归误差 tensor(0.1686, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################# 4 100 平均相对误差2： tensor(0.5398, dtype=torch.float64, grad_fn=<DivBackward0>)   4 100 置换矩阵误差： tensor(1.3928, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0379, dtype=torch.float64)   实验回归误差 tensor(0.0913, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "###################### 4 100 平均相对误差2： tensor(0.9833, dtype=torch.float64, grad_fn=<DivBackward0>)   4 100 置换矩阵误差： tensor(1.4071, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1090, dtype=torch.float64)   实验回归误差 tensor(0.1311, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "##################################### 4 100 平均相对误差2： tensor(0.5204, dtype=torch.float64, grad_fn=<DivBackward0>)   4 100 置换矩阵误差： tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0671, dtype=torch.float64)   实验回归误差 tensor(0.1131, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################## 4 100 平均相对误差2： tensor(0.4312, dtype=torch.float64, grad_fn=<DivBackward0>)   4 100 置换矩阵误差： tensor(1.3638, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0947, dtype=torch.float64)   实验回归误差 tensor(0.0702, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "##################################################################################################### 5 100 平均相对误差2： tensor(0.8988, dtype=torch.float64, grad_fn=<DivBackward0>)   5 100 置换矩阵误差： tensor(1.3856, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0364, dtype=torch.float64)   实验回归误差 tensor(0.0994, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "###################################### 5 100 平均相对误差2： tensor(0.2571, dtype=torch.float64, grad_fn=<DivBackward0>)   5 100 置换矩阵误差： tensor(1.3491, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0291, dtype=torch.float64)   实验回归误差 tensor(0.0757, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################### 5 100 平均相对误差2： tensor(1.0829, dtype=torch.float64, grad_fn=<DivBackward0>)   5 100 置换矩阵误差： tensor(1.4000, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0641, dtype=torch.float64)   实验回归误差 tensor(0.1181, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################################################################################## 5 100 平均相对误差2： tensor(0.6336, dtype=torch.float64, grad_fn=<DivBackward0>)   5 100 置换矩阵误差： tensor(1.3856, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0563, dtype=torch.float64)   实验回归误差 tensor(0.1051, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################################## 5 100 平均相对误差2： tensor(0.0127, dtype=torch.float64, grad_fn=<DivBackward0>)   5 100 置换矩阵误差： tensor(1.1576, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0454, dtype=torch.float64)   实验回归误差 tensor(0.0271, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################ 5 100 平均相对误差2： tensor(1.2955, dtype=torch.float64, grad_fn=<DivBackward0>)   5 100 置换矩阵误差： tensor(1.4071, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0369, dtype=torch.float64)   实验回归误差 tensor(0.1488, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "####################### 5 100 平均相对误差2： tensor(1.1755, dtype=torch.float64, grad_fn=<DivBackward0>)   5 100 置换矩阵误差： tensor(1.4000, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0506, dtype=torch.float64)   实验回归误差 tensor(0.1898, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################################################## 5 100 平均相对误差2： tensor(0.5261, dtype=torch.float64, grad_fn=<DivBackward0>)   5 100 置换矩阵误差： tensor(1.3928, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0906, dtype=torch.float64)   实验回归误差 tensor(0.1236, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "##################### 5 100 平均相对误差2： tensor(0.6409, dtype=torch.float64, grad_fn=<DivBackward0>)   5 100 置换矩阵误差： tensor(1.4071, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0500, dtype=torch.float64)   实验回归误差 tensor(0.0839, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################################################### 5 100 平均相对误差2： tensor(0.8202, dtype=torch.float64, grad_fn=<DivBackward0>)   5 100 置换矩阵误差： tensor(1.4071, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0738, dtype=torch.float64)   实验回归误差 tensor(0.0854, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########### 1 120 平均相对误差2： tensor(0.0085, dtype=torch.float64, grad_fn=<DivBackward0>)   1 120 置换矩阵误差： tensor(1.2715, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0620, dtype=torch.float64)   实验回归误差 tensor(0.0386, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########## 1 120 平均相对误差2： tensor(0.0371, dtype=torch.float64, grad_fn=<DivBackward0>)   1 120 置换矩阵误差： tensor(1.3229, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1589, dtype=torch.float64)   实验回归误差 tensor(0.0709, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########## 1 120 平均相对误差2： tensor(0.0350, dtype=torch.float64, grad_fn=<DivBackward0>)   1 120 置换矩阵误差： tensor(1.3416, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.2346, dtype=torch.float64)   实验回归误差 tensor(0.0903, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######## 1 120 平均相对误差2： tensor(0.0121, dtype=torch.float64, grad_fn=<DivBackward0>)   1 120 置换矩阵误差： tensor(1.3663, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.2430, dtype=torch.float64)   实验回归误差 tensor(0.0747, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "#### 1 120 平均相对误差2： tensor(0.0116, dtype=torch.float64, grad_fn=<DivBackward0>)   1 120 置换矩阵误差： tensor(1.3038, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1057, dtype=torch.float64)   实验回归误差 tensor(0.0442, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "###### 1 120 平均相对误差2： tensor(53.0971, dtype=torch.float64, grad_fn=<DivBackward0>)   1 120 置换矩阵误差： tensor(1.4082, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.9994, dtype=torch.float64)   实验回归误差 tensor(0.1467, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######## 1 120 平均相对误差2： tensor(0.0421, dtype=torch.float64, grad_fn=<DivBackward0>)   1 120 置换矩阵误差： tensor(1.3478, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1741, dtype=torch.float64)   实验回归误差 tensor(0.0739, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############## 1 120 平均相对误差2： tensor(0.0046, dtype=torch.float64, grad_fn=<DivBackward0>)   1 120 置换矩阵误差： tensor(1.2383, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0574, dtype=torch.float64)   实验回归误差 tensor(0.0332, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########### 1 120 平均相对误差2： tensor(0.0019, dtype=torch.float64, grad_fn=<DivBackward0>)   1 120 置换矩阵误差： tensor(1.2845, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0827, dtype=torch.float64)   实验回归误差 tensor(0.0444, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################### 1 120 平均相对误差2： tensor(19.8582, dtype=torch.float64, grad_fn=<DivBackward0>)   1 120 置换矩阵误差： tensor(1.4083, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(1.0013, dtype=torch.float64)   实验回归误差 tensor(0.1372, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################## 2 120 平均相对误差2： tensor(0.0027, dtype=torch.float64, grad_fn=<DivBackward0>)   2 120 置换矩阵误差： tensor(1.2583, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0655, dtype=torch.float64)   实验回归误差 tensor(0.0405, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "##################################################### 2 120 平均相对误差2： tensor(0.3860, dtype=torch.float64, grad_fn=<DivBackward0>)   2 120 置换矩阵误差： tensor(1.3723, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0472, dtype=torch.float64)   实验回归误差 tensor(0.0981, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################ 2 120 平均相对误差2： tensor(0.7349, dtype=torch.float64, grad_fn=<DivBackward0>)   2 120 置换矩阵误差： tensor(1.3964, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.5052, dtype=torch.float64)   实验回归误差 tensor(0.1214, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################## 2 120 平均相对误差2： tensor(0.1753, dtype=torch.float64, grad_fn=<DivBackward0>)   2 120 置换矩阵误差： tensor(1.3601, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0550, dtype=torch.float64)   实验回归误差 tensor(0.0582, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "##################################### 2 120 平均相对误差2： tensor(1.2056, dtype=torch.float64, grad_fn=<DivBackward0>)   2 120 置换矩阵误差： tensor(1.4024, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0902, dtype=torch.float64)   实验回归误差 tensor(0.1372, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################################## 2 120 平均相对误差2： tensor(0.7026, dtype=torch.float64, grad_fn=<DivBackward0>)   2 120 置换矩阵误差： tensor(1.3964, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0977, dtype=torch.float64)   实验回归误差 tensor(0.1081, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "#################### 2 120 平均相对误差2： tensor(1.4659, dtype=torch.float64, grad_fn=<DivBackward0>)   2 120 置换矩阵误差： tensor(1.4083, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0670, dtype=torch.float64)   实验回归误差 tensor(0.1332, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "#################### 2 120 平均相对误差2： tensor(0.1319, dtype=torch.float64, grad_fn=<DivBackward0>)   2 120 置换矩阵误差： tensor(1.3723, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1173, dtype=torch.float64)   实验回归误差 tensor(0.0576, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "#################### 2 120 平均相对误差2： tensor(0.6180, dtype=torch.float64, grad_fn=<DivBackward0>)   2 120 置换矩阵误差： tensor(1.4024, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0983, dtype=torch.float64)   实验回归误差 tensor(0.1010, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "####################################### 2 120 平均相对误差2： tensor(0.5571, dtype=torch.float64, grad_fn=<DivBackward0>)   2 120 置换矩阵误差： tensor(1.3784, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0977, dtype=torch.float64)   实验回归误差 tensor(0.0872, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################## 3 120 平均相对误差2： tensor(1.8901, dtype=torch.float64, grad_fn=<DivBackward0>)   3 120 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0332, dtype=torch.float64)   实验回归误差 tensor(0.1058, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################################### 3 120 平均相对误差2： tensor(1.6352, dtype=torch.float64, grad_fn=<DivBackward0>)   3 120 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0518, dtype=torch.float64)   实验回归误差 tensor(0.1958, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "##################### 3 120 平均相对误差2： tensor(0.3705, dtype=torch.float64, grad_fn=<DivBackward0>)   3 120 置换矩阵误差： tensor(1.3663, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0476, dtype=torch.float64)   实验回归误差 tensor(0.0950, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################### 3 120 平均相对误差2： tensor(1.2202, dtype=torch.float64, grad_fn=<DivBackward0>)   3 120 置换矩阵误差： tensor(1.4024, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0750, dtype=torch.float64)   实验回归误差 tensor(0.1590, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################### 3 120 平均相对误差2： tensor(1.1391, dtype=torch.float64, grad_fn=<DivBackward0>)   3 120 置换矩阵误差： tensor(1.4024, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0681, dtype=torch.float64)   实验回归误差 tensor(0.1525, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################################ 3 120 平均相对误差2： tensor(0.0133, dtype=torch.float64, grad_fn=<DivBackward0>)   3 120 置换矩阵误差： tensor(1.1762, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0477, dtype=torch.float64)   实验回归误差 tensor(0.0282, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################## 3 120 平均相对误差2： tensor(0.8675, dtype=torch.float64, grad_fn=<DivBackward0>)   3 120 置换矩阵误差： tensor(1.4024, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0599, dtype=torch.float64)   实验回归误差 tensor(0.1041, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################### 3 120 平均相对误差2： tensor(0.9348, dtype=torch.float64, grad_fn=<DivBackward0>)   3 120 置换矩阵误差： tensor(1.4083, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0650, dtype=torch.float64)   实验回归误差 tensor(0.1057, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################### 3 120 平均相对误差2： tensor(0.1641, dtype=torch.float64, grad_fn=<DivBackward0>)   3 120 置换矩阵误差： tensor(1.3102, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0585, dtype=torch.float64)   实验回归误差 tensor(0.0558, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "###################################### 3 120 平均相对误差2： tensor(0.4495, dtype=torch.float64, grad_fn=<DivBackward0>)   3 120 置换矩阵误差： tensor(1.3904, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0862, dtype=torch.float64)   实验回归误差 tensor(0.1006, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "##################### 4 120 平均相对误差2： tensor(1.1989, dtype=torch.float64, grad_fn=<DivBackward0>)   4 120 置换矩阵误差： tensor(1.4024, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0759, dtype=torch.float64)   实验回归误差 tensor(0.1170, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############## 4 120 平均相对误差2： tensor(1.4078, dtype=torch.float64, grad_fn=<DivBackward0>)   4 120 置换矩阵误差： tensor(1.4083, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0654, dtype=torch.float64)   实验回归误差 tensor(0.1670, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "###################################################### 4 120 平均相对误差2： tensor(0.3782, dtype=torch.float64, grad_fn=<DivBackward0>)   4 120 置换矩阵误差： tensor(1.4024, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0385, dtype=torch.float64)   实验回归误差 tensor(0.1088, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################ 4 120 平均相对误差2： tensor(0.9884, dtype=torch.float64, grad_fn=<DivBackward0>)   4 120 置换矩阵误差： tensor(1.3904, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0666, dtype=torch.float64)   实验回归误差 tensor(0.1030, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################ 4 120 平均相对误差2： tensor(0.7518, dtype=torch.float64, grad_fn=<DivBackward0>)   4 120 置换矩阵误差： tensor(1.3844, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0949, dtype=torch.float64)   实验回归误差 tensor(0.0969, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################################## 4 120 平均相对误差2： tensor(1.0354, dtype=torch.float64, grad_fn=<DivBackward0>)   4 120 置换矩阵误差： tensor(1.3964, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0323, dtype=torch.float64)   实验回归误差 tensor(0.0972, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "###################################### 4 120 平均相对误差2： tensor(0.7903, dtype=torch.float64, grad_fn=<DivBackward0>)   4 120 置换矩阵误差： tensor(1.4083, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0734, dtype=torch.float64)   实验回归误差 tensor(0.1147, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################################################################## 4 120 平均相对误差2： tensor(1.5876, dtype=torch.float64, grad_fn=<DivBackward0>)   4 120 置换矩阵误差： tensor(1.4083, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0434, dtype=torch.float64)   实验回归误差 tensor(0.1235, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################################ 4 120 平均相对误差2： tensor(1.3153, dtype=torch.float64, grad_fn=<DivBackward0>)   4 120 置换矩阵误差： tensor(1.3844, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0377, dtype=torch.float64)   实验回归误差 tensor(0.1711, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "####################################### 4 120 平均相对误差2： tensor(1.3440, dtype=torch.float64, grad_fn=<DivBackward0>)   4 120 置换矩阵误差： tensor(1.4083, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0413, dtype=torch.float64)   实验回归误差 tensor(0.1731, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################## 5 120 平均相对误差2： tensor(1.1100, dtype=torch.float64, grad_fn=<DivBackward0>)   5 120 置换矩阵误差： tensor(1.3964, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0406, dtype=torch.float64)   实验回归误差 tensor(0.1654, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "#################################################### 5 120 平均相对误差2： tensor(1.1287, dtype=torch.float64, grad_fn=<DivBackward0>)   5 120 置换矩阵误差： tensor(1.4083, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0706, dtype=torch.float64)   实验回归误差 tensor(0.1189, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "######################################################### 5 120 平均相对误差2： tensor(0.9348, dtype=torch.float64, grad_fn=<DivBackward0>)   5 120 置换矩阵误差： tensor(1.4083, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0412, dtype=torch.float64)   实验回归误差 tensor(0.0788, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################################## 5 120 平均相对误差2： tensor(1.2910, dtype=torch.float64, grad_fn=<DivBackward0>)   5 120 置换矩阵误差： tensor(1.3964, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0399, dtype=torch.float64)   实验回归误差 tensor(0.1457, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################################### 5 120 平均相对误差2： tensor(1.4589, dtype=torch.float64, grad_fn=<DivBackward0>)   5 120 置换矩阵误差： tensor(1.3964, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0491, dtype=torch.float64)   实验回归误差 tensor(0.1841, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "####################################################### 5 120 平均相对误差2： tensor(0.8298, dtype=torch.float64, grad_fn=<DivBackward0>)   5 120 置换矩阵误差： tensor(1.3964, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0354, dtype=torch.float64)   实验回归误差 tensor(0.0978, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "####################################################################### 5 120 平均相对误差2： tensor(0.9330, dtype=torch.float64, grad_fn=<DivBackward0>)   5 120 置换矩阵误差： tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0908, dtype=torch.float64)   实验回归误差 tensor(0.1420, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################################################# 5 120 平均相对误差2： tensor(0.2583, dtype=torch.float64, grad_fn=<DivBackward0>)   5 120 置换矩阵误差： tensor(1.3601, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0683, dtype=torch.float64)   实验回归误差 tensor(0.0694, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################################################### 5 120 平均相对误差2： tensor(1.3233, dtype=torch.float64, grad_fn=<DivBackward0>)   5 120 置换矩阵误差： tensor(1.4083, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0511, dtype=torch.float64)   实验回归误差 tensor(0.0933, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "####################################################################### 5 120 平均相对误差2： tensor(0.8968, dtype=torch.float64, grad_fn=<DivBackward0>)   5 120 置换矩阵误差： tensor(1.4083, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0796, dtype=torch.float64)   实验回归误差 tensor(0.1038, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gama_=1\n",
    "eta=0\n",
    "starts=1\n",
    "for num_example in range(60,121,20): \n",
    "    for num_X2feature in range(1,6,1):\n",
    "        for i____ in range(10):\n",
    "            (y_,X2_,true_w2,true_P,error_reg1)=generatedata(noise=0.5)\n",
    "            y=y_\n",
    "            X2=X2_\n",
    "            results_Loss = []\n",
    "            results_w2=[]\n",
    "            results_error=[]\n",
    "            for i__ in range(starts):\n",
    "#                     P_array=np.random.permutation(num_example)\n",
    "#                     P=torch.zeros(num_example,num_example,dtype=torch.float64)\n",
    "#                     for i in range(num_example):\n",
    "#                         P[i][P_array[i]]=1\n",
    "#                     X_=torch.cat([X1,X2],1)\n",
    "#                     X=torch.mm(P,X_)\n",
    "#                     w=torch.mm(torch.mm(torch.tensor(np.linalg.inv(torch.mm(X.transpose(1,0),X))),X.transpose(1,0)),y)\n",
    "#                     w1,w2=w.split([num_X1feature,num_X2feature],dim=0)\n",
    "#                     w1=torch.from_numpy(np.random.normal(0, 0,(num_X1feature,1)))\n",
    "#                     w2=torch.from_numpy(np.random.normal(0, 0,(num_X2feature,1)))\n",
    "                w2=generateinitialw(method='zeros')\n",
    "                #w2=true_w2\n",
    "                w2.requires_grad_(requires_grad=True)\n",
    "#                 results_Loss = []\n",
    "                lr=0.0002\n",
    "                results_S=[]\n",
    "                t=0\n",
    "                before1=0\n",
    "                while True:                     \n",
    "                    Y1=y\n",
    "                    Y2=torch.mm(X2,w2)\n",
    "                    C=torch.zeros(num_example,num_example,dtype=torch.float64)\n",
    "                    for i in range(num_example):\n",
    "                        for j in range(num_example):\n",
    "                            C[i][j]=(Y1[i]-Y2[j])**2            \n",
    " \n",
    "                    #S=SinkhornIPOT(C)\n",
    "                    a=torch.ones(num_example,1,dtype=torch.float64)\n",
    "                    b=torch.ones(num_example,1,dtype=torch.float64)\n",
    "                    S=sinkhorn_epsilon_scaling(a, b, C, 0.00000001)\n",
    "                    #print(S.transpose(1,0).half())\n",
    "                    #results_S.append(S)\n",
    "                    #if t>0:\n",
    "                        #print('        S变化',(torch.norm(results_S[t]-results_S[t-1]))/(torch.norm(results_S[t-1])))\n",
    "                    #Loss=torch.sum(S*C)\n",
    "                    Loss=torch.norm(Y1-torch.mm(S,Y2))**2\n",
    "                    if Loss<1e-2:\n",
    "                        break\n",
    "                    Loss.backward()\n",
    "#                         results_Loss.append(Loss)\n",
    "#                         for i_ in range(num_X1features):\n",
    "#                             results_w1[t][i_]=(w1[i_].data)\n",
    "#                         for i_ in range(num_X2features):\n",
    "#                             results_w2[t][i_]=(w2[i_].data)\n",
    "                    w2.data-=lr*(w2.grad+np.random.normal(0,np.sqrt(eta/(1+t)**gama_)))\n",
    "                    #print(w2.grad)\n",
    "#                     if t==num_epochs-1:\n",
    "#                         print('最终w1梯度：',w1.grad)\n",
    "#                         print('最终w2梯度：',w2.grad)\n",
    "                    w2.grad.data.zero_() \n",
    "\n",
    "                    #print('Loss',t,'=',Loss)\n",
    "#                     if t%6==0:\n",
    "#                         if torch.norm(Loss-before1)<1e-4:\n",
    "#                             break\n",
    "#                         before1=Loss\n",
    "                    if torch.norm(Loss-before1)/before1<1e-10:\n",
    "                        break\n",
    "                    before1=Loss\n",
    "                    if t>=500:\n",
    "                        print('超过迭代上限')\n",
    "                        break\n",
    "                    if math.isnan(Loss):\n",
    "                        break\n",
    "                    t+=1\n",
    "                    print('#',end='')\n",
    "                    \n",
    "\n",
    "\n",
    "                print(' ',end='')\n",
    "                error_each=(torch.norm(w2-true_w2))/(torch.norm(true_w2))\n",
    "                results_error.append(error_each)\n",
    "                #results_Loss.append(Loss)\n",
    "                #results_w1.append(w1.data)\n",
    "                #results_w2.append(w2.data)\n",
    "\n",
    "\n",
    "            #w1=results_w1[results_Loss.index(min(results_Loss))]\n",
    "            #w2=results_w2[results_Loss.index(min(results_Loss))]\n",
    "\n",
    "#                     for i_ in range(starts):\n",
    "#                         results_w1[i_]=(w1[i_].data)\n",
    "#                     for i_ in range(starts):\n",
    "#                         results_w2[i_]=(w2[i_].data)\n",
    "\n",
    "\n",
    "            #error_w=((torch.norm(w1-true_w1))/(torch.norm(true_w1))+(torch.norm(w2-true_w2))/(torch.norm(true_w2)))/2\n",
    "            #print(num_X1feature,num_X2feature,num_example,'平均相对误差1：',error_w)\n",
    "            print(num_X2feature,num_example,'平均相对误差2：',np.min(results_error),end='   ')\n",
    "            \n",
    "            #print('真实置换矩阵为：',true_P)\n",
    "            error_P=(torch.norm(S.transpose(1,0)-true_P))/(torch.norm(true_P))\n",
    "            print(num_X2feature,num_example,'置换矩阵误差：',error_P,end='   ')\n",
    "            error_reg2=(torch.norm(y_-torch.mm(torch.mm(S,X2_),w2))/torch.norm(y_))\n",
    "            print('真实回归误差',error_reg1,end='   ')\n",
    "            print('实验回归误差',error_reg2)\n",
    "            #print('双随机矩阵S为：',S.transpose(1,0).half())\n",
    "            #print(results)\n",
    "#                 plt.figure(figsize=(6,6))\n",
    "#                 plt.plot(results_w1_0,results_Loss, '-o',label='$w1[0]$')\n",
    "#                 plt.plot(results_w1_1,results_Loss, '-o',label='$w1[1]$')\n",
    "#                 plt.plot(results_w1_2,results_Loss, '-o',label='$w1[2]$')\n",
    "#                 plt.plot(results_w2_0,results_Loss, '-o',label='$w2[0]$')\n",
    "#                 plt.plot(results_w2_1,results_Loss, '-o',label='$w2[1]$')\n",
    "#                 plt.plot(results_w2_2,results_Loss, '-o',label='$w2[2]$')\n",
    "#                 plt.legend()\n",
    "#                 plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
