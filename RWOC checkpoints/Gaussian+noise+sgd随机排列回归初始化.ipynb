{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline \n",
    "import torch \n",
    "#from IPython import display \n",
    "#from matplotlib import pyplot as plt\n",
    "import numpy as np \n",
    "import random\n",
    "import math\n",
    "#from time import time\n",
    "#from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatedata(noise,showpermutation=False,showtrue_w=False):\n",
    "    true_w1 = torch.from_numpy(np.random.normal(0, 1,(num_X1feature,1)))\n",
    "    true_w2 = torch.from_numpy(np.random.normal(0, 1,(num_X2feature,1)))\n",
    "    if showtrue_w:\n",
    "        print('true_w1:',true_w1)\n",
    "        print('true_w2:',true_w2)\n",
    "    X1_ = 5*torch.from_numpy(np.random.normal(0, 1, (num_example, num_X1feature)))\n",
    "    X2_before_ =5* torch.from_numpy(np.random.normal(0, 1, (num_example, num_X2feature)))\n",
    "    y_ = torch.mm(X1_,true_w1)+torch.mm(X2_before_,true_w2)\n",
    "    y_ += torch.from_numpy(np.random.normal(0, noise ,size=y_.size()))\n",
    "    P_array=np.random.permutation(num_example)\n",
    "    P=torch.zeros(num_example,num_example,dtype=torch.float64)\n",
    "    for i in range(num_example):\n",
    "        P[i][P_array[i]]=1\n",
    "    if showpermutation:\n",
    "        print('打乱X2的置换矩阵为',P)\n",
    "    X2_=torch.mm(P,X2_before_)\n",
    "    #X2_=X2_before_\n",
    "    error_reg=(torch.norm(y_-torch.mm(X1_,true_w1)-torch.mm(X2_before_,true_w2))/torch.norm(y_-torch.mm(X1_,true_w1)))\n",
    "    return y_,X1_,X2_,true_w1,true_w2,P,error_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SinkhornIPOT(X,eps=5e-3,beta=12,num_L=1):\n",
    "    l=torch.ones(num_example,1,dtype=torch.float64)\n",
    "    b=l/num_example\n",
    "    G=torch.exp(-X/beta)\n",
    "    gama=torch.ones(num_example,num_example,dtype=torch.float64)   \n",
    "    i=0\n",
    "    before=torch.zeros(num_example,num_example,dtype=torch.float64)  \n",
    "    while True:\n",
    "        Q=G*gama\n",
    "        for j in range(num_L):\n",
    "            a=l/(torch.mm(Q,b))\n",
    "            b=l/(torch.mm(Q.transpose(1,0),a))\n",
    "        gama=torch.mm(torch.mm(torch.diag(a.squeeze(1)),Q),torch.diag(b.squeeze(1)))\n",
    "        if i%20==0:\n",
    "            if torch.norm(gama-before)<eps:\n",
    "                break\n",
    "            before=gama            \n",
    "            #print('IPOT循环是否收敛',torch.norm(ga2-ga1))\n",
    "        i+=1\n",
    "        if i>=100000:\n",
    "            break\n",
    "        if math.isnan(gama[0][0]):\n",
    "            break\n",
    "    #print(gama.half())\n",
    "    #print(i)\n",
    "    return gama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateinitialw(method,showinitialw=False):\n",
    "    if method=='normal':\n",
    "        w1 = torch.from_numpy(np.random.normal(0, 1,(num_X1feature,1)))\n",
    "        w2 = torch.from_numpy(np.random.normal(0, 1,(num_X2feature,1)))\n",
    "    if method=='zeros':\n",
    "        w1=torch.zeros(num_X1feature,1,dtype=torch.float64)\n",
    "        w2=torch.zeros(num_X2feature,1,dtype=torch.float64)\n",
    "    if method=='regression':\n",
    "        X=torch.cat([X1,X2],1)\n",
    "        w=torch.mm(torch.mm(torch.tensor(np.linalg.inv(torch.mm(X.transpose(1,0),X))),X.transpose(1,0)),y)\n",
    "        w1,w2=w.split([num_X1feature,num_X2feature],dim=0)\n",
    "    if showinitialw:\n",
    "        print('initial w1:',w1)\n",
    "        print('initial w2:',w2)\n",
    "    return w1,w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## 1 1 60 平均相对误差2： tensor(0.0727, dtype=torch.float64, grad_fn=<DivBackward0>)   1 1 60 置换矩阵误差： tensor(0.9916, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0530, dtype=torch.float64)   实验回归误差 tensor(0.0343, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "###################################################### 1 1 60 平均相对误差2： tensor(0.3246, dtype=torch.float64, grad_fn=<DivBackward0>)   1 1 60 置换矩阵误差： tensor(1.1418, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.8445, dtype=torch.float64)   实验回归误差 tensor(0.1446, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "###################################################################################################### 2 2 60 平均相对误差2： tensor(0.1377, dtype=torch.float64, grad_fn=<DivBackward0>)   2 2 60 置换矩阵误差： tensor(1.2277, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0842, dtype=torch.float64)   实验回归误差 tensor(0.0716, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################################################################################################################################################################################################################################################################################################################################################################################################## 2 2 60 平均相对误差2： tensor(0.1407, dtype=torch.float64, grad_fn=<DivBackward0>)   2 2 60 置换矩阵误差： tensor(1.2019, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.4064, dtype=torch.float64)   实验回归误差 tensor(0.1507, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "#################################################################################################################################################################################### 3 3 60 平均相对误差2： tensor(1.0626, dtype=torch.float64, grad_fn=<DivBackward0>)   3 3 60 置换矩阵误差： tensor(1.3808, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0953, dtype=torch.float64)   实验回归误差 tensor(0.0893, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "################################################################################################################################################################################################################################################################################################################################################################################################ 3 3 60 平均相对误差2： tensor(1.0595, dtype=torch.float64, grad_fn=<DivBackward0>)   3 3 60 置换矩阵误差： tensor(1.3775, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0944, dtype=torch.float64)   实验回归误差 tensor(0.0712, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "########################################################################################################################################################################################################################################################################################################################################################################################################################################################################################### 4 4 60 平均相对误差2： tensor(0.3631, dtype=torch.float64, grad_fn=<DivBackward0>)   4 4 60 置换矩阵误差： tensor(1.3406, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.1634, dtype=torch.float64)   实验回归误差 tensor(0.0732, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "#################################################################################################################################### 4 4 60 平均相对误差2： tensor(0.8618, dtype=torch.float64, grad_fn=<DivBackward0>)   4 4 60 置换矩阵误差： tensor(1.3744, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0302, dtype=torch.float64)   实验回归误差 tensor(0.0747, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "#################################################################################################################### 5 5 60 平均相对误差2： tensor(nan, dtype=torch.float64, grad_fn=<DivBackward0>)   5 5 60 置换矩阵误差： tensor(nan, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0527, dtype=torch.float64)   实验回归误差 tensor(nan, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "############################################################################################################################################################################################################################################################################################################################################################################################################ 5 5 60 平均相对误差2： tensor(1.0005, dtype=torch.float64, grad_fn=<DivBackward0>)   5 5 60 置换矩阵误差： tensor(1.3727, dtype=torch.float64, grad_fn=<DivBackward0>)   真实回归误差 tensor(0.0486, dtype=torch.float64)   实验回归误差 tensor(0.0846, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "#####################################################################################"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6ee8b37ea892>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m3e-2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                     \u001b[0mLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;31m#                         results_Loss.append(Loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m#                         for i_ in range(num_X1features):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gama_=3\n",
    "eta=0\n",
    "starts=1\n",
    "for num_example in range(60,91,10):\n",
    "    #for num_X2feature in range(2,6,1):    \n",
    "    for num_X1feature in range(1,6,1):\n",
    "        num_X2feature=num_X1feature\n",
    "        for i____ in range(2):\n",
    "            #num_epochs=70\n",
    "            (y_,X1_,X2_,true_w1,true_w2,true_P,error_reg1)=generatedata(noise=0.5)\n",
    "            y=y_\n",
    "            X1=X1_\n",
    "            X2=X2_\n",
    "            results_Loss = []\n",
    "            results_w1=[]\n",
    "            results_w2=[]\n",
    "            results_error=[]\n",
    "            for i__ in range(starts):\n",
    "#                     P_array=np.random.permutation(num_example)\n",
    "#                     P=torch.zeros(num_example,num_example,dtype=torch.float64)\n",
    "#                     for i in range(num_example):\n",
    "#                         P[i][P_array[i]]=1\n",
    "#                     X_=torch.cat([X1,X2],1)\n",
    "#                     X=torch.mm(P,X_)\n",
    "#                     w=torch.mm(torch.mm(torch.tensor(np.linalg.inv(torch.mm(X.transpose(1,0),X))),X.transpose(1,0)),y)\n",
    "#                     w1,w2=w.split([num_X1feature,num_X2feature],dim=0)\n",
    "#                     w1=torch.from_numpy(np.random.normal(0, 0,(num_X1feature,1)))\n",
    "#                     w2=torch.from_numpy(np.random.normal(0, 0,(num_X2feature,1)))\n",
    "                w1,w2=generateinitialw(method='zeros')\n",
    "                #w1=true_w1\n",
    "                #w2=true_w2\n",
    "                w1.requires_grad_(requires_grad=True)\n",
    "                w2.requires_grad_(requires_grad=True)\n",
    "#                 results_Loss = []\n",
    "                lr=0.0002\n",
    "                #results_S=[]\n",
    "                t=0\n",
    "                before1=0\n",
    "                while True:\n",
    "                #for t in range(num_epochs):                        \n",
    "\n",
    "                    Y1=y-torch.mm(X1,w1)\n",
    "                    Y2=torch.mm(X2,w2)\n",
    "                    C=torch.zeros(num_example,num_example,dtype=torch.float64)\n",
    "                    for i in range(num_example):\n",
    "                        for j in range(num_example):\n",
    "                            C[i][j]=(Y1[i]-Y2[j])**2            \n",
    " \n",
    "                    S=SinkhornIPOT(C)\n",
    "                    #print(S.transpose(1,0).half())\n",
    "                    #results_S.append(S)\n",
    "                    #if t>0:\n",
    "                        #print('        S变化',(torch.norm(results_S[t]-results_S[t-1]))/(torch.norm(results_S[t-1])))\n",
    "                    #Loss=torch.sum(S*C)\n",
    "                    Loss=torch.norm(Y1-torch.mm(S,Y2))**2\n",
    "                    if Loss<3e-2:\n",
    "                        break\n",
    "                    Loss.backward()\n",
    "#                         results_Loss.append(Loss)\n",
    "#                         for i_ in range(num_X1features):\n",
    "#                             results_w1[t][i_]=(w1[i_].data)\n",
    "#                         for i_ in range(num_X2features):\n",
    "#                             results_w2[t][i_]=(w2[i_].data)\n",
    "                    w1.data-=lr*(w1.grad+np.random.normal(0,np.sqrt(eta/(1+t)**gama_)))\n",
    "                    w2.data-=lr*(w2.grad+np.random.normal(0,np.sqrt(eta/(1+t)**gama_)))\n",
    "                    #print(w1.grad)\n",
    "                    #print(w2.grad)\n",
    "#                     if t==num_epochs-1:\n",
    "#                         print('最终w1梯度：',w1.grad)\n",
    "#                         print('最终w2梯度：',w2.grad)\n",
    "                    w1.grad.data.zero_() \n",
    "                    w2.grad.data.zero_() \n",
    "\n",
    "                    #print('Loss',t,'=',Loss)\n",
    "                    if t%6==0:\n",
    "                        if torch.norm(Loss-before1)<1e-4:\n",
    "                            break\n",
    "                        before1=Loss                                  \n",
    "                    if t>=500:\n",
    "                        break\n",
    "                    if math.isnan(Loss):\n",
    "                        break\n",
    "                    t+=1\n",
    "                    print('#',end='')\n",
    "                    \n",
    "\n",
    "\n",
    "                print(' ',end='')\n",
    "                error_each=((torch.norm(w1-true_w1))/(torch.norm(true_w1))+(torch.norm(w2-true_w2))/(torch.norm(true_w2)))/2\n",
    "                results_error.append(error_each)\n",
    "                #results_Loss.append(Loss)\n",
    "                #results_w1.append(w1.data)\n",
    "                #results_w2.append(w2.data)\n",
    "\n",
    "\n",
    "            #w1=results_w1[results_Loss.index(min(results_Loss))]\n",
    "            #w2=results_w2[results_Loss.index(min(results_Loss))]\n",
    "\n",
    "#                     for i_ in range(starts):\n",
    "#                         results_w1[i_]=(w1[i_].data)\n",
    "#                     for i_ in range(starts):\n",
    "#                         results_w2[i_]=(w2[i_].data)\n",
    "\n",
    "\n",
    "            #error_w=((torch.norm(w1-true_w1))/(torch.norm(true_w1))+(torch.norm(w2-true_w2))/(torch.norm(true_w2)))/2\n",
    "            #print(num_X1feature,num_X2feature,num_example,'平均相对误差1：',error_w)\n",
    "            print(num_X1feature,num_X2feature,num_example,'平均相对误差2：',np.min(results_error),end='   ')\n",
    "            \n",
    "            #print('真实置换矩阵为：',true_P)\n",
    "            error_P=(torch.norm(S.transpose(1,0)-true_P))/(torch.norm(true_P))\n",
    "            print(num_X1feature,num_X2feature,num_example,'置换矩阵误差：',error_P,end='   ')\n",
    "            error_reg2=(torch.norm(y_-torch.mm(X1_,w1)-torch.mm(torch.mm(S,X2_),w2))/torch.norm(y_-torch.mm(X1_,w1)))\n",
    "            print('真实回归误差',error_reg1,end='   ')\n",
    "            print('实验回归误差',error_reg2)\n",
    "            #print('双随机矩阵S为：',S.transpose(1,0).half())\n",
    "            #print(results)\n",
    "#                 plt.figure(figsize=(6,6))\n",
    "#                 plt.plot(results_w1_0,results_Loss, '-o',label='$w1[0]$')\n",
    "#                 plt.plot(results_w1_1,results_Loss, '-o',label='$w1[1]$')\n",
    "#                 plt.plot(results_w1_2,results_Loss, '-o',label='$w1[2]$')\n",
    "#                 plt.plot(results_w2_0,results_Loss, '-o',label='$w2[0]$')\n",
    "#                 plt.plot(results_w2_1,results_Loss, '-o',label='$w2[1]$')\n",
    "#                 plt.plot(results_w2_2,results_Loss, '-o',label='$w2[2]$')\n",
    "#                 plt.legend()\n",
    "#                 plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
